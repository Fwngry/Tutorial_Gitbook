{"./":{"url":"./","title":"Introduction","keywords":"","body":"1. 快速开始1. 快速开始 Emmm,没有什么好说的~ 一些笔记 "},"0 - 快速使用/3 分钟快速配置 Git.html":{"url":"0 - 快速使用/3 分钟快速配置 Git.html","title":"3 分钟快速配置 Git","keywords":"","body":"1.1.1. 1 在 Github 上创建远程仓1.1.2. 2 在 VSCode 上初始化本地仓1.1.3. 3 在 VSCode 上绑定远程仓1.1.4. 4 在 VSCode 上拉取远程仓内容1.1.5. 5 在 VSCode 上新建文件并提交到本地仓1.1.6. 6 在 VSCode 上将本地仓更改推送到 Github 远程仓1.2. GIT 基础知识备忘录 原文地址 blog.csdn.net 目录 1 在 Github 上创建远程仓 2 在 VSCode 上初始化本地仓 3 在 VSCode 上绑定远程仓 4 在 VSCode 上拉取远程仓内容 5 在 VSCode 上新建文件并提交到本地仓 6 在 VSCode 上将本地仓更改推送到 Github 远程仓 # 设置邮箱和用户名 git config --global user.email \"wyfsgm@gmail.com\" git config --global user.name \"Fwngry\" #改为国内网络 git init git remote add origin https://github.com/Fwngry/DeepSort_DataUtils.git # 先进行拉取 git pull origin master git add --all git commit -m \"commit Message\" # git原生分支master，github原生分支main，改名后直接提到main中 git branch -M main git push -u origin master https://www.bilibili.com/video/av95681488/) 1.1.1. 1 在 Github 上创建远程仓 创建时、可直接选择 ☑️ \"Initialize this repository with a README\"、初始化仓库。 此步骤相当于创建空的远程仓后在终端执行以下几句话： echo \"# GIT\" >> README.md git init git add README.md git commit -m \"first commit\" 1.1.2. 2 在 VSCode 上初始化本地仓 在本地电脑上创建空文件夹、如 GitTestForMac。 初始化本地 Git 存储库的工作区文件夹、点击『源代码管理』右上角『+』、在弹出对话框中选择默认的 GitTestForMac 1.1.3. 3 在 VSCode 上绑定远程仓 按 Shift+Command+P 、在弹出框中选择 『Git 添加远程仓』、远程仓库名称填写『origin』、远程仓的 URL 输入在 GITHUB 创建远程仓的 git 地址。 此步骤相当于用终端输入： git remote add origin https://github.com/Reykou/GitTest.git 如不执行以上动作会提示错误：『存储库未配置任何要推送到的远程存储库。』 1.1.4. 4 在 VSCode 上拉取远程仓内容 点击『源代码管理器: Git』界面中的右上角的『...』、选择『拉取自...』、在弹出菜单中依次选择『origin』,『origin/master』。拉取成功后完成后即可在文件栏中看到文件 README.md 常见问题：在第二个弹出菜单中没有『origin/master』选项怎么办？ 稍等 2-3 分钟、重试即可出现。 相当于在终端输入： git pull origin master 1.1.5. 5 在 VSCode 上新建文件并提交到本地仓 添加文件、在『源码管理器中』选择『√』，提交到本地仓。 1.1.6. 6 在 VSCode 上将本地仓更改推送到 Github 远程仓 在『源码管理器中』选择『推送到...』、在弹出框中选择分支『origin』。推送成功后、刷新 GitHut 工程界面、即可看到提交的新文件。 此步骤相当于在终端输入： git push -u origin master 1.2. GIT 基础知识备忘录 三种状态 Git 有三种状态，你的文件可能处于其中之一： 已提交（committed）、已修改（modified） 和 已暂存（staged）。 已修改表示修改了文件，但还没保存到数据库中。 已暂存表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 已提交表示数据已经安全地保存在本地数据库中。 这会让我们的 Git 项目拥有三个阶段：工作区、暂存区以及 Git 目录。 This leads us to the three main sections of a Git project: the working tree, the staging area, and the Git directory. 基本的 Git 工作流程如下： 在工作区中修改文件。 将你想要下次提交的更改选择性地暂存，这样只会将更改的部分添加到暂存区。 提交更新，找到暂存区的文件，将快照永久性存储到 Git 目录。 参考：GIT 官方文档 "},"0 - 快速使用/Conda 常用命令整理.html":{"url":"0 - 快速使用/Conda 常用命令整理.html","title":"Conda 常用命令整理","keywords":"","body":"1.1. 0. 获取版本号1.2. 1. 获取帮助1.3. 2. 环境管理1.4. 3. 分享环境1.5. 4. 包管理 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 主要参考 Anaconda 官方指南 Using Conda：https://conda.io/docs/using/index.html 环境：Win10 64bit with conda 4.3.14以下命令均在 windows 命令行中输入。一般来讲，无论是在 Linux，OS X 还是在 windows 系统中，在命令行窗口中输入的 conda 命令基本是一致的，除非有特别标注。 1.1. 0. 获取版本号 conda --version 或 conda -V 1.2. 1. 获取帮助 conda --help conda -h 查看某一命令的帮助，如 update 命令及 remove 命令 conda update --help conda remove --help 同理，以上命令中的--help也可以换成-h。 1.3. 2. 环境管理 查看环境管理的全部命令帮助 conda env -h 创建环境 conda create --name your_env_name 输入y确认创建。 创建制定 python 版本的环境 conda create --name your_env_name python=2.7 conda create --name your_env_name python=3 conda create --name your_env_name python=3.5 创建包含某些包的环境 conda create --name your_env_name numpy scipy 创建指定 python 版本下包含某些包的环境 conda create --name your_env_name python=3.5 numpy scipy 列举当前所有环境 conda info --envs conda env list 进入某个环境 activate your_env_name 退出当前环境 deactivate 复制某个环境 conda create --name new_env_name --clone old_env_name 删除某个环境 conda remove --name your_env_name --all 1.4. 3. 分享环境 如果你想把你当前的环境配置与别人分享，这样 ta 可以快速建立一个与你一模一样的环境（同一个版本的 python 及各种包）来共同开发 / 进行新的实验。一个分享环境的快速方法就是给 ta 一个你的环境的.yml文件。 首先通过activate target_env要分享的环境target_env，然后输入下面的命令会在当前工作目录下生成一个environment.yml文件， conda env export > environment.yml 小伙伴拿到environment.yml文件后，将该文件放在工作目录下，可以通过以下命令从该文件创建环境 conda env create -f environment.yml .yml是这个样子的 当然，你也可以手写一个.yml文件用来描述或记录你的 python 环境。 1.5. 4. 包管理 列举当前活跃环境下的所有包 conda list 列举一个非当前活跃环境下的所有包 conda list -n your_env_name 为指定环境安装某个包 conda install -n env_name package_name 如果不能通过 conda install 来安装，文档中提到可以从 Anaconda.org 安装，但我觉得会更习惯用 pip 直接安装。pip 在 Anaconda 中已安装好，不需要单独为每个环境安装 pip。如需要用 pip 管理包，activate 环境后直接使用即可。 TBA "},"0 - 快速使用/Linux&Shell常用命令整理.html":{"url":"0 - 快速使用/Linux&Shell常用命令整理.html","title":"Linux&Shell常用命令整理","keywords":"","body":"1.1. 远程连接服务器 Server1.2. Linux包管理器1.2.1. Conda1.2.2. pip1.3. Debug1.4. VIM1.5. 查询ip地址1.6. 文件与路径1.7. 文件树状结构 - Tree1.8. 重命名2. wget 命令 - 用终端下载文件2.1. Linux shell批处理命令2.2. 定位进程与杀进程2.3. &&（命令执行控制）2.4. pathlib2.4.1. as_posix()2.4.2. python shutil.copy()用法2.5. Debug1.1. 远程连接服务器 Server # 威富 ssh wyfubt@193.168.1.213 ssh wyfubt@server.natappfree.cc -p 37420 # 半导体 ssh wangyangfan@172.16.0.92 1.2. Linux包管理器 建议大家尽快适应并开始首先使用 apt。不仅因为广大 Linux 发行商都在推荐 apt，更主要的还是它提供了 Linux 包管理的必要选项。 1.2.1. Conda Conda管理虚拟环境 # list所有虚拟环境 conda env list # 创建虚拟环境 conda create -n tf tensorflow # 激活虚拟环境 conda activate tf # 关闭虚拟环境 conda deactivate # conda升级所有包 conda upgrade --all -y # 在某环境下安装包 conda install -n DeepsortYolov5 pandas # list某环境下安装的包 conda list -n env_name # 在某环境下安装reqirements conda install -n DeepsortYolov5 --yes --file requirements.txt 1.2.2. pip #更新pip pip install --upgrade pip pip 批量导出包含环境中所有组件的 requirements.txt 文件 pip freeze > requirements.txt pip 批量安装 requirements.txt 文件中包含的组件依赖 pip install -r requirements.txt conda 批量导出包含环境中所有组件的 requirements.txt 文件 conda list -e > requirements.txt Conda 批量安装 requirements.txt 文件中包含的组件依赖 conda install --yes --file requirements.txt 1.3. Debug To run a command as administrator (user \"root\"), use \"sudo \". 原因：权限不够，在你输入的命令前面加上sudo,如：sudo xxxx 保持现状：如果你要运行 \"ls\" 这条命令,那么你就在命令行上输入 \"sudo ls\",然后会提示你输入密码,那么你就输入密码。 但如果你想要保持在超级用户(也就是root,相当于管理员)状态的话,可以执行\"sudo -s\"然后输入密码,你就能作为超级用户执行命令了而不必在每一个命令前加\"sudo\"了. 1.4. VIM 进入vim模式 Vim /est/ssh/sshd_config vim编辑模式 # 进入编辑模式 >> i # 退出编辑模式 >> esc 退出vim模式 # 退出vim模式 >> :q # 强制退出 >> :q! # 保存修改 >> :w # 强制保存 >> :w! #保存并退出 >> :wq 1.5. 查询ip地址 查询ip地址：用“ifconfig”命令 - 观察inet 1.6. 文件与路径 命令pwd用于显示当前所在目录 删除文件 rm –rf命令 相对路径：基于当前路径 cd test # ..表示上一级菜单 绝对路径：斜杠开头 cd /etc/ssh 1.7. 文件树状结构 - Tree 原文地址 www.runoob.com Linux tree 命令用于以树状图列出目录的内容。 执行 tree 指令，它会列出指定目录下的所有文件，包括子目录里的文件。 以树状图列出当前目录结构。可直接使用如下命令： # install sudo apt-get install tree # Usage tree -N tree --help 1.8. 重命名 rename OldName NewName 原文地址 blog.csdn.net 坚持使用 Linux 终端，该如何从终端下载文件？ Linux 中没有下载命令，但是有几个用于下载文件的 Linux 命令。 2. wget 命令 - 用终端下载文件 wget 是非交互式的，可以轻松在后台运行。这意味着您可以轻松地在脚本中使用它，甚至可以构建 uGet 下载管理器之类的工具。 大多数 Linux 发行版都预装有 wget。大多数发行版的存储库中也提供了该软件，您可以使用发行版的程序包管理器轻松安装它。 对于 Linux 和类UNIX 的系统，wget 可能是最常用的命令行下载管理器。您可以使用 wget 下载单个文件，多个文件，整个目录，甚至整个网站。 安装 wget 下载&命名指定目录 把 index.html 文件保存到 \"/root/test\" 目录下. wget -P /root/test \"http://www.baidu.com/index.html\" 下载&指定命名 把 index.hmtl 保存到当前目录, 命令为 \"baidu.html\" wget -O \"baidu.html\" \"http://www.baidu.com/index.html\" 下载多个文件 要下载多个文件，可以将它们的URL保存在一个文本文件中，并提供该文本文件作为wget的输入，如下所示： wget -i download_files 恢复不完整的下载 如果由于某些原因按下 C 放弃了下载，则可以使用选项 - c 恢复上一次下载。 wget -c 2.1. Linux shell批处理命令 新建脚本 touch natstart.sh 写入命令 vim natstart.sh cd app ./Natapp -authtoken=de714e7ebdbbf2d5 执行命令 sh natstart.sh 2.2. 定位进程与杀进程 原文地址 www.linuxprobe.com 定位进程：top 和 ps 命令 top命令：能够知道到所有当前正在运行的进程有哪些。 top Chrome 浏览器反映迟钝，依据top 命令显示，我们能够辨别的有四个 Chrome 浏览器的进程在运行，进程的 pid 号分别是 3827、3919、10764 和 11679。这个信息是重要的，可以用一个特殊的方法来结束进程。 尽管 top 命令很是方便，但也不是得到你所要信息最有效的方法。 你明确要杀死的 Chrome 进程，并且你也不想看 top 命令所显示的实时信息。 鉴于此，使用 ps 命令然后用 grep 命令来过滤出输出结果。 ps aux | grep chrome 结束进程 现在我们开始结束进程的任务。我们有两种可以帮我们杀死错误的进程的信息。 你用哪一个将会决定终端命令如何使用，通常有两个命令来结束进程： kill - 通过进程 ID 来结束进程 killall - 通过进程名字来结束进程 最经常使用的结束进程的信号是： Signal NameSingle ValueEffectSIGHUP1挂起SIGINT2键盘的中断信号SIGKILL9发出杀死信号SIGTERM15发出终止信号SIGSTOP17, 19, 23停止进程 好的是，你能用信号值来代替信号名字。所以你没有必要来记住所有各种各样的信号名字。 所以，让我们现在用 kill 命令来杀死 Chrome 浏览器的进程。这个命令的结构是： kill SIGNAL PID 这里 SIGNAL 是要发送的信号，PID 是被杀死的进程的 ID。我们已经知道，来自我们的 ps 命令显示我们想要结束的进程 ID 号是 3827、3919、10764 和 11679。所以要发送结束进程信号，我们输入以下命令： kill -9 3827 kill -9 3919 kill -9 10764 kill -9 11679 一旦我们输入了以上命令，Chrome 浏览器的所有进程将会成功被杀死。 我们有更简单的方法！如果我们已经知道我们想要杀死的那个进程的名字，我们能够利用 killall 命令发送同样的信号 killall -9 chrome 附带说明的是，上边这个命令可能不能捕捉到所有正在运行的 Chrome 进程。如果，运行了上边这个命令之后，你输入 ps aux | grep chrome 命令过滤一下，看到剩下正在运行的 Chrome 进程有那些，最好的办法还是回到 kIll 命令通过进程 ID 来发送信号值 9 来结束这个进程。 举例 正如你看到的，杀死错误的进程并没有你原本想的那样有挑战性。当我让一个顽固的进程结束的时候，我趋向于用 killall 命令来作为有效的方法来终止，然而，当我让一个真正的活跃的进程结束的时候，kill 命令是一个好的方法。 ps -ef|grep natapp 杀进程时，填写框选出来的内容，最下一行代表查找程序本身, 忽略掉。 kill -9 pid 2.3. &&（命令执行控制） 语法格式如下： 　　command1 && command2 [&& command3 ...] 1 命令之间使用 && 连接，实现逻辑与的功能。 2 只有在 && 左边的命令返回真（命令返回值 $? == 0），&& 右边的命令才会被执行。 3 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。 2.4. pathlib 2.4.1. as_posix() 将 Windows 路径分隔符 ‘\\’ 改为 Unix 样式 ‘/’ 2.4.2. python shutil.copy()用法 shutil.copyfile(src, dst)：复制文件内容（不包含元数据）从src到dst。 DST必须是完整的目标文件名; 2.5. Debug Q1:开启内网穿透却无法进行ssh连接 https://blog.csdn.net/qq_36441027/article/details/81708726 本机连接服务器，会保存服务器的公钥，包括github或其他服务器。但是ip端口号对应的公钥被更改，就可能被误认为被攻击，无法连接，需要删除曾经在~/.ssh/known_hosts对应的公钥 Q2:无法连接SFTP 设置为密码登陆 快捷键 显示桌面：Ctrl + Win + d 切换输入法：win+space Root登录： https://www.youtube.com/watch?v=qziz2iEyLcc 安装网络app chmod a+x //权限 ./ //运行 卸载 dpkg --list | grep 返回 apt remove 无法定位软件包？需要更新源 https://www.jianshu.com/p/7916c6787b4f ubuntu ： 无法安全地用该源进行更新，所以默认禁用该源 解决占用端口问题：sudo lsof -i :7890 vscode在root下打不开，换成普通用户 安装搜狗输入法：https://pinyin.sogou.com/linux/help.php "},"0 - 快速使用/MacOS常用命令整理.html":{"url":"0 - 快速使用/MacOS常用命令整理.html","title":"MacOS常用命令整理","keywords":"","body":"1.1. 环境变量1.2. Finder分栏模式的宽度设置1.3. 快捷键1.1. 环境变量 原文地址 www.jianshu.com 打开环境变量配置 Mac 的环境变量配置保存在用户目录wangyangfan下的.bash_profile中，是共享的目录同级，下载目录的上一级。 显示隐藏文件:如果你想显示隐藏的文件夹，可以使用快捷键：⌘+ ⇧+.三个按键，再按下去就是隐藏了。 在该目录下，打开终端，输入如下命令，就可以打开该文件： open .bash_profile 增加环境变量 例如 Android 开发环境，需要在别的目录调用 adb 系列命令，就要添加 Android 目录，默认目录在 Users 目录下，后面的 lizhi 是电脑名，根据你的配置来改喔 export ANDROID_HOME=/Users/lizhi/Library/Android/sdk export PATH=${PATH}:${ANDROID_HOME}/tools export PATH=${PATH}:${ANDROID_HOME}/platform-tools Python3 开发环境配置，注意每个人安装的版本不一样，目录文件夹就不一样，后面的 3.7.7 要根据安装的具体版本改喔 export PATH=${PATH}:/usr/local/Cellar/python/3.7.7/bin alias python=\"/usr/local/Cellar/python/3.7.7/bin/python3\" alias pip=\"/usr/local/Cellar/python/3.7.7/bin/pip3\" 保存环境变量 改完文件，保存，并不代表应用，需要使用如下命令 source ./.bash_profile 验证 应用完毕后，在终端输入 adb，能出现 adb 命令的帮助。或者 python --version，能出现 python3 版本提示，就代表配置成功了。 1.2. Finder分栏模式的宽度设置 原文地址 blog.csdn.net 问题描述： 默认情况下，访达 (Finder) 分栏显示 - 分栏的宽度很窄，文件名长的时候显示不全，很是不方便。 解决方法： 1、打开 Finder，设置以分栏方式显示 2、按住option键，用鼠标拖动调整分栏的宽度。这样再次打开 Finder 时分栏默认宽度就是刚调整的宽度。 3、验证，按住option键，右键 Finder 图标–> 重新开启，就会发现分栏宽度是调整后的宽度。 1.3. 快捷键 ”您不能使用'.'开头的名称 ，因为这些名称已经被系统预留，请选择其他名称“ > Finder显示隐藏文件：⌘+ ⇧+.三个按键 重启Finder：按住option键，右键 Finder 图标–> 重新开启 "},"0 - 快速使用/Pycharm.html":{"url":"0 - 快速使用/Pycharm.html","title":"Pycharm","keywords":"","body":"1.1. 导包提示 unresolved reference1.2. no python interpreter configured1.1. 导包提示 unresolved reference 原文地址 blog.csdn.net 描述：模块部分，写一个外部模块导入的时候居然提示 unresolved reference，如下，程序可以正常运行，但是就是提示包部分红色，看着特别不美观，下面是解决办法 解决：https://u.nu/4saec 1. 进入PyCharm->Preferences->Build,Excution,Deployment->Console->Python Console勾选上Add source roots to PYTHONPATH; 进入PyCharm->Preferences->Project->Project Structure,通过选中某一目录右键添加sources; 点击Apply和OK即可. 1.2. no python interpreter configured File–>Setting–>Project，这时候看到选中栏显示的是 No interpreter Terminal：which python 3,把结果填入刚才的地址栏 "},"0 - 快速使用/VSCODE常用快捷键整理.html":{"url":"0 - 快速使用/VSCODE常用快捷键整理.html","title":"VSCODE常用快捷键整理","keywords":"","body":"1.1. 代码折叠 fold1.2. 打开Setting.json1.3. 外部引用问题1.4. Pylance - 插件1.1. 代码折叠 fold 操作所有代码块： 折叠所有 Ctrl+K+0 展开所有 Ctrl+K+J 操作代码块： 折叠 Ctrl+Shift+[ 展开 Ctrl+Shift+] 1.2. 打开Setting.json 在VS Code中键入ctrl+shift+P全局快捷键，打开命令搜索窗，输入settings.json即可打开首选项。 1.3. 外部引用问题 www.pianshen.com 问题分析： 外部引用无法“转到定义” 报错：unresolved reference 解决思路： 问题根源在于，VSCODE未能识别到外部引用目录。因此需要为 launch.json添加配置，同时在项目根文件夹中新建.env文件并写明对应的外部库路径，一箭双雕解决“转到引用”和“外部引用”无法识别的问题。 为 launch.json添加配置 launch.json的位置：${ProjectFolds} -.vscode - launch.json。launch.json属于项目，用于启动和调试。 \"env\": {\"PYTHONPATH\":\"${workspaceRoot}\"}, \"envFile\": \"${workspaceRoot}/.env\" .env添加路径 Command+Shift+.：显示隐藏文件 新建.env： PYTHONPATH=./lib 1.4. Pylance - 插件 支持 \"转到定义\" F12 功能，还讲源码中的包名和类名的关键字进行颜色区分显示，真的是实力与颜值俱在！ 当然，此时已自动将 settings.json 中 python 语言服务器设置为 Pylance： \"python.languageServer\": \"Pylance\" VS Code 中\"转到定义\"功能，核心是受 settings.json 中的 python.languageServer 参数控制，该参数合法取值有 Jedi、Microsoft 和 None，安装 Pylance 插件后还支持 Pylance。当设置为 Microsoft 和 None 时，无法实现转到定义，而设置 Jedi 和 Pylance 时可以。 VS Code 中搭建 Python 环境，建议安装两个插件：即 Python+Pylance，其中前者是 VS Code 支持 Python 编译的前提，后者是基于 Python 的扩展，支持自动补全、参数提示、转到定义等多项功能改进。 "},"0 - 快速使用/XCODE常用快捷键整理.html":{"url":"0 - 快速使用/XCODE常用快捷键整理.html","title":"XCODE常用快捷键整理","keywords":"","body":"Xcode自带Git的使用｜https://u.nu/1vtp Debug -调试 设置断点 逐语句与逐过程 逐语句是进入函数内部，进行单步调试。 逐过程就是把一个函数当成一条语句，不进入函数内部。 单步调试step into/step out/step over区别 step into：单步执行，遇到子函数就进入并且继续单步执行（简而言之，进入子函数）； step out：当单步执行到子函数内时，用step out就可以执行完子函数余下部分，并返回到上一层函数。 step over：在单步执行时，在函数内遇到子函数时不会进入子函数内单步执行，而是将子函数整个执行完再停止，把子函数整个作为一步。在不存在子函数的情况下是和step into效果一样的（越过子函数，但子函数会执行）。 编译并发布released 版本 更改 Xcode中所用的编译器类型： 运行到当前行： 代码折叠 在Xcode菜单里选择Preference——Text Editing，你会发现里面有一个“code folding ribbon”，勾选它就能恢复代码折叠功能了。 format 选中代码，control+i 批量重命名变量： 选中变量名-> refactor -> rename 恢复已关闭的标签页 Crtl + Shift + T 真正的全屏 Shift+cmd+F 上下切换标签页 下一页：^ + -> 上一页：^ + shift + -> "},"1 - Git_Github/1. Github添加公钥.html":{"url":"1 - Git_Github/1. Github添加公钥.html","title":"1. Github添加公钥","keywords":"","body":"问题描述：Clone Github 被拒绝 问题分析：公钥过期或未能正确添加 解决：添加公钥 - 见电子书《Github入门与实践》 $ ssh-keygen -t rsa -C \"your_email@example.com\" //创建公钥 $ cat ~/.ssh/id_rsa.pub //查看公钥 $ ssh -T git@github.com //测试 原理：《跟阿铭学linux》P48 - 使用秘钥登录 客户机-欲登录的主机；服务器-待登录的主机 客户机在.ssh/id_rsa.pub下得到自己的公钥后，将其复制到服务器的.ssh/authorized_keys文件中，这意味着客户端授信给了服务器 提示：加入Github时需要包含“ssh-rsa 公开密钥的内容 your_email@example.com”这三部分的完整内容 "},"1 - Git_Github/2. 关于git clone.html":{"url":"1 - Git_Github/2. 关于git clone.html","title":"2. 关于git clone","keywords":"","body":"1.1. clone 某项目包含的子项目1.2. 单独clone Github某个分支1.3. Clone 速度慢 - 安全方法1.1. clone 某项目包含的子项目 Git clone --recursive https://www.cnblogs.com/love-zf/p/13192734.html # Make sure to clone with --recursive git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git 如果Git仓库中含有子项目，--recursive把它依赖的一些项目同时下载下来. 某个工作中的项目，需要包含并使用另一个项目(也许是第三方库，或者你独立开发的，用于多个父项目的库)。你想要把它们当做两个独立的项目，同时又想在一个项目中使用另一个。 Git 通过子模块来解决这个问题：子模块允许你将一个 Git 仓库作为另一个 Git 仓库的子目录。它能让你将另一个仓库克隆到自己的项目中，同时还保持提交的独立。通过在 git submodule add 命令后面加上想要跟踪的项目的相对或绝对 URL 来添加新的子模块。 默认情况下，会将子项目放到一个与仓库同名的目录中。如果你想要放到其他地方，那么可以在命令结尾添加一个不同的路径。 1.2. 单独clone Github某个分支 StackOver回答： git clone -b yolov4-large --single-branch git@github.com:WongKinYiu/ScaledYOLOv4. git 1.3. Clone 速度慢 - 安全方法 只需要将 www.github.com/后面为代码库 改为[www.github.com. cnpmjs.org/后面为代码库地址 就可以实现一键式加速。亲测访问以及git clone 有效 我通过ping github.com发现，在ping的时候发现其 ip 竟然是192.*.*.*, 这是因为之前 github 卡，所以根据网上的教程改了 hosts，所以此时你需要重新改回来，只需要更改 hosts 将 github 这一行删除即可。 "},"1 - Git_Github/3. 关于.gitconfig.html":{"url":"1 - Git_Github/3. 关于.gitconfig.html","title":"3. 关于.gitconfig","keywords":"","body":"1.1. 1.位置1.2. 2.第一次配置1.3. 3.洁癖：全局忽略macOS自带文件1.4. 4.用于网络设置1.5. 5.中途加入ignore1.1. 1.位置 https://www.cnblogs.com/carriezhao/p/10775275.html 全局config在用户目录下 - /Users/wangyangfan/.gitconfig 仓库config在仓库目录下 隐藏打开：shift+command+. 1.2. 2.第一次配置 git config --global user.name \"Fwngry\" git config --global user.email \"wyfsgm@gmail.com\" 1.3. 3.洁癖：全局忽略macOS自带文件 https://www.cnblogs.com/everlose/p/12825937.html pwd：/Users/wangyangfan/.gitignore_global # .gitignore_global #################################### ######## OS generated files ######## #################################### .DS_Store .DS_Store? *.swp ._* .Spotlight-V100 .Trashes Icon? ehthumbs.db Thumbs.db #################################### ############# Packages ############# #################################### *.7z *.dmg *.gz *.iso *.jar *.rar *.tar *.zip pwd：/Users/wangyangfan/.gitconfig [core] excludesfile = ~/.gitignore_global 1.4. 4.用于网络设置 1.取消代理 pwd：/Users/wangyangfan/.gitconfig 删除以下代码 [http] sslBackend = openssl [https] proxy = localhost:1080 2.取消代理sudo vim /etc/hosts 3.关闭clash代理 4.重启terminal 1.5. 5.中途加入ignore 如果是项目做到一半才开始加入.gitignore,则需要在commit所有已经修改文件后，执行以下命令保证.gitignore开始生效。 git rm -r --cached . git add . git commit -m 'update .gitignore' "},"1 - Git_Github/4. 关于.gitignore.html":{"url":"1 - Git_Github/4. 关于.gitignore.html","title":"4. 关于.gitignore","keywords":"","body":"1.1. .gitignore文件的作用1.2. .gitignore文件的语法1.2.1. 注释1.2.2. 忽略文件 / 文件夹1.2.3. 不忽略文件 / 文件夹1.2.4. 在指定文件夹里不忽略指定的文件1.2.5. 使用通配符及其他符号1.3. 参考链接 原文地址 www.cnblogs.com 1.1. .gitignore文件的作用 .gitignore文件用来忽略被指定的文件或文件夹的改动，被记录在.gitignore文件里的文件或文件夹，是无法被 git 跟踪到的，换句话说，被忽略的文件是不会被放入到远程仓库里的。 如果文件已经存在于远程仓库中，是无法通过.gitignore文件来忽略的。 .gitignore文件存放于 git 仓库的根目录下。 1.2. .gitignore文件的语法 1.2.1. 注释 #表示注释，如下： # Here is comment. 1.2.2. 忽略文件 / 文件夹 直接写入文件或文件夹名即可，指定文件夹里的所有文件也会一起被忽略，如下： # Here is comment. # ignore target folder */.DS_Store # ignore Eclipse files .DS_Store 1.2.3. 不忽略文件 / 文件夹 !表示不忽略指定的文件，如下： # don't ignore src folder !src/ 1.2.4. 在指定文件夹里不忽略指定的文件 通过!可以实现更加有意思的用法，如下： # ignore scaffolds folder, but don't ignore draft.md under scaffolds folder. scaffolds/* !scaffolds/draft.md 注意：这里必须在文件夹后面加上/*，否则是无法实现。 1.2.5. 使用通配符及其他符号 可以使用通配符及其他符号来指定复杂条件的文件，如下： *.log day_1?.txt hello[0-9].txt *表示匹配任意字符； ?表示匹配一个字符； []表示匹配中括号内的单个字符： 可以使用-来表示连贯的字符，比如0-9，a-z，A-Z等，[0-9]表示匹配从 0 到 9 的单个字符。 可以使用^来表示除外，比如[^0-9]表示除 0 到 9 之外的单个字符。 1.3. 参考链接 .gitignore 规则写法 - 在已忽略文件夹中不忽略指定文件、文件夹【注意项】 "},"1 - Git_Github/5. 下载github特定子文件夹.html":{"url":"1 - Git_Github/5. 下载github特定子文件夹.html","title":"5. 下载github特定子文件夹","keywords":"","body":"我们在这里介绍两种下载文件的方式： 你当然可以克隆或者下载整个项目，但因为整个项目较大，可能对于一部分人不太方便。这里对于使用Chrome和Firefox的用户，我们推荐一个Chrome插件和Firefox插件，GitZip。该插件可以让我们只下载项目中我们感兴趣的那些文件夹，也就是我们所感兴趣的课程的资料。同时对Opera用户来说，大部分的Chrome插件都可以通过一款名为Install Chrome Extensions的Opera插件安装使用，GitZip也不例外。而对于使用其他浏览器的朋友来说，很遗憾，这个插件暂时只能在这三款浏览器中使用。 或者复制该文件夹的网址，粘贴入DownGit中，选择download即可。 注：对于第一种方法而言，它需要使用者拥有github账号，并且该插件需要使用者的授权；而第二种方法虽然一次只能下载一个文件夹，但它并不需要使用者拥有github账号。 "},"1 - Git_Github/6. git branch.html":{"url":"1 - Git_Github/6. git branch.html","title":"6. git branch","keywords":"","body":"1.1. 分支的概念1.2. 实际操作 原文地址 blog.csdn.net 改名branch，git原生分支master，github原生分支main，改名后提到main中 git branch -M main git push -u origin main git branch //查看本地所有分支 git branch -r //查看远程所有分支 git branch -a //查看本地和远程的所有分支 git branch //新建分支 git branch -d //删除本地分支 git branch -d -r //删除远程分支，删除后还需推送到服务器 git push origin: //删除后推送至服务器 git branch -m //重命名本地分支 //git中一些选项解释: -d --delete：删除 -D --delete --force的快捷键 -f --force：强制 -m --move：移动或重命名 -M --move --force的快捷键 -r --remote：远程 -a --all：所有 1.1. 分支的概念 在介绍两种方法之前，我们需要先了解一下分支的概念： 分支是用来标记特定代码的提交，每一个分支通过 SHA1sum 值来标识，所以对分支的操作是轻量级的，你改变的仅仅是 SHA1sum 值。 如下图所示，当前有 2 个分支，A,C,E 属于 master 分支，而 A、B、D、F 属于 dev 分支。 A----C----E（master） \\ B---D---F(dev) 它们的 head 指针分别指向 E 和 F，对上述做如下操作： git checkout master //选择or切换到master分支 git merge dev //将dev分支合并到当前分支(master)中 合并完成后： A---C---E---G(master) \\ / B---D---F（dev） 现在 ABCDEFG 属于 master，G 是一次合并后的结果，是将 E 和Ｆ的代码合并后的结果，可能会出现冲突。而 ABDF 依然属于 dev 分支。可以继续在 dev 的分支上进行开发: A---C---E---G---H(master) \\ / B---D---F---I（dev） 1.2. 实际操作 1 查看本地分支 $ git branch * br-2.1.2.2 master 2 查看所有（含远程）分支 $ git branch -a # * 是当前branch，前两个是本地分支，其余几个是远程分支 * br-2.1.2.2 master remotes/origin/HEAD -> origin/master remotes/origin/br-2.1.2.1 remotes/origin/br-2.1.2.2 remotes/origin/br-2.1.3 remotes/origin/master 3 创建本地分支 $ git branch test $ git branch * br-2.1.2.2 master test 4 查看+切换分支到 test $ git branch * br-2.1.2.2 master test $ git checkout test Switched to branch 'test' $ git branch br-2.1.2.2 master * test 5 把分支推到远程分支 $ git push origin test 6 删除本地分支 git branch -d xxxxx $ git checkout br-2.1.2.2 Switched to branch 'br-2.1.2.2' $ git branch * br-2.1.2.2 master test $ git branch -d test Deleted branch test (was 17d28d9). $ git branch * br-2.1.2.2 master 补充说明： M 表示 从原来分支（上一次修改没有提交 br-2.1.2.2）带过来的修改 origin：远程仓库默认命名为origin "},"1 - Git_Github/7. git fetch & pull 详解.html":{"url":"1 - Git_Github/7. git fetch & pull 详解.html","title":"7. git fetch & pull 详解","keywords":"","body":"1.1. 1、Fetch&Pull的区别1.2. 2、git fetch 用法1.3. 3、git pull 用法 本文由 简悦 SimpRead 转码， 原文地址 www.cnblogs.com 1.1. 1、Fetch&Pull的区别 先用一张图来理一下git fetch和git pull的概念： 可以简单的概括为： git fetch是将远程主机的最新内容拉到本地，用户在检查了以后决定是否合并到工作本机分支中。 而git pull 则是将远程主机的最新内容拉下来后直接合并，即：git pull = git fetch + git merge，这样可能会产生冲突，需要手动解决。 下面我们来详细了解一下git fetch 和git pull 的用法。 1.2. 2、git fetch 用法 git fetch 命令： $ git fetch //这个命令将某个远程主机的更新全部取回本地 如果只想取回特定分支的更新，可以指定分支名： $ git fetch //注意之间有空格 最常见的命令如取回origin 主机的master 分支： $ git fetch origin master 取回更新后，会返回一个FETCH_HEAD ，指的是某个 branch 在服务器上的最新状态，我们可以在本地通过它查看刚取回的更新信息： $ git log -p FETCH_HEAD 我们可以通过这些信息来判断是否产生冲突，以确定是否将更新 merge 到当前分支。 1.3. 3、git pull 用法 前面提到，git pull 的过程可以理解为： git fetch origin master //从远程主机的master分支拉取最新内容 git merge FETCH_HEAD //将拉取下来的最新内容合并到当前所在的分支中 即将远程主机的某个分支的更新取回，并与本地指定的分支合并，完整格式可表示为： $ git pull : 如果远程分支是与当前分支合并，则冒号后面的部分可以省略： $ git pull origin next "},"1 - Git_Github/8. Gitbook与GitPages.html":{"url":"1 - Git_Github/8. Gitbook与GitPages.html","title":"8. Gitbook与GitPages","keywords":"","body":"1.1. 资源1.2. 思路1.2.1. 1. 安装1.2.2. 2. 初始化与本地使用1.2.3. 3. 目录结构 & 自动生成目录1.2.4. 4. 构建与部署1.2.5. 5. 进阶1.2.6. 6. github action 添加自动化1.3. 后续 1.1. 资源 Npm - gitbook ： https://www.npmjs.com/package/gitbook 基础教程：大师兄2020 - 播放列表 ： https://space.bilibili.com/31238770/channel/detail?cid=84070 打造完美写作系统：Gitbook+Github Pages+Github Actions：https://www.cnblogs.com/phyger/p/14035937.html 1.2. 思路 Gitbook - 由markdown自动生成可部署的web文件； Git&Github - 版本控制、发布仓库； Github pages - 免费部署网站； Github Action - 将markdown push到main分支，触发trigger，打包自动实现前三步 1.2.1. 1. 安装 依赖：node.js、npm npm install gitbook-cli -g 1.2.2. 2. 初始化与本地使用 gitbook init gitbook serve 1.2.3. 3. 目录结构 & 自动生成目录 Gitbook会Summary.md 生成index.md的目录边栏 根据目录生成Summary.md - 选用文中的方案2 https://blog.csdn.net/weixin_34383618/article/details/91629912 npm install -g gitbook-summary book sm 【注意】 summary.md只使用无序列表，否则会无法显示！！！ 重新生成 summary.md 需要删除_book 1.2.4. 4. 构建与部署 gitbook build git上传到相应的仓库·分支，开启Github Pages即可 1.2.5. 5. 进阶 book.json - 设置主题、插件、生成summary { \"title\": \"Summary\", \"plugins\" : [ \"expandable-chapters\", \"github-buttons\", \"copy-code-button\", \"anchor-navigation-ex\", \"-highlight\", \"-lunr\", \"-search\", \"search-pro\", \"splitter\" ], \"ignores\" : [\"_book\", \"node_modules\"] } 1.2.6. 6. github action 添加自动化 思路：Markdown文件上传到 master 分支中，html 上传到 gh-pages 分支中。 新建文件夹与文件: /.github/workflows/[配置.yml]：注意，把原文中的main改回master name: auto-generate-gitbook on: #在master分支上进行push时触发 push: branches: - master jobs: master-to-gh-pages: runs-on: ubuntu-latest steps: - name: checkout master uses: actions/checkout@v2 with: ref: master - name: install nodejs uses: actions/setup-node@v1 - name: configue gitbook run: | npm install -g gitbook-cli gitbook install npm install -g gitbook-summary - name: generate _book folder run: | book sm gitbook build cp SUMMARY.md _book - name: push _book to branch gh-pages env: TOKEN: $ REF: github.com/$ MYEMAIL: wyfsgm@gmail.com # ！！记得修改为自己github设置的邮箱 MYNAME: $ run: | cd _book git config --global user.email \"${MYEMAIL}\" git config --global user.name \"${MYNAME}\" git init git remote add origin https://${REF} git add . git commit -m \"Updated By Github Actions With Build $ of $ For Github Pages\" git push --force --quiet \"https://${TOKEN}@${REF}\" master:gh-pages 在settings -> Developer settings -> personal accsess tokens，复制得到token 粘贴到对应的repository - secrets 中 【注意！！】Github Pages build failure：https://blog.csdn.net/weixin_46831482/article/details/117532728 解决：Github误认为你的博客采用Jekyll，这时，需要在GitHub博客目录下建立一个为 .nojekyll的空文件 1.3. 后续 部署在云服务器上 选择样式 "},"1 - Git_Github/报错 - Failed to connect to localhost port 1080: Connection refused.html":{"url":"1 - Git_Github/报错 - Failed to connect to localhost port 1080: Connection refused.html","title":"报错 - Failed to connect to localhost port 1080: Connection refused","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 主要原因是因为使用了 proxy 代理，需要关闭代理。 git config --global http.proxy // 查看代理 结果为：localhost:1080 git config --global --unset http.proxy // 不设置代理 再拉取就没有问题了。 "},"2 - 操作系统_服务器与网络/1. Download Ubuntu 镜像文件.html":{"url":"2 - 操作系统_服务器与网络/1. Download Ubuntu 镜像文件.html","title":"1. Download Ubuntu 镜像文件","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com Ubuntu，是一款基于 Debian Linux 的以桌面应用为主的操作系统，内容涵盖文字处理、电子邮件、软件开发工具和 Web 服务等，可供用户免费下载、使用和分享。但是对于国内的用户来说如果直接从官网下载文件，那速度简直比蜗牛还慢，所以我个人还是比较推荐通过国内镜像站点进行下载的。本文我就主要为大家介绍一下国内的一些比较好用、更新及时的 Ubuntu 镜像站点并附上下载链接，大家可以根据自己的需要自由选择。首先，给大家贴一下官方的下载地址： 官方地址：https://www.ubuntu.com/download 中国官网：https://cn.ubuntu.com/ 接下来给大家详细介绍下几个好用的镜像站点。 清华镜像源： 下载地址：https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/ 使用帮助：https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ 中科大镜像源： 下载地址：http://mirrors.ustc.edu.cn/ubuntu/ http://mirrors.ustc.edu.cn/ubuntu-releases/ 使用帮助：http://mirrors.ustc.edu.cn/help/ubuntu.html 阿里云镜像源： 下载地址：https://mirrors.aliyun.com/ubuntu/ https://mirrors.aliyun.com/ubuntu-releases/ 使用帮助：https://developer.aliyun.com/mirror/ubuntu 腾讯软件源： 下载地址：https://mirrors.cloud.tencent.com/ubuntu/ https://mirrors.cloud.tencent.com/ubuntu-releases/ 使用帮助：https://mirrors.cloud.tencent.com/help/ubuntu.html "},"2 - 操作系统_服务器与网络/1. Ubuntu下的翻墙攻略.html":{"url":"2 - 操作系统_服务器与网络/1. Ubuntu下的翻墙攻略.html","title":"1. Ubuntu下的翻墙攻略","keywords":"","body":"代理软件 - Clash github repositiry：https://github.com/Dreamacro/clash 步骤：https://blog.csdn.net/mjzjz_c/article/details/116661398 下载clash for ubuntu - github repositiry：https://github.com/Dreamacro/clash 安装与配置：从macos替换配置文件（注意注意，这里有效的源文件叫做MDSS，但文件夹里存在config.yaml和Country.mmdb） 网络设置-系统设置： 服务器代理：A不代理、B系统代理、C自动设置、D手动配置 web管理工具：http://clash.razord.top/#/proxies 其他思路：SSR-electron GUI客户端，死活装不上，算了算了。 github repository：https://github.com/qingshuisiyuan/electron-ssr-backup 设置方法：https://github.com/qingshuisiyuan/electron-ssr-backup/blob/master/Ubuntu.md "},"2 - 操作系统_服务器与网络/1. 免疫挂断：nohup 和 & .html":{"url":"2 - 操作系统_服务器与网络/1. 免疫挂断：nohup 和 & .html","title":"1. 免疫挂断：nohup 和 & ","keywords":"","body":"1.1. 0. 测试代码1.2. 1. 测试代码 + &1.3. 2. nohup + 测试代码1.4. 3. nohup + 测试代码 + & 原文地址 mp.weixin.qq.com nohup 和 & 究竟有啥区别？ SIGNIN = Cltr+C SIGNUP = 关闭session &免疫SIGNIN（Cltr+C）；nohup免疫SIGNUP（关闭窗口 ） 1.1. 0. 测试代码 测试代码如下： 是一个输出 hello 与循环轮数的死循环程序，每输出一行就休眠 1 秒。 使用 ./a.out 前台运行程序，会是什么效果呢？ 程序每隔一秒会在终端输出一个字符串。 此时如果键入 Ctrl+C ，程序会收到一个 SIGINT 信号，如果不做特殊处理，程序的默认行为是终止（如上图）。 1.2. 1. 测试代码 + & 使用 ./a.out& 后台运行程序，会是什么效果呢？ 如上图： 首先会在终端显示进程号是 32389 键入 Ctrl + C，发出 SIGINT 信号，程序会继续运行 ps 确认一下，确认进程依然在运行，进程号是 32389。 此时如果关掉 session，程序会收到一个 SIGHUP 信号，此时会怎么样呢？ ps 再次确认，可以看到关闭 session 之后，进程号是 32389 的 a.out 进程也关闭了。 1.3. 2. nohup + 测试代码 使用 nohup ./a.out 又会是什么效果呢？ 使用 nohup 运行程序 a.out，会发现： 前台没有出现进程号 有一个 “忽略输入，输出至 nohup.out” 的提示 hello 的输出也没有出现在前台 手动 ps 看进程号，这次 a.out 的进程号是 32437。 此时如果关掉 session，程序会收到一个 SIGHUP 信号，程序会不会关闭呢？ 关掉 session 后，再次 ps 看一下，ID 为 32437 的 a.out 进程还在。 这些只能通过 kill 把程序干掉了，killall 之后，ps 查看进程已经关闭。 killall 之后，查看发现多了一个 nohup.out 文件，不过这个文件的大小是 0，有点奇怪，启动程序的时候，明明提示了 “appending output to nohup.out” 呀，先把问题遗留在这，测试一下 Ctrl +C。 仍如上图，使用 nohup 启动 a.out，如果键入 Ctrl+C ，程序收到 SIGINT 信号后，直接关闭了。 1.4. 3. nohup + 测试代码 + & 最后测试一下 nohup 和 & 同时使用，即用 nohup./a.out & 运行程序，又会是什么效果呢？ 使用 nohup ./a.out & 运行程序后，可以看到： 会在终端显示进程号是 32524 也会有一个 “忽略输入，输出至 nohup.out” 的提示 键入 Ctrl + C，发送 SIGINT 信号，似乎没反应。 关闭 session，发送 SIGHUP 信号，再来看看。 ID 为 32524 的进程依然存在，后续也只能用 kill 来关闭它。 结论 使用 & 后台运行程序： 结果会输出到终端 使用 Ctrl + C 发送 SIGINT 信号，程序免疫 关闭 session 发送 SIGHUP 信号，程序关闭 使用 nohup 运行程序： 结果默认会输出到 nohup.out 使用 Ctrl + C 发送 SIGINT 信号，程序关闭 关闭 session 发送 SIGHUP 信号，程序免疫 平日线上经常使用 nohup 和 & 配合来启动程序： 同时免疫 SIGINT 和 SIGHUP 信号 同时，还有一个最佳实践： 不要将信息输出到终端标准输出，标准错误输出，而要用日志组件将信息记录到日志里 "},"2 - 操作系统_服务器与网络/3. NATAPP 将内网映射到外网.html":{"url":"2 - 操作系统_服务器与网络/3. NATAPP 将内网映射到外网.html","title":"3. NATAPP 将内网映射到外网","keywords":"","body":" 原文地址 blog.csdn.net cd /Users/wangyangfan/Downloads ./natapp -authtoken=d7458447251eb80d 官网下载对应 natapp 客户端 natapp 官网 我下载的是 64 位的。下载完成之后是这个样子的。 授权 打开终端 cd natapp 的完整路径（可以直接将刚刚下载的 natapp 拖进来）chmod a+x natapp 然后再运行 ./natapp 可能会出现 那么问题是你没有注册注册账号 点击注册 登录后, 点击左边 购买隧道, 免费 / 付费均可 由于自己测试我使用免费的拿到 authtoken 终端运行 ./natapp -authtoken=你刚才拿到的值 运行成功, 都可以得到如下界面 将 natapp 分配的网址, 鼠标选定然后复制下来 (选定之后单击鼠标右键), 在浏览器中访问, 可以看到内网穿透成功了! 确保 http://127.0.0.1 可以访问才能成功 debug： 解决思路：这代表natapp客户端已经连接成功,你本地端口的Web服务没有开启或者端口不正确，可以用http://127.0.0.1 来确认已开启web服务 (不输入端口代表80) 比如 Apache默认端口是80，Tomat的默认端口是 8080。因此需要在隧道配置处,要修改为匹配的端口号,保存后重启客户端.用浏览器打开 "},"2 - 操作系统_服务器与网络/3. 实验室主机联网奇怪经历.html":{"url":"2 - 操作系统_服务器与网络/3. 实验室主机联网奇怪经历.html","title":"3. 实验室主机联网奇怪经历","keywords":"","body":"2021年7月10日 周六 最终解决方案: usb无线网卡，使用Wi-Fi连接网络 硬件购买：需要注意硬件支持的系统，ubuntu20 驱动安装 官方推荐：绿联USB2.0无线网卡RTL8811CU芯片驱动（Windows+Linux系统）丨CM448 有效博客：Ubuntu20安装RTL8811/RTL8812无线网卡驱动 背景：周四正式进432实验室，显示器&主机设备很不给力，显卡性能平平、输入输出都只能到VGA，从威富搬回来的4k显示器也救不了；网络环境也一言难尽。有线接口必须使用代理，无线连接也不稳定。给配备的主机上了win10+ubuntu双系统作为工程实验机，主要不愿过多使用自己的mbp。 问题：Ubuntu主机接网线不能使用网络 思路1：继续使用网线，效仿win10系统进行ss代理 代理软件问题 wine + windows下的exe，安装wine软件需要联网，悖论❌ 命令行式配置各种代理软件，成本太高❌ GUI使用代理软件（SS、SSR、V2Ray、Clash），简单可以考虑✅ 思路2：usb无线网卡，使用Wi-Fi连接网络✅ "},"2 - 操作系统_服务器与网络/3. 服务器与MacOS之间的SFTP.html":{"url":"2 - 操作系统_服务器与网络/3. 服务器与MacOS之间的SFTP.html","title":"3. 服务器与MacOS之间的SFTP","keywords":"","body":"1. SFTP服务搭建1.1. 1. linux服务器端1.1.1. 一、新建文件夹1.1.2. 二、创建用户和组：创建sftp组和mysftp用户1.1.3. 三、设置用户密码1.1.4. 四、指定为mysftp组用户的home目录1.1.5. 五、编辑配置文件/etc/ssh/sshd_config1.1.6. 六、设置Chroot目录权限1.1.7. 七、设置上传文件夹并授权1.1.8. 八、重启ssh1.2. 2. Macos客户端1. SFTP服务搭建 1.1. 1. linux服务器端 https://www.cnblogs.com/reachos/p/11157329.html 1.1.1. 一、新建文件夹 wyfubt@asdf1234:~$ mkdir data wyfubt@asdf1234:~$ cd data & mkdir sftp 1.1.2. 二、创建用户和组：创建sftp组和mysftp用户 wyfubt@asdf1234:~/data$ sudo useradd -g sftp -s /sbin/nologin -d /data/sftp/mysftp mysftp 1.1.3. 三、设置用户密码 wyfubt@asdf1234:~/data$ sudo passwd mysftp Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully 1.1.4. 四、指定为mysftp组用户的home目录 wyfubt@asdf1234:~/data$ sudo usermod -d /home mysftp 1.1.5. 五、编辑配置文件/etc/ssh/sshd_config wyfubt@asdf1234:~/data$ sudo vim /etc/ssh/sshd_config 将如下这行用#符号注释掉 # Subsystem sftp /usr/libexec/openssh/sftp-server 在文件末尾添加 Subsystem sftp internal-sftp Match Group sftp ChrootDirectory /home ForceCommand internal-sftp AllowTcpForwarding no X11Forwarding no 1.1.6. 六、设置Chroot目录权限 wyfubt@asdf1234:~/data$ sudo chown root:sftp /home wyfubt@asdf1234:~/data$ sudo chmod 755 /home 1.1.7. 七、设置上传文件夹并授权 wyfubt@asdf1234:~/data$ cd ../.. wyfubt@asdf1234:/home$ sudo mkdir upload wyfubt@asdf1234:/home$ chown mysftp:sftp upload wyfubt@asdf1234:/home$ sudo chown mysftp:sftp upload 1.1.8. 八、重启ssh wyfubt@asdf1234:/home$ service sshd restart 1.2. 2. Macos客户端 MacOS的Finder没有自带sftp管理工具，因此选择filezilla进行sftp文件传输。 下载链接：https://dl1.cdn.filezilla-project.org/client/FileZilla_3.53.1_macosx-x86.app.tar.bz2?h=5O5-nX76Qt-Anj9C7RWPig&x=1620299337 "},"2 - 操作系统_服务器与网络/3. 远程连接服务器 - Natapp内网穿透服务器.html":{"url":"2 - 操作系统_服务器与网络/3. 远程连接服务器 - Natapp内网穿透服务器.html","title":"3. 远程连接服务器 - Natapp内网穿透服务器","keywords":"","body":"1. 远程连接服务器 - Natapp内网穿透set服务器1.1. 前言1.2. 零、关于 Natapp1.3. 一、服务器：下载&权限 NatApp1.4. 二、WEB：NatApp官网设置隧道1.5. 三、服务器：运行NatApp1.6. 四、客户端：远程登录1.7. 五、进阶 - 批处理&挂起1.7.1. 批处理1.7.2. 挂起1.7.3. 定位进程+杀进程1. 远程连接服务器 - Natapp内网穿透set服务器 一句指令启动内网穿透：nohup bash natstart.sh 1.1. 前言 环境：工程环境 - wavekingdom 服务器 Ubuntu 18.05 ；客户机 - Shell 远程连接 思路：将公司服务器暴露于公网，给远程连接提供基础。注意，用于ssh远程连接使用TCP协议，并且端口号设置为22 工具：在服务器上下载运行 NatApp - NatApp官网 https://natapp.cn设置隧道 - 1.2. 零、关于 Natapp Natapp 是基于 ngrok 的国内收费内网穿透工具， 免费版本：提供 http,https,tcp 全隧道穿透、随机域名、TCP 端口、不定时强制更换域名 / 端口、自定义本地端口。 1.3. 一、服务器：下载&权限 NatApp 选定目录 pwd cd /users/local/ 下载NatApp # 网页获取到下载链接 # 保存并另存为“Natapp wget -O \"natapp\" \"https://cdn.natapp.cn/assets/downloads/clients/2_3_9/natapp_linux_amd64/natapp?version=20190730\" 在 Linux/Mac 下 需要先给执行权限 sudo chmod a+x natapp 1.4. 二、WEB：NatApp官网设置隧道 首先注册一个账户（需要用支付宝实名认证一下），选择购买隧道 - 免费购买。 因为 SSH 是基于 TCP 协议的，所以隧道协议选择 TCP，注意本地端口选择 22 点击免费购买后，可以得到一个 authtoken 1.5. 三、服务器：运行NatApp 然后运行 sudo ./natapp -authtoken=657329cb9e2e1153/de714e7ebdbbf2d5 运行效果（显示 Online 表示成功暴露） 1.6. 四、客户端：远程登录 远程登陆，实现内网穿透 ssh myuser@s1.natapp.cc -p 6553 1.7. 五、进阶 - 批处理&挂起 1.7.1. 批处理 新建脚本 touch natstart.sh 写入命令 vim natstart.sh cd app ./natapp -authtoken=de714e7ebdbbf2d5 执行命令 sh natstart.sh 1.7.2. 挂起 #免疫 关闭session nohup bash natstart.sh -log=stdout 1.7.3. 定位进程+杀进程 # 方式1 # 定位程序的进程 ps -ef|grep natapp # kill 特定id的进程 kill -9 3827 # 方式2 # kill 程序的所有进程 killall -9 natapp 参考 https://natapp.cn/article/natapp_newbie https://natapp.cn/article/tcp https://blog.csdn.net/Return_0_/article/details/86745225 natapp.cn www.linuxprobe.com "},"2 - 操作系统_服务器与网络/4. CMake3分钟上手.html":{"url":"2 - 操作系统_服务器与网络/4. CMake3分钟上手.html","title":"4. CMake3分钟上手","keywords":"","body":"1. CMake 3分钟上手1. CMake 3分钟上手 外部链接 为什么需要用Cmake？ ~ 为了实现大型项目的编译 当程序规模越来越大时，一个工程可能 有许多个文件夹和源文件，这时输入的编译命令将越来越长。通常一个小型 C++ 项目可能含有十几个类，各类间还存在着复杂的依赖关系。其中一部分要编译成可执行文件，另一部分编译成库文件。 如果仅靠 g++ 命令，我们需要输入大量的编译指令，整个编译过程会变得异常烦琐。因此，对于 C++ 项目，使用一些工程管理工具会更加高效。cmake 在工程上广泛使用，我们会看到后面提到的大多数库都 使用 cmake 来管理源代码。 案例：如何使用Cmake进行编译？ ~ 1⃣️ 新建一个 CMakeLists.txt；2⃣️ 用 cmake 命令生成一个 makefile 文件；3⃣️用 make 命令根据这个 makefile 文件的内容编译整个工程。 安装cmake apt-get install cmake 新建一个 CMakeLists.txt，把cmake编译信息放在其中 # 声明要求的 cmake 最低版本 cmake_minimum_required(VERSION 2.8) cmake_minimum_required(VERSION 2.8) # 声明一个cmake工程project(HelloSLAM) project(HelloSLAM) # 添加一个可执行程序 add_executable(helloSLAM helloSLAM.cpp) 调用 cmake 对该工程进行 cmake 编译，用 make 命令对工程进行编译 cmake . make 得到在 CMakeLists.txt 中声明的可执行程序 helloSLAM,并执行它 ./helloSlam 解决：cmake编译时，中间文件过多的问题 cmake 生成的中间文件还留在代码文件当中。一种更好的做法是把中间文件和源代码进行分离，让这些中间文件都放在一个中间目录中，在编译成功后，把这个中间目录删除即可。所以，更常见的编译 cmake 工程的做法如下： mkdir build cd build cmake .. make 我们新建了一个中间文件夹“build”，然后进入 build 文件夹，通过 cmake .. 命令对上一层文件夹，也就是代码所在的文件夹进行编译。这样，cmake 产生的中间文件就会生成在 build 文件夹中， 与源代码分开。当发布源代码时，只要把 build 文件夹删掉即可。请读者自行按照这种方式对 ch2 中 的代码进行编译，然后调用生成的可执行程序（请记得把上一步产生的中间文件删掉）。 对比Cmake和g++的区别，Cmake的意义 虽然这个过程中多了调用 cmake 和 make 的步骤，但我们对项目的编译管理工作，从输入一串 g++ 命令，变成了维护若干个比较直观的 CMakeLists.txt 文件，这将明显降低维护整个工程的难度。比如，如果想新增一个可执行文件，只需在 CMakeLists.txt 中添加一行“add_executable”命令即可，而后续的步骤是不变的。cmake 会帮我们解决代码的依赖 关系，而无须输入一大串 g++ 命令。 库文件与调用 程序=头文件+源文件；如果源文件中含有main函数，它可以被编译为可执行程序；不含有main，则被编译为库文件。 使用cmake生成并使用库的步骤有： cmake编译生成静态库/共享库：CMakeLists.txt、cmake、make 为库文件新建头文件，规范调用的格式 在可执行程序的源代码中调用库文件：include头文件，参考头文件的格式书写调用语句 cmake将可执行程序链接到库上：CMakeLists.txt、cmake、make 演示： Step 1. cmake编译生成静态库/共享库 库文件的源代码 - libHelloSLAM.cpp #include using namespace std; void printHello(){ //不含main函数 cout CMakeList.txt // 如果编译成静态库 add_library(hello libHelloSLAM.cpp) //如果编译成共享库 add_library( hello_shared SHARED libHelloSLAM.cpp ) 库文件分成静态库和共享库两种 À 。静态库以.a 作为后缀名，共享库以.so 结尾。所有库都是一些函数打包后的集合，差别在于静态库每次被调用都会生成一个副本，而共享库则只有 一个副本，更省空间。 终端输入： mkdir build cd build cmake .. make 输出的静态库：libhello.a；输出的共享库：libhello_shared.so Step 2. 为库文件新建头文件 libHelloSLAM.h #ifndef LIBHELLOSLAM_H_ #define LIBHELLOSLAM_H_ //上面的宏定义是为了防止重复引用这个头文件而引起的重定义错误 //打印一句hello的函数void printHello(); void printHello(); #endif Step 3. 在可执行程序的源代码中调用库文件：include头文件，参考头文件的格式书写调用语句useHello.cpp #include \"libHelloSLAM.h\" //inclued 头文件 // 使用 libHelloSLAM.h 中的 printHello() 函数 int main(int argc, char ∗∗argv) { printHello(); return 0; } step 4. 可执行程序链接到库上 CMakeList.txt add_executable(useHello useHello.cpp) target_link_libraries(useHello hello_shared) "},"2 - 操作系统_服务器与网络/4. Mac 下安装 OpenCV.html":{"url":"2 - 操作系统_服务器与网络/4. Mac 下安装 OpenCV.html","title":"4. Mac 下安装 OpenCV","keywords":"","body":"1.1. 谁也不在意的一段原文··· 原文地址 blog.csdn.net 安装OpenCV的集中方式： 手动方式：在官网下载包，安装，编译··· - 官网 (https://docs.opencv.org/master/d0/db2/tutorial_macos_install.html)（ https://u.nu/l3s11）https://blog.csdn.net/sinat_38221796/article/details/80230645?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control 包管理器：pip、homebrew、Conda -（https://blog.csdn.net/wo164683812/article/details/80114999） 集成环境：Anaconda IDE：Pycharm 结论：homebrew装上了，Pycharm、Anaconda都很慢 1.1. 谁也不在意的一段原文··· 因为当时学习 python 的时候不想使用 anoconda 这种集成环境，我始终觉得这种集成环境来开发代码感觉不明不白的，感觉心里没底，不知道这个环境是怎样配置的，把一些下载的包放在哪里了，反正就是一系列的问题。 所以我义无反顾的选择了看上去比较简单粗暴的方法。直接在系统中安装 python3，然后结合 sublime 编写 python 程序，通过 pip3 install xxx / pip3 list 这种方式让我明确知道我自己安装了哪些包。 但是最近学习 opencv 的时候我 google 了一下，貌似 Mac 上装 opencv 还比较麻烦，详情参考这篇文章：https://blog.csdn.net/pyufftj/article/details/79808693 使用 anoconda 安装非常方便，简单的就动动手指就行了，这里就不多说了。 不使用 anoconda 安装就比较麻烦了，首先要安装 homebrew，以下为详细步骤： 我寻思着不对，不应该这么麻烦，直接打开控制台，直接 pip3 install opencv-python 后： 提示我由于权限的问题并没有安装成功, 加上 --user 就行了： pip3 install opencv-python --user ------------------------------- 还有一种方式 -------------------------------------- 通过 pycharm 来安装 "},"2 - 操作系统_服务器与网络/软硬链接 与 Linux 文件系统.html":{"url":"2 - 操作系统_服务器与网络/软硬链接 与 Linux 文件系统.html","title":"软硬链接 与 Linux 文件系统","keywords":"","body":"1. 理解 inode1.1. 一、索引节点 inode1.2. 二、目录文件1.3. 三、硬链接1.3.1. 最后我们来做个总结： 原文地址 blog.csdn.net 1. 理解 inode 1.1. 一、索引节点 inode inode 是一个重要概念，是理解 Unix/Linux 文件系统和硬盘储存的基础。 我觉得，理解 inode，不仅有助于提高系统操作水平，还有助于体会 Unix 设计哲学，即如何把底层的复杂性抽象成一个简单概念，从而大大简化用户接口。 理解 inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做 \"扇区\"（Sector）。每个扇区储存 512 字节（相当于 0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个 \"块\"（block）。这种由多个扇区组成的 \"块\"，是文件存取的最小单位。 \"块\" 的大小，最常见的是 4KB，即连续八个 sector 组成一个 block。 文件数据都储存在 \"块\" 中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做 inode，中文译名为 \"索引节点\"(inode)。 每一个文件都有对应的 inode，里面包含了与该文件有关的一些信息。 二、inode 的内容 inode 包含文件的元信息，具体来说有以下内容： 　　* 文件的字节数 　　* 文件拥有者的 User ID 　　* 文件的 Group ID 　　* 文件的读、写、执行权限 　　* 文件的时间戳，共有三个：ctime 指 inode 上一次变动的时间，mtime 指文件内容上一次变动的时间，atime 指文件上一次打开的时间。 　　* 链接数，即有多少文件名指向这个 inode 　　* 文件数据 block 的位置 可以用 stat 命令，查看某个文件的 inode 信息： stat example.txt 总之，除了文件名以外的所有文件信息，都存在 inode 之中。至于为什么没有文件名，下文会有详细解释。 三、inode 的大小 inode 也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。1⃣️一个是数据区，存放文件数据；2⃣️另一个是 inode 区（inode table），存放 inode 所包含的信息。inode 节点的总数，在格式化时就给定。 每个 inode 节点的大小，一般是 128 字节或 256 字节。一般是每 1KB 或每 2KB 就设置一个 inode。假定在一块 1GB 的硬盘中，每个 inode 节点的大小为 128 字节，每 1KB 就设置一个 inode，那么 inode table 的大小就会达到 128MB，占整块硬盘的 （1:8）。 查看每个硬盘分区的 inode 总数和已经使用的数量，可以使用 df 命令。 df -i 查看每个 inode 节点的大小，可以用如下命令： sudo dumpe2fs -h /dev/hda | grep \"Inode size\" 由于每个文件都必须有一个 inode，因此有可能发生 inode 已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。（inode>file_num） 四、inode 号码 每个 inode 都有一个号码，操作系统用 inode 号码来识别不同的文件。 这里值得重复一遍，Unix/Linux 系统内部不使用文件名，而使用 inode 号码来识别文件。对于系统来说，文件名只是 inode 号码便于识别的别称或者绰号。 表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的 inode 号码；其次，通过 inode 号码，获取 inode 信息；最后，根据 inode 信息，找到文件数据所在的 block，读出数据。 使用 ls -i 命令，可以看到文件名对应的 inode 号码： ls -i example.txt 1.2. 二、目录文件 Unix/Linux 系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。 目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的 inode 号码。 ls 命令只列出目录文件中的所有文件名： ls /etc ls -i 命令列出整个目录文件，即文件名和 inode 号码： ls -i /etc 如果要查看文件的详细信息，就必须根据 inode 号码，访问 inode 节点，读取信息。ls -l 命令列出文件的详细信息。 ls -l /etc 理解了上面这些知识，就能理解目录的权限。目录文件的读权限（r）和写权限（w），都是针对目录文件本身（即不同用户能以什么权限访问操作对该目录文件，例如这里不同用户对 tmp 目录文件（d 可以查出 tmp 是目录文件，d 表示 directory，即目录）分别为 rwxr-xr-x，第一组的三个字符，即 rwx，表示文件拥有者用户的对该文件的读写权限，第二组的三个字符，即 r-x，表示文件拥有者用户所在的用户组里的其他用户对该文件的读写权限，第三组的三个字符，即 r-x，表示文件拥有者用户所在的用户组以外的用户对该文件的读写权限。一个某个用户下运行的进程访问操作该目录文件只能以该用户所具有的对该目录文件的权限进行操作）。 由于目录文件内只有文件名和 inode 号码，所以如果只有读权限，只能获取文件名，无法获取其他信息，因为其他信息都储存在 inode 节点中，而读取 inode 节点内的信息需要目录文件的执行权限（x）。 1.3. 三、硬链接 一般情况下，文件名和 inode 号码是 \"一一对应\" 关系，每个 inode 号码对应一个文件名。 但是，Unix/Linux 系统允许，多个文件名指向同一个 inode 号码。 这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为 \"硬链接\"（hard link）。 ln 命令可以创建硬链接： 　　ln 源文件 目标文件 运行上面这条命令以后，源文件与目标文件的 inode 号码相同，都指向同一个 inode。inode 信息中有一项叫做 \"链接数\"，记录指向该 inode 的文件名总数，这时就会增加 1。 反过来，删除一个文件名，就会使得 inode 节点中的 \"链接数\" 减 1。当这个值减到 0，表明没有文件名指向这个 inode，系统就会回收这个 inode 号码，以及其所对应 block 区域。 这里顺便说一下目录文件的 \"链接数\"。创建目录时，默认会生成两个目录项：\".\" 和 \"..\"。前者的 inode 号码就是当前目录的 inode 号码，等同于当前目录的 \"硬链接\"；后者的 inode 号码就是当前目录的父目录的 inode 号码，等同于父目录的 \"硬链接\"。所以，任何一个目录的 \"硬链接\" 总数，总是等于 2（某一目录的目录名和该目录的当前目录名）~加上它的子目录总数（含隐藏目录）。（因为 inode 信息中有一项叫做 \"链接数\"，记录指向该 inode 的文件名总数）~ 七、软链接 除了硬链接以外，还有一种特殊情况。 文件 A 和文件 B 的 inode 号码虽然不一样，但是文件 A 的内容是文件 B 的路径。读取文件 A 时，系统会自动将访问者导向文件 B。因此，无论打开哪一个文件，最终读取的都是文件 B。这时，文件 A 就称为文件 B 的 \"软链接\"（soft link）或者 \" 符号链接（symbolic link）。 这意味着，文件 A 依赖于文件 B 而存在，如果删除了文件 B，打开文件 A 就会报错：\"No such file or directory\"。这是软链接与硬链接最大的不同：文件 A 指向文件 B 的文件名，而不是文件 B 的 inode 号码，文件 B 的 inode\"链接数\" 不会因此发生变化。 ln -s 命令可以创建软链接。 　　ln -s 源文文件或目录 目标文件或目录 八、inode 的特殊作用 由于 inode 号码与文件名分离，这种机制导致了一些 Unix/Linux 系统特有的现象。 　　1. 有时，文件名包含特殊字符，无法正常删除。这时，直接删除 inode 节点，就能起到删除文件的作用。 　　2. 移动文件或重命名文件，只是改变文件名，不影响 inode 号码。 　　3. 一个文件打开并运行以后，系统就以 inode 号码来识别这个文件，不再考虑文件名。因此，通常来说，系统无法从 inode 号码得知文件名（无法反推）。 第 3 点使得软件更新变得简单，可以在不关闭软件的情况下进行更新，不需要重启：识别到运行中的文件，系统通过 inode 号码而不通过文件名；更新的时候，新版文件以同样的文件名，生成一个新的 inode，不会影响到运行中的文件。等到下一次运行这个软件的时候，文件名就自动指向新版文件，旧版文件的 inode 则被回收。 附加： 二、硬链接和软链接 其中每个 dentry 都有一个唯一的 inode，而每个 inode 则可能有多个 dentry，这种情况是由 ln 硬链接产生的。** 硬链接：其实就是同一个文件具有多个别名，具有相同 inode，而 dentry 不同。 1. 文件具有相同的 inode 和 data block； 2. 只能对已存在的文件进行创建； 3. 不同交叉文件系统进行硬链接的创建 4. 不能对目录进行创建，只能对文件创建硬链接 5. 删除一个硬链接并不影响其他具有相同 inode 号的文件； 软链接：软链接具有自己的 inode，即具有自己的文件，只是这个文件中存放的内容是另一个文件的路径名。因此软链接具有自己的 inode 号以及用户数据块。 1. 软链接有自己的文件属性及权限等； 2. 软链接可以对不存在的文件或目录创建； 3. 软链接可以交叉文件系统； 4. 软链接可以对文件或目录创建； 5. 创建软链接时，链接计数 i_nlink 不会增加； 6. 删除软链接不会影响被指向的文件，但若指向的原文件被删除，则成死链接，但重新创建指向 的路径即可恢复为正常的软链接，只是源文件的内容可能变了。 http://blog.chinaunix.net/uid-14518381-id-3957854.html **一、文件分配方式是索引分配时的文件系统结构（粗略的说，是分区结构）：** 一个文件系统里的文件分为目录文件和普通文件这两类。 如果文件分配方式是索引分配的话，则有索引节点这个概念的出现。 inode 也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是 inode 区（inode table），存放 inode 所包含的信息。 每个 inode 节点的大小，一般是 128 字节或 256 字节。inode 节点的总数，在格式化时就给定，一般是每 1KB 或每 2KB 就设置一个 inode。假定在一块 1GB 的硬盘中，每个 inode 节点的大小为 128 字节，每 1KB 就设置一个 inode，那么 inode table 的大小就会达到 128MB，占整块硬盘的 12.8%。 查看每个硬盘分区的 inode 总数和已经使用的数量，可以使用 df 命令：df -i 2、分区 （1）分区结构 **分区（partition）在被 Linux 的文件系统（比如 ext2）格式化的时候，会分成 inode table 和 block table 两部分，且大小都是固定的。**该分区的所有 inode 都在 inode table 里，所有 block 都在 block table 里。 http://blog.csdn.net/poechant/article/details/7214926 文件、目录、目录项、索引节点、超级块 如上的几个概念在磁盘中的位置关系如图 4 所示。 图 4. 磁盘与文件系统 目录块里存放的是一个个的 FCB（文件控制块，一个一般 128 字节）【FCB 就是目录文件存放的业务数据】，而数据块里存放的是普通文件的业务数据。普通文件由目录块里的一个 FCB 加上多个数据块组成，而目录文件由目录块里的一个 FCB 加上多个其他多个目录块组成。一个索引节点只能被一个文件（无论是目录文件，还是普通文件）所用，不能同时被其他文件所用。一个目录块里只能存放位于目录树里处于同级的文件（无论是目录文件，还是普通文件），所以一个根目录文件的 FCB 所在的目录块只能存放根目录文件的 FCB，与根目录文件同级的只有根目录文件自己。一个文件的 FCB 指向他的索引节点，他的索引节点指向该文件所拥有的块（如果该文件是目录文件，则该文件所拥有的块就是目录块；如果该文件是普通文件，则该文件所拥有的块就是数据块；） http://www.ibm.com/developerworks/cn/linux/l-cn-vfs/ http://blog.chinaunix.net/uid-14518381-id-3957854.html Superblock 是文件系统最基本的元数据，它定义了文件系统的类似、大小、状态，和其他元数据结构的信息（元数据的元数据）。Superblock 对于文件系统来说是非常关键的，因此对于每个文件系统它都冗余存储了多份。Superblock 对于文件系统来说是一个非常 “高等级” 的元数据结构。例如，如果 /var 分区的 Superblock 损坏了，那么 /var 分区将无法挂载。在这时候，一般会执行 fsck 来自动选择一份 Superblock 备份来替换损坏的 Superblock，并尝试修复文件系统。主 Superblock 存储在分区的 block 0 或者 block 1 中，而 Superblock 的备份则分散存储在文件系统的多组 block 中。当需要手工恢复时，我们可以使用 dumpe2fs /dev/sda1 | grep -i superblock 来查看 sda1 分区的 superblock 备份有哪一份是可用的。我们假设 dumpe2fs 输出了这样一行：Backup superblock at 163840, Group descriptors at 163841-163841 ，通过这条信息，我们就可以尝试使用这个 superblock 备份：/sbin/fsck.ext3 -b 163840 -B 1024 /dev/sda1。请注意，这里我们假设 block 的大小为 1024 字节。 http://www.elmerzhang.com/2012/12/suerblock-inode-dentry-file-of-filesystem/ ======================================= 1.3.1. 最后我们来做个总结： 1、一个 Inode 对应一个文件，而一个文件根据其大小，会占用多块 blocks。2、更为准确的来说，一个文件只对应一个 Inode。因为硬链接其实不是创建新文件，只是在 Directory 中写入了新的对应关系而已。3、当我们删除文件的时候，只是把 Inode 标记为可用，文件在 block 中的内容是没有被清除的，只有在有新的文件需要占用 block 的时候，才会被覆盖。 本文关键词：一天一点, 学习 Linux,Inode,ln,Inode 详解, 软链接, 硬链接 Inode Inode 详解 ln 一天一点 学习 Linux 硬链接 软链接 "},"3 - 环境搭建_开发效率/2. Conda_ENV中正确使用pip Install.html":{"url":"3 - 环境搭建_开发效率/2. Conda_ENV中正确使用pip Install.html","title":"2. Conda_ENV中正确使用pip Install","keywords":"","body":"1. 在conda_ENV中正确使用pip Install1.1. 问题描述1.2. 解决方案1.3. pip与conda install的区别1.4. 相关阅读1. 在conda_ENV中正确使用pip Install 1.1. 问题描述 经常能看到以下的代码： conda create env_name # 创建 conda activate env_name # 激活 pip install -r requirements.txt # 在环境中安装依赖 我们激活虚拟环境，在虚拟环境中使用pip install package，但实验发现： conda list 没有在当前环境下安装包 which pip发现，使用的pip在主环境下：/Users/wangyangfan/opt/anaconda3/bin/pip 虽然conda install package 仍然可以顺利安装到环境中，但这显然不是我们想要的效果 那么，在已激活的虚拟环境中，如何使用pip install安装到该环境中呢？ 1.2. 解决方案 conda create env_name # 创建 conda activate env_name # 激活 conda install pip # 安装pip pip install -r requirements.txt # 在环境中安装依赖 在line3添加\"conda install pip\"在虚拟环境中安装pip 那么再使用pip install，则会在当前环境中而非主环境中安装package which pip：/Users/wangyangfan/opt/anaconda3/envs/test/bin/pip 可以看到，当前pip已经在虚拟环境中 1.3. pip与conda install的区别 Pip install只是安装你需要安装的那个包本身。 Conda install 除了安装需要安装的包本身，还会自动安装所需包的依赖。 比如说安装tensorflow-gpu： 用pip install的话只是安装了tensorflow-gpu这个包，但是它的依赖（比如说cuda这些）你需要手动安装，而且版本配置不对的话，可能导致tensorflow-gpu无法使用。 用conda install安装的话，除了安装tensorflow-gpu这个包，它会自动检测这个包所需的依赖的情况，如果没有，会自动安装一遍。 简而言之，pip install之后，这个包可能依旧不能使用。而conda install安装之后，这个包就可以正常使用。 所以推荐优先conda install 1.4. 相关阅读 每次在虚拟环境内部使用pip，需要获取虚拟环境内部的 pip命令路径：/Users/stefanannihilater/anaconda3/envs/setests3/bin/pip，显然这样非常的繁琐。 /Users/stefanannihilater/anaconda3/envs/setests3/bin/pip install tagging 如果想在虚拟环境内部使用简单的pip install package调用虚拟环境内部的pip命令的话，只需要我们在创建虚拟环境的时候指定pip只对虚拟环境生效，而不影响全局库： conda create -n 虚拟环境名 pip pysocks pip：是为了指定pip命令只对当前虚拟环境生效 pysocks：是pip命令的依赖，如果不写，在虚拟环境内使用 pip命令的时候会出现Missing dependencies for SOCKS support.的报错。 之后再使用 pip命令就会默认使用虚拟环境中的 pip命令（/Users/stefanannihilater/anaconda3/envs/setests3/bin/pip） 在 conda中使用 pip，需要先开启虚拟环境，并确保该虚拟环境安装了 pip、pysocks包，如果没有安装这两个包，请使用 conda命令安装。 "},"3 - 环境搭建_开发效率/2. Conda添加镜像 - 解决下载过慢.html":{"url":"3 - 环境搭建_开发效率/2. Conda添加镜像 - 解决下载过慢.html","title":"2. Conda添加镜像 - 解决下载过慢","keywords":"","body":"1. 添加镜像源命令2. 显示所有镜像通道路径命令3. 清除添加的镜像源 原文地址 blog.csdn.net 博主在学习使用 conda 命令安装 torch 包时写下此篇博文，具体安装方法详见：使用 pip 和 conda 命令 安装 torch 包 考虑添加国内的镜像源，常用的镜像站有清华大学开源镜像站、中国科学技术大学开源镜像站和阿里巴巴开源镜像站： 1. 添加镜像源命令 注意添加的顺序，会影响检索的顺序，conda install 会优先检索后添加的源。类似于数据结构里栈原理，后入栈的置顶。例如：依次添加 A，B，C；会优先检索 C，然后 B，然后 A，最后是默认的镜像源。如果 C 里有包，就不再检索后面的了，所以也可以理解成更换镜像源。 添加清华大学镜像源命令： conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 添加中科大镜像源命令： conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/ 添加阿里镜像源命令： conda config --add channels https://mirrors.aliyun.com/pypi/simple/ 添加镜像源后建议再加一句显示检索路径的命令，这样每次安装包时会将包源路径显示出来： conda config --set show_channel_urls yes 2. 显示所有镜像通道路径命令 conda config --show channels 3. 清除添加的镜像源 原本我想使用 conda 命令通过清华的https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/这一镜像源导入 pytorch 包，目前我只看到这一国内镜像源里面有 pytorch 包，但此通道目前也已无法访问。 移除命令：（注此命令会清除所有用户添加的镜像源路径，只保留默认的路径 repo.anaconda.com 开头的） conda config --remove-key channels pip 命令下载缓慢问题解决方法详见：点击查看 参考：清华开源软件镜像站 Anaconda 镜像使用帮助：点此查看中科大镜像源：点此查看博客：点此查看 "},"3 - 环境搭建_开发效率/2. Linux 系统下 conda 的安装与使用.html":{"url":"3 - 环境搭建_开发效率/2. Linux 系统下 conda 的安装与使用.html","title":"2. Linux 系统下 conda 的安装与使用","keywords":"","body":"1.1. 1. wget 下载安装包1.2. 2 . 安装命令1.3. 3. 配置环境变量1.4. 4. 验证1.5. 5. 添加清华大学的镜像源1.6. 6 . 进入环境安装依赖包1.7. 7. 换回 conda 默认源 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1. 1. wget 下载安装包 wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh 1.2. 2 . 安装命令 chmod 777 Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh 1.3. 3. 配置环境变量 输入 conda 命令，如未成功输出，见下图 使用 vim 工具，编辑. bashrc 文件, 在最下行输入 miniconda3 的安装目录作为环境变量，与上面保存的安装目录相同 # 注意没有引号，同时路径要写到bin目录下 [wrong]export PATH=\"/root/miniconda3/bin:\"$PATH [right]export PATH=/home/wangyangfan/miniconda3/bin:$PATH source ~/.bashrc 1.4. 4. 验证 输入 conda 命令，如正常返回，说明 conda 安装成功 1.5. 5. 添加清华大学的镜像源 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ conda config --set show_channel_urls yes conda config --get channels 1.6. 6 . 进入环境安装依赖包 进入环境后，可使用如下命令安装依赖的包，使用的是已经配置好的清华的源，这里以 “opencv-python” 包为例，由于使用了清华大学的镜像源，下载速度很快。 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python 1.7. 7. 换回 conda 默认源 conda config --remove-key channels "},"3 - 环境搭建_开发效率/2. Mac 必备神器 Homebrew.html":{"url":"3 - 环境搭建_开发效率/2. Mac 必备神器 Homebrew.html","title":"2. Mac 必备神器 Homebrew","keywords":"","body":" 原文地址 zhuanlan.zhihu.com Homebrew 基本用法： 假设需要安装的软件是 wget 安装 brew： git clone git://mirrors.ustc.edu.cn/homebrew-core.git//usr/local/Homebrew/Library/Taps/homebrew/homebrew-core --depth=1 （注意：如果有 / usr/local/Homebrew/Library/Taps/homebrew/homebrew-core 目录，可以不执行，也可以，直接把这个目录删掉，再执行） 安装 brew cask： git clone git://mirrors.ustc.edu.cn/homebrew-cask.git//usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask --depth=1 （注意：如果有 / usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask 目录，可以不执行，也可以，直接把这个目录删掉，再执行） 替换成国内源： cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-cask\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git 1、Homebrew 是什么？ 引用官方的一句话：Homebrew 是 Mac OS 不可或缺的套件管理器。 Homebrew 是一款 Mac OS 平台下的软件包管理工具，拥有安装、卸载、更新、查看、搜索等很多实用的功能。简单的一条指令，就可以实现包管理，而不用你关心各种依赖和文件路径的情况，十分方便快捷。 2、Homebrew 的安装方法 官网给出的安装方法：将以下命令粘贴到终端 /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 但这种方法并不适用国内的 Mac 用户，因为网络资源的原因，电脑下载是龟速，实在是无法忍受，不信你自己试试就知道了。 解决下载慢有两个办法： 一是替换镜像源，将下载资源改为国内镜像资源即可（推荐） 二是科学上网，通过全局代理来进行安装，也是解决网络问题的一种方法（不推荐，不爱喝茶） 下面来说一下，怎样替换镜像源： 步骤一： 获取 install 文件：将以下命令粘贴到终端 + 回车 curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install >> brew_install 步骤二：更改文件中的链接资源，将原有的链接资源替换成清华大学的镜像资源 把这两句用 #注释掉 BREW_REPO = “https://github.com/Homebrew/brew“.freeze CORE_TAP_REPO = “https://github.com/Homebrew/homebrew-core“.freeze 修改为这两句 BREW_REPO = \"git://mirrors.ustc.edu.cn/brew.git\".freeze CORE_TAP_REPO = \"git://mirrors.ustc.edu.cn/homebrew-core.git\".freeze 步骤三：安装，运行修改了的 brew_install，然后是漫长的等待 /usr/bin/ruby ~/brew_install 执行之后你会看到如下界面： 出现这个因为源不通，代码无法下载到本地，解决方法是更换成国内镜像源，执行如下命令，更换到中科院的镜像： git clone git://mirrors.ustc.edu.cn/homebrew-core.git//usr/local/Homebrew/Library/Taps/homebrew/homebrew-core --depth=1 然后把 Homebrew-core 的镜像地址也设置为中科院的国内镜像 cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git 执行更新 brew 命令： brew update 接着执行 brew 检测命令： brew doctor 如上图出现警告是正常情况，因为我们更改了镜像源。到目前为止，海外用户或者已经设置系统全局代理的用户就可以使用 brew 安装你所需要的软件了。国内用户咱们继续操作，不然龟速下载搞得我想摔电脑！ 让我们把默认源替换为国内 USTC 源： (1) 替换核心软件仓库： cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git (2) 替换 cask 软件仓库： cd \"$(brew --repo)\"/Library/Taps/caskroom/homebrew-cask git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git (3) 替换 Bottle 源： bash 用户（shell 用户）： echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' >> ~/.bash_profile source ~/.bash_profile zsh 用户： echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' >> ~/.zshrc source ~/.zshrc 参考： Homebrew 中文主页 https://brew.sh/index_zh-cn.html Homebrew Bottles 源使用帮助 http://mirrors.ustc.edu.cn/help/homebrew-bottles.html Homebrew Cask 源使用帮助 http://mirrors.ustc.edu.cn/help/homebrew-cask.git.html Homebrew Core 源使用帮助 http://mirrors.ustc.edu.cn/help/homebrew-core.git.html "},"3 - 环境搭建_开发效率/3. VSCODE - 解决 vscode 中使用 pytorch 时 pylint 报错 Module torch has no xxx member.html":{"url":"3 - 环境搭建_开发效率/3. VSCODE - 解决 vscode 中使用 pytorch 时 pylint 报错 Module torch has no xxx member.html","title":"3. VSCODE - 解决 vscode 中使用 pytorch 时 pylint 报错 Module torch has no xxx member","keywords":"","body":"1. 项目场景2. 问题描述3. 原因分析4. 解决方案4.1. 方案一（推荐）4.2. 方案二5. 温馨提示6. 引用参考 本文由 简悦 SimpRead 转码， 原文地址 www.cnblogs.com 1. 项目场景 安装好了pytorch，写一个简单的测试代码，如下： import torch x = torch.rand(5, 3) print(x) 2. 问题描述 正常输出： tensor([[0.3506, 0.0131, 0.4908], [0.8524, 0.1879, 0.2194], [0.0101, 0.6458, 0.9603], [0.7522, 0.2765, 0.6378], [0.6041, 0.6980, 0.8985]]) 但会报错： Module 'torch' has no 'rand' member 3. 原因分析 这个错误是pylint报的，所以肯定与它有关。具体可以看 github 上的第一条评论。 4. 解决方案 4.1. 方案一（推荐） Ctrl+Shift+P打开 vscode 的命令面板，输入settings.json并打开第一项 在settings.json中插入以下代码 \"python.linting.pylintArgs\": [ \"--errors-only\", \"--generated-members=numpy.*, torch.*, cv2.*, cv.*\" ] 插入代码之后记得保存一下 4.2. 方案二 打开 vscode，在settings中搜索python.linting.pylintPath，将原pylint替换为conda安装路径\\pkgs\\pylint文件夹\\Scripts\\pylint前提是你用 conda 安装的 pytorch，无论是 Anaconda 还是 Miniconda 都行。 5. 温馨提示 Pylint 是一个 Python 代码分析工具，它分析 Python 代码中的错误，查找不符合代码风格标准和有潜在问题的代码。 用方案二之后 pylint 不知道为什么不会再进行任何提示了，相当于将 pylint 禁用了，显然这不是我们想要的效果。 而方案一不仅可以解决 torch 的报错问题，pylint 还能继续发挥作用。以下图为例，torch 不再报错，而其它错误比如使用了未定义的变量等，pylint 还是会正常的提示。 6. 引用参考 https://pypi.org/project/pylint/ https://github.com/pytorch/pytorch/issues/701 https://stackoverflow.com/questions/50319943/pytorch-error-message-torch-has-no-member "},"3 - 环境搭建_开发效率/介绍-Jupyter网络笔记本.html":{"url":"3 - 环境搭建_开发效率/介绍-Jupyter网络笔记本.html","title":"介绍-Jupyter网络笔记本","keywords":"","body":"1. Jupyter网络笔记本1. Jupyter网络笔记本 官网: jupyter.org 一、Jupyter Notebook 简易使用教程：https://u.nu/4vr1q Jupyter Notebook 默认是运行在浏览器的网页上的 Jupyter 的文件系统 交互式代码 Markdown 语法撰写技术文档、Markdown 语法插入图片：!()[图片地址] 可用PyCharm 编辑器写 Jupyter Note PyCharm、Jupyter Notebook、Github三者的交互： PyCharm用于编辑、Jupyter Notebook网页端用于操作、Github用于发布。PyCharm中点击url进入网页，保存为Markdown 后复制粘贴到 Github上。 在 Jupyter Notebook网页端可保存笔记为Markdown 、PDF，便于保存、分享、发布。 补：保存为pdf 的方案：先保存为.md ，再用Typora转化为Markdown "},"4 - Python_API/Language - Python.html":{"url":"4 - Python_API/Language - Python.html","title":"Language Python","keywords":"","body":"1. Python1.1. enumerate()函数 - 枚举1.2. Python安装1.2.1. 安装路径1.2.2. 版本号2. TensorFlow2.1. reshape - 重塑形状3. Numpy3.1. random.randint4. PIL库4.1. Image.fromarray与asarray1. Python 1.1. enumerate()函数 - 枚举 原文地址 www.runoob.com 用于将一个可遍历的数据对象 (如列表、元组或字符串) 组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。(下标，对象) 语法 enumerate(sequence, [start=0]) 参数 sequence -- 一个序列、迭代器或其他支持迭代对象。 start -- 下标起始位置。 返回值 返回 enumerate(枚举) 对象。 for 循环使用 enumerate >>> seq = ['one', 'two', 'three'] >>> for i, element in enumerate(seq): ... print i, element ... 0 one 1 two 2 three 1.2. Python安装 1.2.1. 安装路径 原文地址 www.cnblogs.com 1、terminal : input: which Python 或者 which Python3 2、terminal: input : python --->import sys ----> print sys.path 或者 input : python3 --->import sys ----> print sys.path 1.2.2. 版本号 python3 --version 2. TensorFlow 2.1. reshape - 重塑形状 tf.reshape & tf.shape 输出张量形状/重塑张量形状 https://www.tensorflow.org/api_docs/python/tf/reshape Given tensor, this operation returns a new tf.Tensor that has the same values as tensor in the same order, except with a new shape given by shape. #拉成一维列向量 t2 = tf.reshape(t1, [-1,]) tf.cast 类型转换 https://www.tensorflow.org/api_docs/python/tf/cast 3. Numpy 3.1. random.randint 在范围内生成随机整型数 原文地址 blog.csdn.net numpy.random.randint(low, high=None, size=None, dtype='l') 函数的作用是，返回一个随机整型数，范围从低（包括）到高（不包括），即 [low, high)。 如果没有写参数 high 的值，则返回 [0,low) 的值。 参数如下： low: int 生成的数值最低要大于等于 low。 hign = None 时，生成的数值要在 [0, low) 区间内） high: int (可选) 如果使用这个值，则生成的数值在 [low, high) 区间。 size: int or tuple of ints(可选) 输出随机数的尺寸，比如 size = (m n k) 则输出同规模即 m n k 个随机数。默认是 None 的，仅仅返回满足要求的单一随机数。 dtype: dtype(可选)： 想要输出的格式。如int64、int等等。 输出： out: int or ndarray of ints 返回一个随机数或随机数数组 例子 >>> np.random.randint(2, size=10) array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) >>> np.random.randint(1, size=10) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) >>> np.random.randint(5, size=(2, 4)) array([[4, 0, 2, 1], [3, 2, 2, 0]]) >>>np.random.randint(2, high=10, size=(2,3)) array([[6, 8, 7], [2, 5, 2]]) 4. PIL库 图像库PIL的类Image 4.1. Image.fromarray与asarray 原文地址 blog.csdn.net PIL image 转换成 array img = np.asarray(image) 需要注意的是，如果出现 read-only 错误，并不是转换的错误，一般是你读取的图片的时候，默认选择的是 \"r\",\"rb\" 模式有关。 修正的办法:　手动修改图片的读取状态 img.flags.writeable = True # 将数组改为读写模式 array 转换成 image Image.fromarray(np.uint8(img)) "},"4 - Python_API/Lib - TensorFlow_API.html":{"url":"4 - Python_API/Lib - TensorFlow_API.html","title":"Lib Tensor Flow API","keywords":"","body":""},"4 - Python_API/Python 中 with 用法及原理.html":{"url":"4 - Python_API/Python 中 with 用法及原理.html","title":"Python 中 with 用法及原理","keywords":"","body":"1.1. 前言1.2. 问题引出1.3. 改进：try···catch···1.4. 改进：with···as1.5. with 工作原理1.6. 总结1.7. 参考网址 原文地址 blog.csdn.net 1.1. 前言 代替try···catch··· with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的 “清理” 操作，释放资源，比如文件使用后自动关闭／线程中锁的自动获取和释放等。 1.2. 问题引出 如下代码： file = open(\"１.txt\") data = file.read() file.close() 上面代码存在２个问题： （１）文件读取发生异常，但没有进行任何处理； （２）可能忘记关闭文件句柄； 1.3. 改进：try···catch··· try: f = open('xxx') except: print('fail to open') exit(-1) try: do something except: do something finally: f.close() 1.4. 改进：with···as 而使用 with 的话，能够减少冗长，还能自动处理上下文环境产生的异常。如下面代码： with open(\"１.txt\") as file: data = file.read() 1.5. with 工作原理 （１）紧跟 with 后面的语句被求值后，返回对象的 “–enter–()” 方法被调用，这个方法的返回值将被赋值给 as 后面的变量； （２）当 with 后面的代码块全部被执行完之后，将调用前面返回对象的 “–exit–()” 方法。 with 工作原理代码示例： class Sample: def __enter__(self): print \"in __enter__\" return \"Foo\" def __exit__(self, exc_type, exc_val, exc_tb): print \"in __exit__\" def get_sample(): return Sample() with get_sample() as sample: print \"Sample: \", sample 代码的运行结果如下： in __enter__ Sample: Foo in __exit__ 可以看到，整个运行过程如下： （１）enter() 方法被执行； （２）enter() 方法的返回值，在这个例子中是”Foo”，赋值给变量 sample； （３）执行代码块，打印 sample 变量的值为”Foo”； （４）exit() 方法被调用； 1.6. 总结 实际上，在 with 后面的代码块抛出异常时，exit() 方法被执行。开发库时，清理资源，关闭文件等操作，都可以放在 exit() 方法中。 总之，with-as 表达式极大的简化了每次写 finally 的工作，这对代码的优雅性是有极大帮助的。 如果有多项，可以这样写： With open('1.txt') as f1, open('2.txt') as f2: do something 1.7. 参考网址 http://blog.kissdata.com/2014/05/23/python-with.html "},"4 - Python_API/python 中，花括号，中括号，小括号的区别.html":{"url":"4 - Python_API/python 中，花括号，中括号，小括号的区别.html","title":"python 中，花括号，中括号，小括号的区别","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 www.cnblogs.com Python 主要有三种数据类型：字典、列表、元组。其分别由花括号，中括号，小括号表示。 如： 字典：dic={'a':12,'b':34} 列表：list=[1,2,3,4] 元组：tup=(1,2,3,4) 至于这三者的具体区别，有很多介绍的，我就不在这里赘述了。 "},"4 - Python_API/Python 判断列表中是否存在某元素.html":{"url":"4 - Python_API/Python 判断列表中是否存在某元素.html","title":"Python 判断列表中是否存在某元素","keywords":"","body":"1.1. 成员运算符1.1.1. 实例： 本文由 简悦 SimpRead 转码， 原文地址 www.cnblogs.com 1.1. 成员运算符 运算符描述in如果在指定的序列中找到值返回 True，否则返回 Falsenot in如果在指定的序列中没有找到值返回 True，否则返回 False 1.1.1. 实例： ; \"复制代码\") ; \"复制代码\") 结果： 1 在列表 lista 中cf 在列表 lista 中 is 与 == 区别：is 用于判断两个变量引用对象是否为同一个， == 用于判断引用变量的值是否相等 ; \"复制代码\") #-*- coding:utf-8 -*- python 3.6.2 a=1 b=1 lista=[1,'5','s','cf'] listb=[1,'5','s','cf'] if a is b: print('a=b') if listb is lista: print('lista is listb') if lista == listb: print('lista=listb') ; \"复制代码\") 结果： a=blista=listb "},"4 - Python_API/python 获取当前工作目录.html":{"url":"4 - Python_API/python 获取当前工作目录.html","title":"python 获取当前工作目录","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 www.cnblogs.com py 文件所在位置 / test/pj/hello.py 用户所在位置：/ 用户执行命令 python /test/pj/hello.py os.getcwd() 　　返回的是执行命令的位置 / 2.sys.paht[0] 　　返回的是脚本所在的位置 /test/pj/ EOF 本文作者：童小哥总是不开心。本文链接：https://www.cnblogs.com/tongchengbin/p/7101764.html关于博主：评论和私信会在第一时间回复。或者直接私信我。版权声明：本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！声援博主：如果您觉得文章对您有帮助，可以点击文章右下角【推荐;)】一下。您的鼓励是博主的最大动力！ "},"4 - Python_API/python 读写 csv 文件（创建，追加，覆盖）.html":{"url":"4 - Python_API/python 读写 csv 文件（创建，追加，覆盖）.html","title":"python 读写 csv 文件（创建，追加，覆盖）","keywords":"","body":"1.1. 总述：1.2. 创建：1.3. 追加：1.4. 读：1.5. 附加： 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1. 总述： 这篇博客讲述 python 怎样创建，读写，追加 csv 文件 1.2. 创建： 利用 csv 包中的 writer 函数，如果文件不存在，会自动创建，需要注意的是，文件后缀一定要是. csv，这样才会创建 csv 文件 这里创建好文件，将 csv 文件的头信息写进了文件。 import csv def create_csv(): path = \"aa.csv\" with open(path,'wb') as f: csv_write = csv.writer(f) csv_head = [\"good\",\"bad\"] csv_write.writerow(csv_head) 1.3. 追加： 在 python 中，以 a + 的方式打开，是追加 def write_csv(): path = \"aa.csv\" with open(path,'a+') as f: csv_write = csv.writer(f) data_row = [\"1\",\"2\"] csv_write.writerow(data_row) 1.4. 读： 利用 csv.reader 可以读 csv 文件，然后返回一个可迭代的对象 csv_read，我们可以直接从 csv_read 中取数据 def read_csv(): path = \"aa.csv\" with open(path,\"rb\") as f: csv_read = csv.reader(f) for line in csv_read: print line 1.5. 附加： python 利用 open 打开文件的方式： w：以写方式打开，a：以追加模式打开 (从 EOF 开始, 必要时创建新文件)r+：以读写模式打开w+：以读写模式打开 (参见 w)a+：以读写模式打开 (参见 a)rb：以二进制读模式打开wb：以二进制写模式打开 (参见 w)ab：以二进制追加模式打开 (参见 a)rb+：以二进制读写模式打开 (参见 r+)wb+：以二进制读写模式打开 (参见 w+)ab+：以二进制读写模式打开 (参见 a+) "},"4 - Python_API/python 逐行读取文件内容的三种方法.html":{"url":"4 - Python_API/python 逐行读取文件内容的三种方法.html","title":"python 逐行读取文件内容的三种方法","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 一、使用 open 打开文件后一定要记得调用文件对象的 close() 方法。比如可以用 try/finally 语句来确保最后能关闭文件。 二、需要导入 import os 三、下面是逐行读取文件内容的三种方法： 1、第一种方法： f = open(\"foo.txt\") # 返回一个文件对象 line = f.readline() # 调用文件的 readline()方法 while line: print line, # 后面跟 ',' 将忽略换行符 #print(line, end = '')　 # 在 Python 3 中使用 line = f.readline() f.close() 2、第二种方法： for line in open(\"foo.txt\"): print line, 3、第三种方法： f = open(\"c:\\\\1.txt\",\"r\") lines = f.readlines() #读取全部内容 ，并以列表方式返回 for line in lines print line 四、一次性读取整个文件内容： file_object = open('thefile.txt') try: all_the_text = file_object.read() finally: file_object.close() 五、区别对待读取文本 和 二进制： 1、如果是读取文本 读文本文件 input = open('data', 'r') #第二个参数默认为r input = open('data') 2、如果是读取二进制 input = open('data', 'rb') 读固定字节 chunk = input.read(100) "},"4 - Python_API/python读写 JSON 数据.html":{"url":"4 - Python_API/python读写 JSON 数据.html","title":"python读写 JSON 数据","keywords":"","body":"1.1. 问题 ¶1.2. 解决方案 ¶1.3. 讨论 ¶ 本文由 简悦 SimpRead 转码， 原文地址 python3-cookbook.readthedocs.io python3-cookbook 1.1. 问题 ¶ 你想读写 JSON(JavaScript Object Notation) 编码格式的数据。 1.2. 解决方案 ¶ json 模块提供了一种很简单的方式来编码和解码 JSON 数据。 其中两个主要的函数是 json.dumps() 和 json.loads() ， 要比其他序列化函数库如 pickle 的接口少得多。 下面演示如何将一个 Python 数据结构转换为 JSON： import json data = { 'name' : 'ACME', 'shares' : 100, 'price' : 542.23 } json_str = json.dumps(data) 下面演示如何将一个 JSON 编码的字符串转换回一个 Python 数据结构： data = json.loads(json_str) 如果你要处理的是文件而不是字符串，你可以使用 json.dump() 和 json.load() 来编码和解码 JSON 数据。例如： # Writing JSON data with open('data.json', 'w') as f: json.dump(data, f) # Reading data back with open('data.json', 'r') as f: data = json.load(f) 1.3. 讨论 ¶ JSON 编码支持的基本数据类型为 None ， bool ， int ， float 和 str ， 以及包含这些类型数据的 lists，tuples 和 dictionaries。 对于 dictionaries，keys 需要是字符串类型 (字典中任何非字符串类型的 key 在编码时会先转换为字符串)。 为了遵循 JSON 规范，你应该只编码 Python 的 lists 和 dictionaries。 而且，在 web 应用程序中，顶层对象被编码为一个字典是一个标准做法。 JSON 编码的格式对于 Python 语法而已几乎是完全一样的，除了一些小的差异之外。 比如，True 会被映射为 true，False 被映射为 false，而 None 会被映射为 null。 下面是一个例子，演示了编码后的字符串效果： >>> json.dumps(False) 'false' >>> d = {'a': True, ... 'b': 'Hello', ... 'c': None} >>> json.dumps(d) '{\"b\": \"Hello\", \"c\": null, \"a\": true}' >>> 如果你试着去检查 JSON 解码后的数据，你通常很难通过简单的打印来确定它的结构， 特别是当数据的嵌套结构层次很深或者包含大量的字段时。 为了解决这个问题，可以考虑使用 pprint 模块的 pprint() 函数来代替普通的 print() 函数。 它会按照 key 的字母顺序并以一种更加美观的方式输出。 下面是一个演示如何漂亮的打印输出 Twitter 上搜索结果的例子： >>> from urllib.request import urlopen >>> import json >>> u = urlopen('http://search.twitter.com/search.json?q=python&rpp=5') >>> resp = json.loads(u.read().decode('utf-8')) >>> from pprint import pprint >>> pprint(resp) {'completed_in': 0.074, 'max_id': 264043230692245504, 'max_id_str': '264043230692245504', 'next_page': '?page=2&max_id=264043230692245504&q=python&rpp=5', 'page': 1, 'query': 'python', 'refresh_url': '?since_id=264043230692245504&q=python', 'results': [{'created_at': 'Thu, 01 Nov 2012 16:36:26 +0000', 'from_user': ... }, {'created_at': 'Thu, 01 Nov 2012 16:36:14 +0000', 'from_user': ... }, {'created_at': 'Thu, 01 Nov 2012 16:36:13 +0000', 'from_user': ... }, {'created_at': 'Thu, 01 Nov 2012 16:36:07 +0000', 'from_user': ... } {'created_at': 'Thu, 01 Nov 2012 16:36:04 +0000', 'from_user': ... }], 'results_per_page': 5, 'since_id': 0, 'since_id_str': '0'} >>> 一般来讲，JSON 解码会根据提供的数据创建 dicts 或 lists。 如果你想要创建其他类型的对象，可以给 json.loads() 传递 object_pairs_hook 或 object_hook 参数。 例如，下面是演示如何解码 JSON 数据并在一个 OrderedDict 中保留其顺序的例子： >>> s = '{\"name\": \"ACME\", \"shares\": 50, \"price\": 490.1}' >>> from collections import OrderedDict >>> data = json.loads(s, object_pairs_hook=OrderedDict) >>> data OrderedDict([('name', 'ACME'), ('shares', 50), ('price', 490.1)]) >>> 下面是如何将一个 JSON 字典转换为一个 Python 对象例子： >>> class JSONObject: ... def __init__(self, d): ... self.__dict__ = d ... >>> >>> data = json.loads(s, object_hook=JSONObject) >>> data.name 'ACME' >>> data.shares >>> data.price 490.1 >>> 最后一个例子中，JSON 解码后的字典作为一个单个参数传递给 __init__() 。 然后，你就可以随心所欲的使用它了，比如作为一个实例字典来直接使用它。 在编码 JSON 的时候，还有一些选项很有用。 如果你想获得漂亮的格式化字符串后输出，可以使用 json.dumps() 的 indent 参数。 它会使得输出和 pprint() 函数效果类似。比如： >>> print(json.dumps(data)) {\"price\": 542.23, \"name\": \"ACME\", \"shares\": 100} >>> print(json.dumps(data, indent=4)) { \"price\": 542.23, \"name\": \"ACME\", \"shares\": 100 } >>> 对象实例通常并不是 JSON 可序列化的。例如： >>> class Point: ... def __init__(self, x, y): ... self.x = x ... self.y = y ... >>> p = Point(2, 3) >>> json.dumps(p) Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python3.3/json/__init__.py\", line 226, in dumps return _default_encoder.encode(obj) File \"/usr/local/lib/python3.3/json/encoder.py\", line 187, in encode chunks = self.iterencode(o, _one_shot=True) File \"/usr/local/lib/python3.3/json/encoder.py\", line 245, in iterencode return _iterencode(o, 0) File \"/usr/local/lib/python3.3/json/encoder.py\", line 169, in default raise TypeError(repr(o) + \" is not JSON serializable\") TypeError: is not JSON serializable >>> 如果你想序列化对象实例，你可以提供一个函数，它的输入是一个实例，返回一个可序列化的字典。例如： def serialize_instance(obj): d = { '__classname__' : type(obj).__name__ } d.update(vars(obj)) return d 如果你想反过来获取这个实例，可以这样做： # Dictionary mapping names to known classes classes = { 'Point' : Point } def unserialize_object(d): clsname = d.pop('__classname__', None) if clsname: cls = classes[clsname] obj = cls.__new__(cls) # Make instance without calling __init__ for key, value in d.items(): setattr(obj, key, value) return obj else: return d 下面是如何使用这些函数的例子： >>> p = Point(2,3) >>> s = json.dumps(p, default=serialize_instance) >>> s '{\"__classname__\": \"Point\", \"y\": 3, \"x\": 2}' >>> a = json.loads(s, object_hook=unserialize_object) >>> a >>> a.x >>> a.y >>> json 模块还有很多其他选项来控制更低级别的数字、特殊值如 NaN 等的解析。 可以参考官方文档获取更多细节。 "},"4 - Python_API/使用 opencv 进行简单的抠图.html":{"url":"4 - Python_API/使用 opencv 进行简单的抠图.html","title":"使用 opencv 进行简单的抠图","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com # 导入相关模块和包 import cv2 as cv import numpy as np # 定义抠图函数 def get_face(image): face = image[40:600, 110:400] cv.imshow('get_face', face) # 读取图片 src = cv.imread(r'C:\\Users\\Administrator.PC-20201106KUIO\\Desktop\\lena.jpg') # 展示图片 cv.imshow('artwork mester', src) # 抠图函数 get_face(src) # 等待用户按下任意键 cv.waitKey(0) # 释放内存 cv.destroyAllWindows() "},"5 - 实验笔记/Pytoch自定义类改写数据/Datasets & Dataloaders — PyTorch Tutorials 1.8.1+cu102 documentation.html":{"url":"5 - 实验笔记/Pytoch自定义类改写数据/Datasets & Dataloaders — PyTorch Tutorials 1.8.1+cu102 documentation.html","title":"Datasets & Dataloaders — PyTorch Tutorials 1.8.1+cu102 documentation","keywords":"","body":"1.1. Loading a Dataset1.2. Iterating and Visualizing the Dataset1.3. Creating a Custom Dataset for your files1.3.1. init1.3.2. len1.3.3. getitem1.4. Preparing your data for training with DataLoaders1.5. Iterate through the DataLoader 本文由 简悦 SimpRead 转码， 原文地址 pytorch.org Note Click here to download the full example code Learn the Basics || Quickstart || Tensors || Datasets & DataLoaders || Transforms || Build Model || Autograd || Optimization || Save & Load Model Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples. PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: Image Datasets, Text Datasets, and Audio Datasets 1.1. Loading a Dataset Here is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes. We load the FashionMNIST Dataset with the following parameters: root is the path where the train/test data is stored, train specifies training or test dataset, download=True downloads the data from the internet if it’s not available at root. transform and target_transform specify the feature and label transformations import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt training_data = datasets.FashionMNIST( root=\"data\", train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=\"data\", train=False, download=True, transform=ToTensor() ) Out: Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw Processing... Done! 1.2. Iterating and Visualizing the Dataset We can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data. labels_map = { 0: \"T-Shirt\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\", } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1): sample_idx = torch.randint(len(training_data), size=(1,)).item() img, label = training_data[sample_idx] figure.add_subplot(rows, cols, i) plt.title(labels_map[label]) plt.axis(\"off\") plt.imshow(img.squeeze(), cmap=\"gray\") plt.show() 1.3. Creating a Custom Dataset for your files A custom Dataset class must implement three functions: init, len, and getitem. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file. In the next sections, we’ll break down what’s happening in each of these functions. import os import pandas as pd from torchvision.io import read_image class CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform=None, target_transform=None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.img_labels) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) image = read_image(img_path) label = self.img_labels.iloc[idx, 1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) sample = {\"image\": image, \"label\": label} return sample 1.3.1. init The init function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section). The labels.csv file looks like: tshirt1.jpg, 0 tshirt2.jpg, 0 ...... ankleboot999.jpg, 9 def __init__(self, annotations_file, img_dir, transform=None, target_transform=None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform 1.3.2. len The len function returns the number of samples in our dataset. Example: def __len__(self): return len(self.img_labels) 1.3.3. getitem The getitem function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a Python dict. def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) image = read_image(img_path) label = self.img_labels.iloc[idx, 1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) sample = {\"image\": image, \"label\": label} return sample 1.4. Preparing your data for training with DataLoaders The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval. DataLoader is an iterable that abstracts this complexity for us in an easy API. from torch.utils.data import DataLoader train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True) 1.5. Iterate through the DataLoader We have loaded that dataset into the Dataloader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers). # Display image and label. train_features, train_labels = next(iter(train_dataloader)) print(f\"Feature batch shape: {train_features.size()}\") print(f\"Labels batch shape: {train_labels.size()}\") img = train_features[0].squeeze() label = train_labels[0] plt.imshow(img, cmap=\"gray\") plt.show() print(f\"Label: {label}\") Out: Feature batch shape: torch.Size([64, 1, 28, 28]) Labels batch shape: torch.Size([64]) Label: 8 "},"5 - 实验笔记/Pytoch自定义类改写数据/PyTorch 手把手自定义 Dataloader 读取数据.html":{"url":"5 - 实验笔记/Pytoch自定义类改写数据/PyTorch 手把手自定义 Dataloader 读取数据.html","title":"PyTorch 手把手自定义 Dataloader 读取数据","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 之前刚开始用的时候，写 Dataloader 遇到不少坑。网上有一些教程 分为 all images in one folder 和 each class one folder。后面的那种写的人比较多，我写一下前面的这种，程式化的东西，每次不同的任务改几个参数就好。 等训练的时候写一篇文章把 2333 一. 已有的东西 举例子：用 kaggle 上的一个 dog breed 的数据集为例。数据文件夹里面有三个子目录 test: 几千张图片，没有标签，测试集 train: 10222 张狗的图片，全是 jpg，大小不一，有长有宽，基本都在 400×300 以上 labels.csv ： excel 表格, 图片名称 + 品种名称 我喜欢先用 pandas 把表格信息读出来看一看 import pandas as pd import numpy as np df = pd.read_csv('./dog_breed/labels.csv') print(df.info()) print(df.head()) 看到，一共有 10222 个数据，id 对应的是图片的名字，但是没有后缀 .jpg。 breed 对应的是犬种。 二. 预处理 我们要做的事情是： 1) 得到一个长 list1 : 里面是每张图片的路径 2) 另外一个长 list2: 里面是每张图片对应的标签（整数），顺序要和 list1 对应。 3) 把这两个 list 切分出来一部分作为验证集 1）看看一共多少个 breed, 把每种 breed 名称和一个数字编号对应起来： from pandas import Series,DataFrame breed = df['breed'] breed_np = Series.as_matrix(breed) print(type(breed_np) ) print(breed_np.shape) #(10222,) #看一下一共多少不同种类 breed_set = set(breed_np) print(len(breed_set)) #120 #构建一个编号与名称对应的字典，以后输出的数字要变成名字的时候用： breed_120_list = list(breed_set) dic = {} for i in range(120): dic[ breed_120_list[i] ] = i 2）处理 id 那一列，分割成两段： file = Series.as_matrix(df[\"id\"]) print(file.shape) import os file = [i+\".jpg\" for i in file] file = [os.path.join(\"./dog_breed/train\",i) for i in file ] file_train = file[:8000] file_test = file[8000:] print(file_train) np.save( \"file_train.npy\" ,file_train ) np.save( \"file_test.npy\" ,file_test ) 里面就是图片的路径了 3）处理 breed 那一列，分成两段： breed = Series.as_matrix(df[\"breed\"]) print(breed.shape) number = [] for i in range(10222): number.append( dic[ breed[i] ] ) number = np.array(number) number_train = number[:8000] number_test = number[8000:] np.save( \"number_train.npy\" ,number_train ) np.save( \"number_test.npy\" ,number_test ) 三. Dataloader 我们已经有了图片路径的 list,target 编号的 list。填到 Dataset 类里面就行了。 from torch.utils.data import Dataset, DataLoader from torchvision import transforms, utils normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) preprocess = transforms.Compose([ #transforms.Scale(256), #transforms.CenterCrop(224), transforms.ToTensor(), normalize ]) def default_loader(path): img_pil = Image.open(path) img_pil = img_pil.resize((224,224)) img_tensor = preprocess(img_pil) return img_tensor #当然出来的时候已经全都变成了tensor class trainset(Dataset): def __init__(self, loader=default_loader): #定义好 image 的路径 self.images = file_train self.target = number_train self.loader = loader def __getitem__(self, index): fn = self.images[index] img = self.loader(fn) target = self.target[index] return img,target def __len__(self): return len(self.images) 我们看一下代码，自定义 Dataset 只需要最下面一个 class, 继承自 Dataset 类。有三个私有函数 def init(self, loader=default_loader): 这个里面一般要初始化一个 loader(代码见上面), 一个 images_path 的列表，一个 target 的列表 def getitem(self, index)： 这里吗就是在给你一个 index 的时候，你返回一个图片的 tensor 和 target 的 tensor, 使用了 loader 方法，经过 归一化，剪裁，类型转化，从图像变成 tensor def len(self): return 你所有数据的个数 这三个综合起来看呢，其实就是你告诉它你所有数据的长度，它每次给你返回一个 shuffle 过的 index, 以这个方式遍历数据集，通过 getitem(self, index) 返回一组你要的（input,target） 四. 使用 实例化一个 dataset, 然后用 Dataloader 包起来 train_data = trainset() trainloader = DataLoader(train_data, batch_size=4,shuffle=True) "},"5 - 实验笔记/Pytoch自定义类改写数据/Pytorch 源码解读 - torchvision.datasets.folder.html":{"url":"5 - 实验笔记/Pytoch自定义类改写数据/Pytorch 源码解读 - torchvision.datasets.folder.html","title":"Pytorch 源码解读 - torchvision.datasets.folder","keywords":"","body":" 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net import torch.utils.data as data from PIL import Image import os import os.path IMG_EXTENSIONS = [ '.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', ] def is_image_file(filename): return any(filename.endswith(extension) for extension in IMG_EXTENSIONS) def find_classes(dir): classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))] classes.sort() class_to_idx = {classes[i]: i for i in range(len(classes))} return classes, class_to_idx def make_dataset(dir, class_to_idx): images = [] dir = os.path.expanduser(dir) for target in sorted(os.listdir(dir)): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in sorted(fnames): if is_image_file(fname): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return images def pil_loader(path): # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835) with open(path, 'rb') as f: with Image.open(f) as img: return img.convert('RGB') def accimage_loader(path): import accimage try: return accimage.Image(path) except IOError: # Potentially a decoding problem, fall back to PIL.Image return pil_loader(path) def default_loader(path): from torchvision import get_image_backend if get_image_backend() == 'accimage': return accimage_loader(path) else: return pil_loader(path) [docs]class ImageFolder(data.Dataset): \"\"\"A generic data loader where the images are arranged in this way: :: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png Args: root (string): Root directory path. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. loader (callable, optional): A function to load an image given its path. Attributes: classes (list): List of the class names. class_to_idx (dict): Dict with items (class_name, class_index). imgs (list): List of (image path, class_index) tuples \"\"\" def __init__(self, root, transform=None, target_transform=None, loader=default_loader): classes, class_to_idx = find_classes(root) imgs = make_dataset(root, class_to_idx) if len(imgs) == 0: raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\" \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS))) self.root = root self.imgs = imgs self.classes = classes self.class_to_idx = class_to_idx self.transform = transform self.target_transform = target_transform self.loader = loader [docs] def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (image, target) where target is class_index of the target class. \"\"\" path, target = self.imgs[index] img = self.loader(path) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): return len(self.imgs) "},"5 - 实验笔记/Pytoch自定义类改写数据/torchvision自定义数据读取.html":{"url":"5 - 实验笔记/Pytoch自定义类改写数据/torchvision自定义数据读取.html","title":"torchvision自定义数据读取","keywords":"","body":"1.1.1. 文章目录1.1.2. 一、torchvision 图像数据读取 [0, 1]1.1.3. 二、torchvision 的 Transform1.1.4. 三、读取图像数据类 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1.1. 文章目录 一、torchvision 图像数据读取 [0, 1] 二、torchvision 的 Transform 三、读取图像数据类 3.1 class torchvision.datasets.ImageFolder 默认读取图像数据方法： 3.2 自定义数据读取方法 运行环境安装 Anaconda | python ==3.6.6 conda install pytorch -c pytorch pip install config pip install tqdm #包装迭代器，显示进度条 pip install torchvision pip install scikit-image 1.1.2. 一、torchvision 图像数据读取 [0, 1] import torchvision.transforms as transformstransforms 模块提供了一般的图像转换操作类。class torchvision.transforms.ToTensor功能：把 shape=(H x W x C) 的像素值为 [0, 255] 的 PIL.Image 和 numpy.ndarray转换成 shape=(C x H x W) 的像素值范围为[0.0, 1.0]的 torch.FloatTensor。 class torchvision.transforms.Normalize(mean, std)功能：此转换类作用于 torch.*Tensor。给定均值 (R, G, B) 和标准差(R, G, B)，用公式 channel = (channel - mean) / std 进行规范化。 import torchvision import torchvision.transforms as transforms import cv2 import numpy as np from PIL import Image img_path = \"./data/timg.jpg\" # 引入transforms.ToTensor()功能： range [0, 255] -> [0.0,1.0] transform1 = transforms.Compose([transforms.ToTensor()]) # 直接读取：numpy.ndarray img = cv2.imread(img_path) print(\"img = \", img[0]) #只输出其中一个通道 print(\"img.shape = \", img.shape) # 归一化，转化为numpy.ndarray并显示 img1 = transform1(img) img2 = img1.numpy()*255 img2 = img2.astype('uint8') img2 = np.transpose(img2 , (1,2,0)) print(\"img1 = \", img1) cv2.imshow('img2 ', img2 ) cv2.waitKey() # PIL 读取图像 img = Image.open(img_path).convert('RGB') # 读取图像 img2 = transform1(img) # 归一化到 [0.0,1.0] print(\"img2 = \",img2) #转化为PILImage并显示 img_2 = transforms.ToPILImage()(img2).convert('RGB') print(\"img_2 = \",img_2) img_2.show() 从上到下依次输出：--------------------------------------------- img = [[197 203 202] [195 203 202] ... [200 208 207] [200 208 207]] img.shape = (362, 434, 3) img1 = tensor([[[0.7725, 0.7647, 0.7686, ..., 0.7804, 0.7843, 0.7843], [0.7765, 0.7725, 0.7686, ..., 0.7686, 0.7608, 0.7569], [0.7843, 0.7725, 0.7686, ..., 0.7725, 0.7686, 0.7569], ..., img_transform = tensor([[[0.7922, 0.7922, 0.7961, ..., 0.8078, 0.8118, 0.8118], [0.7961, 0.8000, 0.7961, ..., 0.7922, 0.7882, 0.7843], [0.8039, 0.8000, 0.7961, ..., 0.8118, 0.8039, 0.7922], ..., transforms.Compose 归一化到 [-1.0, 1.0] transform2 = transforms.Compose([transforms.ToTensor()]) transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))]) 1.1.3. 二、torchvision 的 Transform 在深度学习时关于图像的数据读取：由于 Tensorflow 不支持与 numpy 的无缝切换，导致难以使用现成的 pandas 等格式化数据读取工具，造成了很多不必要的麻烦，而 pytorch 解决了这个问题。 pytorch 自定义读取数据和进行 Transform 的部分请见文档：http://pytorch.org/tutorials/beginner/data_loading_tutorial.html 但是按照文档中所描述所完成的自定义 Dataset 只能够使用自定义的 Transform 步骤，而 torchvision 包中已经给我们提供了很多图像 transform 步骤的实现，为了使用这些已经实现的 Transform 步骤，我们可以使用如下方法定义 Dataset： from __future__ import print_function, division import os import torch import pandas as pd from PIL import Image import numpy as np from torch.utils.data import Dataset, DataLoader from torchvision import transforms class FaceLandmarkDataset(Dataset): def __len__(self) -> int: return len(self.landmarks_frame) def __init__(self, csv_file: str, root_dir: str, transform=None) -> None: super().__init__() self.landmarks_frame = pd.read_csv(csv_file) self.root_dir = root_dir self.transform = transform def __getitem__(self, index:int): img_name = self.landmarks_frame.ix[index, 0] img_path = os.path.join('./faces', img_name) with Image.open(img_path) as img: image = img.convert('RGB') landmarks = self.landmarks_frame.as_matrix()[index, 1:].astype('float') landmarks = np.reshape(landmarks,newshape=(-1,2)) if self.transform is not None: image = self.transform(image) return image, landmarks ########################以上为数据读取类（返回：image,landmarks）############################### trans = transforms.Compose(transforms = [transforms.RandomSizedCrop(size=128), transforms.ToTensor()]) face_dataset = FaceLandmarkDataset(csv_file='faces/face_landmarks.csv', root_dir='faces', transform= trans) loader = DataLoader(dataset = face_dataset, batch_size=4, shuffle=True, num_workers=4) 1.1.4. 三、读取图像数据类 3.1 class torchvision.datasets.ImageFolder 默认读取图像数据方法： __init__（ 初始化） classes, class_to_idx = find_classes(root) : 得到分类的类别名（classes）和类别名与数字类别的映射关系字典（class_to_idx）其中 classes (list): List of the class names.其中 class_to_idx (dict): Dict with items (class_name, class_index). imgs = make_dataset(root, class_to_idx)得到 imags 列表。其中 imgs (list): List of (image path, class_index) tuples每个值是一个 tuple，每个 tuple 包含两个元素：图像路径和标签 __getitem__（图像获取） path, target = self.imgs[index] 获取图像（路径，标签） img = self.loader(path)数据读取。 img = self.transform(img)数据、标签 转换成 tensor target = self.target_transform(target) __len__（ 数据集数量） return len(self.imgs) class ImageFolder(data.Dataset): \"\"\"默认图像数据目录结构 root . ├──dog | ├──001.png | ├──002.png | └──... └──cat | ├──001.png | ├──002.png | └──... └──... \"\"\" def __init__(self, root, transform=None, target_transform=None, loader=default_loader): classes, class_to_idx = find_classes(root) imgs = make_dataset(root, class_to_idx) if len(imgs) == 0: raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\" \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS))) self.root = root self.imgs = imgs self.classes = classes self.class_to_idx = class_to_idx self.transform = transform self.target_transform = target_transform self.loader = loader def __getitem__(self, index): \"\"\" index (int): Index Returns:tuple: (image, target) where target is class_index of the target class. \"\"\" path, target = self.imgs[index] img = self.loader(path) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self): return len(self.imgs) 图像获取 __getitem__ 中，self.loader(path) 采用的是 default_loader，如下 def pil_loader(path): # 一般采用pil_loader函数。 with open(path, 'rb') as f: with Image.open(f) as img: return img.convert('RGB') def accimage_loader(path): import accimage try: return accimage.Image(path) except IOError: # Potentially a decoding problem, fall back to PIL.Image return pil_loader(path) def default_loader(path): from torchvision import get_image_backend if get_image_backend() == 'accimage': return accimage_loader(path) else: return pil_loader(path) 3.2 自定义数据读取方法 PyTorch 中和数据读取相关的类都要继承一个基类：torch.utils.data.Dataset。故需要改写其中的 __init__、__len__、__getitem__ 等三个方法即可。 __init__（）初始化传入参数： img_path 里面为所有图像数据（包括训练和测试）txt_path 里面有 train.txt 和 val.txt 两个文件：txt 文件中每行都是图像路径，tab 键，标签。 其中 self.img_name 和 self.img_label 的读取方式就跟你数据的存放方式有关（需要调整的地方) __getitem__（）依然采用 default_loader 方法来读取图像。 Transform中将每张图像都封装成 Tensor class customData(Dataset): def __init__(self, img_path, txt_path, dataset = '',data_transforms=None, loader = default_loader): with open(txt_path) as input_file: \"\"\" 关于json文件解析： https://blog.csdn.net/wsp_1138886114/article/details/83302339 txt文件解析如下，具体文本解析具体分析，没有定数 \"\"\" lines = input_file.readlines() self.img_name = [os.path.join(img_path, line.strip().split('\\t')[0]) for line in lines] self.img_label = [int(line.strip().split('\\t')[-1]) for line in lines] self.data_transforms = data_transforms self.dataset = dataset self.loader = loader def __len__(self): return len(self.img_name) def __getitem__(self, item): img_name = self.img_name[item] label = self.img_label[item] img = self.loader(img_name) if self.data_transforms is not None: try: img = self.data_transforms[self.dataset](img) except: print(\"Cannot transform image: {}\".format(img_name)) return img, label #####################以上为图像数据读取，返回（img, label）######################### # 保证image_datasets与torchvision.datasets.ImageFolder类返回的数据类型一样 image_datasets = {x: customData(img_path='/ImagePath', txt_path=('/TxtFile/' + x + '.txt'), data_transforms=data_transforms, dataset=x) for x in ['train', 'val']} #用torch.utils.data.DataLoader类，将这个batch的图像数据和标签都分别封装成Tensor。 dataloders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']} # 模型保存 torch.save(model, 'output/resnet_epoch{}.pkl'.format(epoch)) https://pytorch-cn.readthedocs.io/zh/latest/package_references/data/#torchutilsdata鸣谢https://www.cnblogs.com/denny402/p/5096001.htmlhttps://blog.csdn.net/VictoriaW/article/details/72822005https://blog.csdn.net/hao5335156/article/details/80593349https://blog.csdn.net/u014380165/article/details/78634829 "},"5 - 实验笔记/Pytoch自定义类改写数据/自定义数据读取方式 orch.utils.data.Dataset 与 Dataloader.html":{"url":"5 - 实验笔记/Pytoch自定义类改写数据/自定义数据读取方式 orch.utils.data.Dataset 与 Dataloader.html","title":"自定义数据读取方式 orch.utils.data.Dataset 与 Dataloader","keywords":"","body":"1.1. 0. 本章内容1.2. 1. torch.utils.data.Dataset 与 torch.utils.data.DataLoader 的理解1.3. 2. torchvision.datasets1.4. 3. torchvision.transforms 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 1.1. 0. 本章内容 在 pytorch 中，提供了一种十分方便的数据读取机制，即使用 torch.utils.data.Dataset 与 Dataloader 组合得到数据迭代器。在每次训练时，利用这个迭代器输出每一个 batch 数据，并能在输出时对数据进行相应的预处理或数据增广操作。 同时，pytorch 可视觉包 torchvision 中，继承 torch.utils.data.Dataset，预定义了许多常用的数据集，并提供了许多常用的数据增广函数。 本章主要进行下列介绍： torch.utils.data.Dataset 与 Dataloader 的理解 torchvision 中的 datasets torchvision ImageFolder torchvision transforms 具体代码可以在 XavierLinNow/pytorch_note_CN 得到 1.2. 1. torch.utils.data.Dataset 与 torch.utils.data.DataLoader 的理解 pytorch 提供了一个数据读取的方法，其由两个类构成：torch.utils.data.Dataset 和 DataLoader 我们要自定义自己数据读取的方法，就需要继承 torch.utils.data.Dataset，并将其封装到 DataLoader 中 torch.utils.data.Dataset 表示该数据集，继承该类可以重载其中的方法，实现多种数据读取及数据预处理方式 torch.utils.data.DataLoader 封装了 Data 对象，实现单（多）进程迭代器输出数据集 下面我们分别介绍下 torch.utils.data.Dataset 以及 DataLoader 1.1 torch.utils.data.Dataset 要自定义自己的 Dataset 类，至少要重载两个方法，len, getitem len返回的是数据集的大小 getitem实现索引数据集中的某一个数据 除了这两个基本功能，还可以在getitem时对数据进行预处理，或者是直接在硬盘中读取数据，对于超大的数据集还可以使用 lmdb 来读取 下面将简单实现一个返回 torch.Tensor 类型的数据集 from torch.utils.data import DataLoader, Dataset import torch class TensorDataset(Dataset): # TensorDataset继承Dataset, 重载了__init__, __getitem__, __len__ # 实现将一组Tensor数据对封装成Tensor数据集 # 能够通过index得到数据集的数据，能够通过len，得到数据集大小 def __init__(self, data_tensor, target_tensor): self.data_tensor = data_tensor self.target_tensor = target_tensor def __getitem__(self, index): return self.data_tensor[index], self.target_tensor[index] def __len__(self): return self.data_tensor.size(0) # 生成数据 data_tensor = torch.randn(4, 3) target_tensor = torch.rand(4) # 将数据封装成Dataset tensor_dataset = TensorDataset(data_tensor, target_tensor) # 可使用索引调用数据 print 'tensor_data[0]: ', tensor_dataset[0] ''' 输出 tensor_data[0]: ( 0.6804 -1.2515 1.6084 [torch.FloatTensor of size 3] , 0.2058754563331604) ''' # 可返回数据len print 'len os tensor_dataset: ', len(tensor_dataset) ''' 输出： len os tensor_dataset: 4 ''' 1.2 torch.utils.data.Dataloader Dataloader 将 Dataset 或其子类封装成一个迭代器 这个迭代器可以迭代输出 Dataset 的内容 同时可以实现多进程、shuffle、不同采样策略，数据校对等等处理过程 tensor_dataloader = DataLoader(tensor_dataset, # 封装的对象 batch_size=2, # 输出的batchsize shuffle=True, # 随机输出 num_workers=0) # 只有1个进程 # 以for循环形式输出 for data, target in tensor_dataloader: print(data, target) # 输出一个batch print 'one batch tensor data: ', iter(tensor_dataloader).next() # 输出batch数量 print 'len of batchtensor: ', len(list(iter(tensor_dataloader))) ''' 输出： one batch tensor data: [ 0.6804 -1.2515 1.6084 -0.1156 -1.1552 0.1866 [torch.FloatTensor of size 2x3] , 0.2059 0.6452 [torch.DoubleTensor of size 2] ] len of batchtensor: 2 ''' 1.3. 2. torchvision.datasets pytorch 专门针对视觉实现了一个 torchvision 包，里面包括了许多常用的 CNN 模型以及一些数据集 torchvision.datasets 包含了 MNIST，cifar10 等数据集，他们都是通过继承上述 Dataset 类实现的 2.1 调用 torchvision 自带的 cifar10 数据集 import torchvision.datasets as dset import torchvision import matplotlib.pyplot as plt import torchvision.transforms as transforms import numpy as np %matplotlib inline def imshow(img, is_unnormlize=False): if is_unnormlize: img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) # 载入cifar数据集 trainset = dset.CIFAR10(root='../data', # 数据集路径 train=True, # 载入train set download=True, # 如果未下载数据集，则自动下载。 # 建议直接下载后压缩到root的路径 transform=transforms.ToTensor() # 转换成Tensor才能被封装为DataLoader ) # 封装成loader trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) # 显示图片 dataiter = iter(trainloader) images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images)) 显示图片 2.2 直接从硬盘中载入自己的图像 torch.datasets 包中的 ImageFolder 支持我们直接从硬盘中按照固定路径格式载入每张数据，其格式如下： 根目录 / 类别 / 图像 root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png 1.4. 3. torchvision.transforms 在刚才，我们见到生成 cifar 数据集时有一个参数 transform，这个参数就是实现各种预处理 在 torchvision.transforms 中，有多种预测方式，如 scale，centercrop 我们可以使用 Compose 将这些预处理方式组成 transforms list，对图像进行多种处理 需要注意，由于这些 transform 是基于 PIL 的，因此 Compose 中，Scale 等预处理需要先调用，ToTensor 需要后与他们 如果觉得 torchvision 自带的预处理不够多，可以使用 https://github.com/ncullen93/torchsample 中的 transforms # 定义transform transform = torchvision.transforms.Compose( [transforms.RandomCrop(20), transforms.ToTensor(), # ToTensor需要在预处理之后进行 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] ) # 载入cifar数据集 trainset = dset.CIFAR10(root='../data', # 数据集路径 train=True, # 载入train set download=True, # 如果未下载数据集，则自动下载。 # 建议直接下载后压缩到root的路径 transform=transform # 进行预处理 ) # 封装成loader trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) # 显示图片 dataiter = iter(trainloader) images, labels = dataiter.next() imshow(torchvision.utils.make_grid(images), True) 经过数据增广的数据： "},"5 - 实验笔记/Yolo+Deepsort/【好文😭】手把手教你用 YOLOv5 训练自己的数据集.html":{"url":"5 - 实验笔记/Yolo+Deepsort/【好文😭】手把手教你用 YOLOv5 训练自己的数据集.html","title":"【好文😭】手把手教你用 YOLOv5 训练自己的数据集","keywords":"","body":"1.1.1. 手把手教你用 YOLOv5 训练自己的数据集2. 1. 安装 Anaconda3. 2. 创建虚拟环境4. 3. 安装 pytorch5. 4. 下载源码和安装依赖库6. 5. 数据标注7. 5. 数据预处理8. 6. 下载预训练模型9. 7. 开始训练 原文地址 blog.csdn.net 1.1.1. 手把手教你用 YOLOv5 训练自己的数据集 安装环境：Anaconda、CUDA、Pytorch、pip install -r requirements.txt、git clone repository 数据准备：labelme打标签、spilt.py+txt2yolo_label.py 转换成yolo格式 模型配置：data/ .yaml、model/ .yaml、weights/* .pt 开始训练 2. 1. 安装 Anaconda Anaconda 官网：https://www.anaconda.com/ 3. 2. 创建虚拟环境 这里我们需要为 yolov5 单独创建一个环境，输入： conda create -n torch107 python=3.7 安装完成后，输入： activate torch107 激活环境 4. 3. 安装 pytorch yolov5 最新版本需要 pytorch1.6 版本以上，因此我们安装 pytorch1.7 版本。由于我事先安装好了 CUDA10.1，因此在环境中输入： pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html 即可安装 然后查看 CUDA 是否可用： 这里显示 True 表明正常安装。 5. 4. 下载源码和安装依赖库 源码地址：https://github.com/ultralytics/yolov5 安装依赖库： pip install -r requirements.txt 6. 5. 数据标注 数据标注我们要用 labelimg，使用 pip 即可安装： pip install labelimg 7. 5. 数据预处理 创建 split.py 文件，内容如下： import os import random import argparse parser = argparse.ArgumentParser() parser.add_argument('--xml_path', default='VOCData/Annotations', type=str, help='input xml label path') parser.add_argument('--txt_path', default='VOCData/labels', type=str, help='output txt label path') opt = parser.parse_args() trainval_percent = 1.0 train_percent = 0.9 xmlfilepath = opt.xml_path txtsavepath = opt.txt_path total_xml = os.listdir(xmlfilepath) if not os.path.exists(txtsavepath): os.makedirs(txtsavepath) num = len(total_xml) list_index = range(num) tv = int(num * trainval_percent) tr = int(tv * train_percent) trainval = random.sample(list_index, tv) train = random.sample(trainval, tr) file_trainval = open(txtsavepath + '/trainval.txt', 'w') file_test = open(txtsavepath + '/test.txt', 'w') file_train = open(txtsavepath + '/train.txt', 'w') file_val = open(txtsavepath + '/val.txt', 'w') for i in list_index: name = total_xml[i][:-4] + '\\n' if i in trainval: file_trainval.write(name) if i in train: file_train.write(name) else: file_val.write(name) else: file_test.write(name) file_trainval.close() file_train.close() file_val.close() file_test.close() 运行结束后，可以看到 VOCData/labels 下生成了几个 txt 文件： 然后新建 txt2yolo_label.py 文件用于将数据集转换到 yolo 数据集格式： # -*- coding: utf-8 -*- import xml.etree.ElementTree as ET from tqdm import tqdm import os from os import getcwd sets = ['train', 'val', 'test'] classes = ['face', 'normal', 'phone', 'write', 'smoke', 'eat', 'computer', 'sleep'] def convert(size, box): dw = 1. / (size[0]) dh = 1. / (size[1]) x = (box[0] + box[1]) / 2.0 - 1 y = (box[2] + box[3]) / 2.0 - 1 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return x, y, w, h def convert_annotation(image_id): # try: in_file = open('VOCData/Annotations/%s.xml' % (image_id), encoding='utf-8') out_file = open('VOCData/labels/%s.txt' % (image_id), 'w', encoding='utf-8') tree = ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes or int(difficult) == 1: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) b1, b2, b3, b4 = b # 标注越界修正 if b2 > w: b2 = w if b4 > h: b4 = h b = (b1, b2, b3, b4) bb = convert((w, h), b) out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n') # except Exception as e: # print(e, image_id) wd = getcwd() for image_set in sets: if not os.path.exists('VOCData/labels/'): os.makedirs('VOCData/labels/') image_ids = open('VOCData/labels/%s.txt' % (image_set)).read().strip().split() list_file = open('VOCData/%s.txt' % (image_set), 'w') for image_id in tqdm(image_ids): list_file.write('VOCData/images/%s.jpg\\n' % (image_id)) convert_annotation(image_id) list_file.close() 转换后可以看到 VOCData/labels 下生成了每个图的 txt 文件： 在 data 文件夹下创建 myvoc.yaml 文件： 内容如下： train: VOCData/train.txt val: VOCData/val.txt # number of classes nc: 8 # class names names: [\"face\", \"normal\", \"phone\", \"write\", \"smoke\", \"eat\", \"computer\", \"sleep\"] 8. 6. 下载预训练模型 我训练 yolov5m 这个模型，因此将它的预训练模型下载到 weights 文件夹下： 9. 7. 开始训练 修改 models/yolov5m.yaml 下的类别数： 然后在 cmd 中输入： python train.py --img 640 --batch 4 --epoch 300 --data ./data/myvoc.yaml --cfg ./models/yolov5m.yaml --weights weights/yolov5m.pt --workers 0 即可开始训练： "},"5 - 实验笔记/Yolo+Deepsort/DeepSort - 基于 YOLOv5 和 DeepSort 的目标跟踪.html":{"url":"5 - 实验笔记/Yolo+Deepsort/DeepSort - 基于 YOLOv5 和 DeepSort 的目标跟踪.html","title":"DeepSort - 基于 YOLOv5 和 DeepSort 的目标跟踪","keywords":"","body":"1.1.1. 软硬件环境1.1.2. 视频看这里1.1.3. YOLOv51.1.4. DeepSort1.1.5. 目标跟踪1.1.6. 参考资料 本文由 简悦 SimpRead 转码， 原文地址 xugaoxiang.com 1.1.1. 软硬件环境 windows 10 64bit pytorch yolov5 deepsort 1.1.2. 视频看这里 此处是youtube的播放链接，需要科学上网。喜欢我的视频，请记得订阅我的频道，打开旁边的小铃铛，点赞并分享，感谢您的支持。 1.1.3. YOLOv5 前文 YOLOv5 目标检测 和 YOLOv5 模型训练 已经介绍过了YOLOv5相关的内容，在目标检测中效果不错。 1.1.4. DeepSort SORT算法的思路是将目标检测算法 (如YOLO) 得到的检测框与预测的跟踪框的iou(交并比) 输入到匈牙利算法中进行线性分配来关联帧间 ID。而DeepSORT算法则是将目标的外观信息加入到帧间匹配的计算中，这样在目标被遮挡但后续再次出现的情况下，还能正确匹配这个ID，从而减少ID的切换，达到持续跟踪的目的。 1.1.5. 目标跟踪 项目地址 https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch，使用的是Pytorch深度学习框架，联合YOLOv5和DeepSort两个目前很火且效果非常不错的算法工程，实现特定物体的目标跟踪。 git clone https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch.git cd Yolov5_DeepSort_Pytorch pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html pip install -r requirements.txt 然后去下载权重文件，YOLOv5的权重文件放置在 [yolov5](https://xugaoxiang.com/tag/yolov5/)/weights文件夹下，DeepSort的权重文件ckpt.t7放置在deep_sort/deep/checkpoint文件夹下 下载链接，百度网盘下载地址， 提取码：u5v3 找个测试视频，来看看效果吧 python track.py --source test.mp4 测试效果图 1.1.6. 参考资料 https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch https://github.com/ZQPei/deep_sort_pytorch https://pytorch.org/get-started/locally/ "},"5 - 实验笔记/Yolo+Deepsort/Market 1501 数据集及在 DeepSort 中的训练.html":{"url":"5 - 实验笔记/Yolo+Deepsort/Market 1501 数据集及在 DeepSort 中的训练.html","title":"Market 1501 数据集及在 DeepSort 中的训练","keywords":"","body":"1.1. Market 1501 数据集1.2. deepsort 模型训练 本文由 简悦 SimpRead 转码， 原文地址 my.oschina.net 软硬件环境 ubuntu 18.04 64bit GTX 1070Ti anaconda with python 3.7 pytorch 1.6 cuda 10.1 1.1. Market 1501 数据集 Market-1501数据集是在清华大学校园中采集，在夏天拍摄，于 2015 年构建并公开。它包括由 6 个摄像头 (其中 5 个高清摄像头和 1 个低分辨率摄像头) 拍摄到的 1501 个行人、32668 个检测到的行人矩形框。每个行人至少有 2 个摄像头捕捉到，并且在一个摄像头中可能具有多张图像。训练集有 751 人，包含 12936 张图像，平均每个人有 17.2 张训练数据；测试集有 750 人，包含 19732 张图像，平均每个人有 26.3 张测试数据。 数据集目录结构 Market-1501-v15.09.15├── bounding_box_test├── bounding_box_train├── gt_bbox├── gt_query├── query└── readme.txt 包含四个文件夹 bounding_box_test: 测试集 bounding_box_train: 训练集 query: 共有 750 个身份。每个摄像机随机选择一个查询图像 gt_query: 包含实际标注 gt_bbox: 手绘边框，主要用于判断自动检测器 DPM边界框是否良好 图片命名规则 以0001_c1s1_000151_01.jpg为例 0001 表示每个人的标签编号，从 0001 到 1501，共有 1501 个人 c1表示第一个摄像头 ( c是 camera)，共有 6 个摄像头 s1 表示第一个录像片段 ( s是 sequence)，每个摄像机都有多个录像片段 000151 表示 c1s1的第 000151 帧图片，视频帧率 fps 为 25 01 表示 c1s1_001051这一帧上的第 1 个检测框，由于采用 DPM自动检测器，每一帧上的行人可能会有多个，相应的标注框也会有多个。00 则表示手工标注框 数据集下载地址： 链接：https://pan.baidu.com/s/1i9aiZx-EC3fjhn3uWTKZjw提取码：up8x 1.2. deepsort 模型训练 前文 《基于 YOLOv5 和 DeepSort 的目标跟踪》 https://xugaoxiang.com/2020/10/17/yolov5-deepsort-pytorch/ 介绍过利用YOLOv5和DeepSort来实现目标的检测及跟踪。现在我们使用Market 1501数据集来训练跟踪器模型。 至于YOLOv5检测模型的训练，参考前面的博文 YOLOv5 模型训练。我们使用原作者提供的yolov5s.pt就可行。 依赖环境就不再说了，参考前文 git clone --recurse-submodules https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch.gitcd Yolov5_DeepSort_Pytorch/deep_sort/deep_sort/deep 接下来将数据集Market拷贝到Yolov5_DeepSort_Pytorch/deep_sort/deep_sort/deep下然后解压，数据集存放的位置是随意的，可以通过参数--data-dir指定。 针对原项目中的训练代码train.py，需要做点修改 将 train_dir = os.path.join(root,\"train\")test_dir = os.path.join(root,\"test\") 改成 train_dir = os.path.join(root,\"\")test_dir = os.path.join(root,\"\") 然后将数据集中的文件夹bounding_box_train重命名为train，bounding_box_test重命名为test。不然的话，训练的时候就会报下面 2 个错 Traceback (most recent call last): File \"train.py\", line 43, in torchvision.datasets.ImageFolder(train_dir, transform=transform_train), File \"/home/xugaoxiang/anaconda3/envs/deepsort/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 208, in __init__ is_valid_file=is_valid_file) File \"/home/xugaoxiang/anaconda3/envs/deepsort/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 100, in __init__ raise RuntimeError(msg)RuntimeError: Found 0 files in subfolders of: data/trainSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp 和 Traceback (most recent call last): File \"train.py\", line 43, in torchvision.datasets.ImageFolder(train_dir, transform=transform_train), File \"/home/xugaoxiang/anaconda3/envs/deepsort/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 208, in __init__ is_valid_file=is_valid_file) File \"/home/xugaoxiang/anaconda3/envs/deepsort/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 94, in __init__ classes, class_to_idx = self._find_classes(self.root) File \"/home/xugaoxiang/anaconda3/envs/deepsort/lib/python3.7/site-packages/torchvision/datasets/folder.py\", line 123, in _find_classes classes = [d.name for d in os.scandir(dir) if d.is_dir()]FileNotFoundError: [Errno 2] No such file or directory: 'Market-1501-v15.09.15/train' 最后，还需要改个地方，编辑model.py，将 def __init__(self, num_classes=751 ,reid=False): 改成 def __init__(self, num_classes=2 ,reid=False): 然后就可以开始训练了 python train.py --data-dir Market-1501-v15.09.15 deepsort_market_pytorch deepsort_market_pytorch 训练结束后，会在checkpoint下生成模型文件ckpt.t7，找个视频，测试一下 deepsort_market_pytorch 参考资料 https://xugaoxiang.com/2020/10/17/yolov5-deepsort-pytorch/ https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch https://github.com/ZQPei/deep_sort_pytorch https://github.com/ZQPei/deep_sort_pytorch/issues/134 https://github.com/ZQPei/deep_sort_pytorch/issues/105 本文分享自微信公众号 - 迷途小书童的 Note（Dev_Club）。如有侵权，请联系 support@oschina.cn 删除。本文参与 “OSC 源创计划”，欢迎正在阅读的你也加入，一起分享。 "},"5 - 实验笔记/Yolo+Deepsort/yolov5 启用数据增强与 tensorboard 可视化.html":{"url":"5 - 实验笔记/Yolo+Deepsort/yolov5 启用数据增强与 tensorboard 可视化.html","title":"yolov5 启用数据增强与 tensorboard 可视化","keywords":"","body":"1.1. yolov5 启用数据增强与 tensorboard 可视化1.2. 一，yolov5 启用数据增强1.3. 二，tensorboard 可视化 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1. yolov5 启用数据增强与 tensorboard 可视化 1.2. 一，yolov5 启用数据增强 1.data 目录下，有两个 hyp 的文件：data/hyp.scratch.yaml 和 data/hyp.finetune.yaml 具体内容如下： # Hyperparameters for VOC fine-tuning # python train.py --batch 64 --cfg '' --weights yolov5m.pt --data voc.yaml --img 512 --epochs 50 # See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials lr0: 0.01 # initial learning rate (SGD=1E-2, Adam=1E-3) momentum: 0.94 # SGD momentum/Adam beta1 weight_decay: 0.0005 # optimizer weight decay 5e-4 giou: 0.05 # GIoU loss gain cls: 0.4 # cls loss gain cls_pw: 1.0 # cls BCELoss positive_weight obj: 0.5 # obj loss gain (scale with pixels) obj_pw: 1.0 # obj BCELoss positive_weight iou_t: 0.20 # IoU training threshold anchor_t: 4.0 # anchor-multiple threshold fl_gamma: 0.0 # focal loss gamma (efficientDet default gamma=1.5) hsv_h: 0.015 # image HSV-Hue augmentation (fraction) hsv_s: 0.7 # image HSV-Saturation augmentation (fraction) hsv_v: 0.6 # image HSV-Value augmentation (fraction) degrees: 1.0 # image rotation (+/- deg) translate: 0.1 # image translation (+/- fraction) scale: 0.6 # image scale (+/- gain) shear: 1.0 # image shear (+/- deg) perspective: 0.0 # image perspective (+/- fraction), range 0-0.001 flipud: 0.01 # image flip up-down (probability) fliplr: 0.5 # image flip left-right (probability) mixup: 0.2 # image mixup (probability) 启用方法在 train.py 中添加指定，当然程序本身也会默认启动 hyp.scratch.yaml 这个，可以直接修改其内部参数，如果需要启用另一个，可以如图：训练时会在终端打印显示出相关参数设置情况 1.3. 二，tensorboard 可视化 良心 yolov5！感觉好多东西都直接写好了，调用即可。models/yolo.py 中，代码最底部作者将 tensorboard 代码注释了，启用即可。取消注释后，点击启动 tensorboard 会话。vs code 上出现如下提示：直接点击使用当前目录时，无法查看效果。需要定位到 runs 文件夹。点击‘选择另一个文件夹’，找到 runs 文件夹。效果如图： "},"5 - 实验笔记/Yolo+Deepsort/YOLOv5 的详细使用教程，以及使用 yolov5 训练自己的数据集.html":{"url":"5 - 实验笔记/Yolo+Deepsort/YOLOv5 的详细使用教程，以及使用 yolov5 训练自己的数据集.html","title":"YOLOv5 的详细使用教程，以及使用 yolov5 训练自己的数据集","keywords":"","body":"1.1. 1.1 克隆项目1.2. 1.2 安装必要的环境依赖1.3. 2.1 下载预训练模型1.3.1. 2.1.1 执行脚本下载预训练模型1.3.2. 2.1.2 直接下载预训练模型，然后保存到/yolov5/weights目录下即可，我已经把预训练模型的url提取出来1.4. 2.2 下载标注的数据集1.4.1. 2.2.1 执行脚本下载1.4.2. 2.2.2 如果下载比较慢，也可以通过 url 链接直接下载coco128.zip1.5. 3.1 创建训练数据集的配置文件 Dataset.yaml1.6. 3.2 创建标签（Labels）1.7. 3.3 组织文件结构1.8. 3.4 选择一个模型训练1.9. 3.5 开始训练1.10. 3.5.1 训练命令1.11. 3.5.2 训练常见错误 11.12. 3.5.3 训练常见错误 21.13. 3.5.4 训练常见错误 31.14. 3.6 使用 tensorboard 可视化结果1.15. 3.7 测试1.16. 4.1 准备数据集1.17. 4.2 修改数据和模型配置文件1.17.1. 4.2.1 修改数据配置文件1.17.2. 4.2.2 修改模型配置文件1.18. 4.3 训练自己的数据集1.19. 4.3.1 使用 yolovs.pt 预训练模型进行训练1.19.1. 4.3.2 使用 yolov5l.pt 预训练模型进行训练1.20. 4.4 使用训练好的预训练模型进行测试1.21. 4.5 在 Tensorbaord 上查看数据的训练过程中的一些指标1.22. 5.1 图像推理测试1.23. 5.2 目录推理测试1.24. 5.3 视频推理测试1.25. 5.4 网络摄像头推理测试1.26. 5.5 http 流推理测试1.27. 5.6 rtsp 流推理测试1.28. 6.1 训练的模型的测试表现可视化1.29. 6.2 训练损失和性能指标视化 本文由 简悦 SimpRead 转码， 原文地址 shliang.blog.csdn.net 欢迎大家关注笔者，你的关注是我持续更博的最大动力 原创文章，转载告知，盗版必究 本人环境声明： 系统环境：Ubuntu18.04.1 cuda版本：10.2.89 cudnn版本：7.6.5 torch版本：1.5.0 torchvision版本：0.6.0 项目代码yolov5，官网，项目开源的时间：20200601 自定义数据集： 1.1. 1.1 克隆项目 git clone https://github.com/ultralytics/yolov5 # clone repo 如果下载比较慢，建议使用下面的镜像下载： git clone https://github.com.cnpmjs.org/ultralytics/yolov5 # clone repo 1.2. 1.2 安装必要的环境依赖 官方给出的要求是：python>=3.7、PyTorch>=1.5，安装依赖： cd yolov5pip install -U -r requirements.txtrequirements.txt # pip install -U -r requirements.txt Cython numpy==1.17 opencv-python torch>=1.5 matplotlib pillow tensorboard PyYAML>=5.3 torchvision scipy tqdm git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI # Nvidia Apex (optional) for mixed precision training -------------------------- # git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex # Conda commands (in place of pip) --------------------------------------------- # conda update -yn base -c defaults conda # conda install -yc anaconda numpy opencv matplotlib tqdm pillow ipython # conda install -yc conda-forge scikit-image pycocotools tensorboard # conda install -yc spyder-ide spyder-line-profiler # conda install -yc pytorch pytorch torchvision # conda install -yc conda-forge protobuf numpy && pip install onnx # https://github.com/onnx/onnx#linux-and-macos 1.3. 2.1 下载预训练模型 1.3.1. 2.1.1 执行脚本下载预训练模型 /yolov5/weights/download_weights.sh脚本定义下载预训练模型，脚本代码内容如下： #!/bin/bash # Download common models python3 -c \"from utils.google_utils import *; attempt_download('weights/yolov5s.pt'); attempt_download('weights/yolov5m.pt'); attempt_download('weights/yolov5l.pt'); attempt_download('weights/yolov5x.pt')\" attempt_download函数在/yolov5/utils/google_utils.py脚本中定义 1.3.2. 2.1.2 直接下载预训练模型，然后保存到/yolov5/weights目录下即可，我已经把预训练模型的url提取出来 大家直接在google driver中下载即可，地址（可能需要科学上网）： 点我——》带你去：https://drive.google.com/drive/folders/1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J 1.4. 2.2 下载标注的数据集 1.4.1. 2.2.1 执行脚本下载 python3 -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f','coco128.zip')\" # download dataset 执行上面的代码，会下载：coco128.zip数据集，该数据是COCO train2017数据的一部分，只取了 coco 数据集中的 128 张标注的图片，coco128.zip 下载完后解压到/yolov5目录下即可，解压后的 coco128 文件结构如下： coco128 |-- LICENSE |-- README.txt # 相关说明 |-- annotations # 空目录 |-- images # 128张jpg图片 `-- labels # 128张标注的txt文件 /yolov5/utils/google_utils.py脚本是下载预训练模型和标注的训练数据集，该脚本代码内容如下： # This file contains google utils: https://cloud.google.com/storage/docs/reference/libraries # pip install --upgrade google-cloud-storage # from google.cloud import storage import os import time from pathlib import Path def attempt_download(weights): # Attempt to download pretrained weights if not found locally weights = weights.strip() msg = weights + ' missing, try downloading from https://drive.google.com/drive/folders/1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J' r = 1 if len(weights) > 0 and not os.path.isfile(weights): d = {'yolov3-spp.pt': '1mM67oNw4fZoIOL1c8M3hHmj66d8e-ni_', # yolov3-spp.yaml 'yolov5s.pt': '1R5T6rIyy3lLwgFXNms8whc-387H0tMQO', # yolov5s.yaml 'yolov5m.pt': '1vobuEExpWQVpXExsJ2w-Mbf3HJjWkQJr', # yolov5m.yaml 'yolov5l.pt': '1hrlqD1Wdei7UT4OgT785BEk1JwnSvNEV', # yolov5l.yaml 'yolov5x.pt': '1mM8aZJlWTxOg7BZJvNUMrTnA2AbeCVzS', # yolov5x.yaml } file = Path(weights).name if file in d: r = gdrive_download(id=d[file], name=weights) if not (r == 0 and os.path.exists(weights) and os.path.getsize(weights) > 1E6): # weights exist and > 1MB os.remove(weights) if os.path.exists(weights) else None # remove partial downloads s = \"curl -L -o %s 'https://storage.googleapis.com/ultralytics/yolov5/ckpt/%s'\" % (weights, file) r = os.system(s) # execute, capture return values # Error check if not (r == 0 and os.path.exists(weights) and os.path.getsize(weights) > 1E6): # weights exist and > 1MB os.remove(weights) if os.path.exists(weights) else None # remove partial downloads raise Exception(msg) def gdrive_download(id='1HaXkef9z6y5l4vUnCYgdmEAj61c6bfWO', name='coco.zip'): # https://gist.github.com/tanaikech/f0f2d122e05bf5f971611258c22c110f # Downloads a file from Google Drive, accepting presented query # from utils.google_utils import *; gdrive_download() t = time.time() print('Downloading https://drive.google.com/uc?export=download&id=%s as %s... ' % (id, name), end='') os.remove(name) if os.path.exists(name) else None # remove existing os.remove('cookie') if os.path.exists('cookie') else None # Attempt file download os.system(\"curl -c ./cookie -s -L \\\"https://drive.google.com/uc?export=download&id=%s\\\" > /dev/null\" % id) if os.path.exists('cookie'): # large file s = \"curl -Lb ./cookie \\\"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=%s\\\" -o %s\" % ( id, name) else: # small file s = \"curl -s -L -o %s 'https://drive.google.com/uc?export=download&id=%s'\" % (name, id) r = os.system(s) # execute, capture return values os.remove('cookie') if os.path.exists('cookie') else None # Error check if r != 0: os.remove(name) if os.path.exists(name) else None # remove partial print('Download error ') # raise Exception('Download error') return r # Unzip if archive if name.endswith('.zip'): print('unzipping... ', end='') os.system('unzip -q %s' % name) # unzip os.remove(name) # remove zip to free space print('Done (%.1fs)' % (time.time() - t)) return r # def upload_blob(bucket_name, source_file_name, destination_blob_name): # # Uploads a file to a bucket # # https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python # # storage_client = storage.Client() # bucket = storage_client.get_bucket(bucket_name) # blob = bucket.blob(destination_blob_name) # # blob.upload_from_filename(source_file_name) # # print('File {} uploaded to {}.'.format( # source_file_name, # destination_blob_name)) # # # def download_blob(bucket_name, source_blob_name, destination_file_name): # # Uploads a blob from a bucket # storage_client = storage.Client() # bucket = storage_client.get_bucket(bucket_name) # blob = bucket.blob(source_blob_name) # # blob.download_to_filename(destination_file_name) # # print('Blob {} downloaded to {}.'.format( # source_blob_name, # destination_file_name)) 1.4.2. 2.2.2 如果下载比较慢，也可以通过 url 链接直接下载coco128.zip 点我——》带你去：https://drive.google.com/uc?export=download&id=1n_oKgR81BJtqk75b00eAjdv03qVCQn2f 上面下载好预训练 准备好上面的环境和下载好文件之后，就可以开始自定义自己的数据集，进行训练啦！ 1.5. 3.1 创建训练数据集的配置文件 Dataset.yaml 上面下载好coco128.zip小型数据集之后，这些数据集可以用于训练和验证/content/yolov5/models/yolov5l.yaml。coco128.yaml中定义了： 训练图片的路径（或训练图片列表的.txt文件） 与验证集相同的图片 目标的类别数 类名列表 下面是/data/coco128.yaml文件中定义的内容： # COCO 2017 dataset http://cocodataset.org - first 128 training images # Download command: python -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f','coco128.zip')\" # Train command: python train.py --data ./data/coco128.yaml # Dataset should be placed next to yolov5 folder: # /parent_folder # /coco128 # /yolov5 # 训练集和验证集 （图片的目录路径或 *.txt图片路径） train: ../coco128/images/train2017/ val: ../coco128/images/train2017/ # 类别数 number of classes nc: 80 # 类别列表 class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] 1.6. 3.2 创建标签（Labels） 对数据集进行打标签，可以选择如下两种打标工具： Labelbox CVAT 也可以使用 LabelImg，选用 ylolo 格式进行标注 将标签导出为darknet格式，每个标注图像有一个*.txt文件（如果图像中没有对象，则不需要*.txt文件），*.txt文件格式如下： 每行一个对象 每行都是：class x_center y_center width height格式 框的坐标格式必须采用归一化格式的xywh（从0到1），如果你框以像素为单位，则将x_center和width除以图像宽度，将y_center和height除以图像的高度 类别是从索引0开始的 通过在器路径名中将/images/*.jpg替换为/label/*.txt，可以定位每个图像的标签文件，示例图像和标签对为： dataset/images/train2017/000000109622.jpg # image dataset/labels/train2017/000000109622.txt # label 例如：000000000009.txt标签文件，表示000000000009.jpg图片中标注了 8 个目标： 45 0.479492 0.688771 0.955609 0.5955 45 0.736516 0.247188 0.498875 0.476417 50 0.637063 0.732938 0.494125 0.510583 45 0.339438 0.418896 0.678875 0.7815 49 0.646836 0.132552 0.118047 0.096937 49 0.773148 0.129802 0.090734 0.097229 49 0.668297 0.226906 0.131281 0.146896 49 0.642859 0.079219 0.148063 0.148062 1.7. 3.3 组织文件结构 根据下图整理自己的训练集和验证集图片及标签。注意：/coco128目录应该和yolov5目录同级，同时确保coco128/labels和coco128/images两个目录同级！ 1.8. 3.4 选择一个模型训练 上面已经修改了自定义数据集的配置文件，同时组织好了数据。下面就可以选择一个模型进行训练了。 从./models目录下选择一个模型的配置文件，这里我们选择yolov5s.ymal，这是一个最小最快的模型。关于其他模型之间的比较下面介绍。选择好模型之后，如果你使用的不是 coco 数据集进行训练，而是自定义的数据集，此时只需要修改*.yaml配置文件中的nc: 80参数和数据的类别列表 下面是yolo5s.ymal配置文件的内容： # parameters nc: 80 # number of classes depth_multiple: 0.33 # model depth multiple width_multiple: 0.50 # layer channel multiple # anchors anchors: - [116,90, 156,198, 373,326] # P5/32 - [30,61, 62,45, 59,119] # P4/16 - [10,13, 16,30, 33,23] # P3/8 # YOLOv5 backbone backbone: # [from, number, module, args] [[-1, 1, Focus, [64, 3]], # 0-P1/2 [-1, 1, Conv, [128, 3, 2]], # 1-P2/4 [-1, 3, BottleneckCSP, [128]], [-1, 1, Conv, [256, 3, 2]], # 3-P3/8 [-1, 9, BottleneckCSP, [256]], [-1, 1, Conv, [512, 3, 2]], # 5-P4/16 [-1, 9, BottleneckCSP, [512]], [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32 [-1, 1, SPP, [1024, [5, 9, 13]]], ] # YOLOv5 head head: [[-1, 3, BottleneckCSP, [1024, False]], # 9 [-1, 1, Conv, [512, 1, 1]], [-1, 1, nn.Upsample, [None, 2, 'nearest']], [[-1, 6], 1, Concat, [1]], # cat backbone P4 [-1, 3, BottleneckCSP, [512, False]], # 13 [-1, 1, Conv, [256, 1, 1]], [-1, 1, nn.Upsample, [None, 2, 'nearest']], [[-1, 4], 1, Concat, [1]], # cat backbone P3 [-1, 3, BottleneckCSP, [256, False]], [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]], # 18 (P3/8-small) [-2, 1, Conv, [256, 3, 2]], [[-1, 14], 1, Concat, [1]], # cat head P4 [-1, 3, BottleneckCSP, [512, False]], [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]], # 22 (P4/16-medium) [-2, 1, Conv, [512, 3, 2]], [[-1, 10], 1, Concat, [1]], # cat head P5 [-1, 3, BottleneckCSP, [1024, False]], [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]], # 26 (P5/32-large) [[], 1, Detect, [nc, anchors]], # Detect(P5, P4, P3) ] yolov5s.yaml配置文件中主要定义了： 参数（parameters）：类别等 anchor YOLOv5 backbone YOLOv5 head 1.9. 3.5 开始训练 1.10. 3.5.1 训练命令 上面一切准备就绪，可以开始训练啦 运行下面的命令训练coco128.ymal，训练 5epochs。可以有两种训练方式，如下参数： --cfg yolov5s.yaml --weights ''：从头开始训练 --cfg yolov5s.yaml --weights yolov5s.pt：从预训练的模型加载开始训练 YOLOv5 在 coco128 上训练 5epochs 的命令： python train.py --img 640 --batch 16 --epochs 5 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights '' 训练的更多可选参数： --epochs：训练的 epoch，默认值300 --batch-size：默认值16 --cfg：模型的配置文件，默认为yolov5s.yaml --data：数据集的配置文件，默认为data/coco128.yaml --img-size：训练和测试输入大小，默认为[640, 640] --rect：rectangular training，布尔值 --resume：是否从最新的last.pt中恢复训练，布尔值 --nosave：仅仅保存最后的 checkpoint，布尔值 --notest：仅仅在最后的 epoch 上测试，布尔值 --evolve：进化超参数（evolve hyperparameters），布尔值 --bucket：gsutil bucket，默认值'' --cache-images：缓存图片可以更快的开始训练，布尔值 --weights：初始化参数路径，默认值'' --name：如果提供，将results.txt重命名为results_name.txt --device：cuda 设备，例如：0或0,1,2,3或cpu，默认'' --adam：使用adam优化器，布尔值 --multi-scale：改变图片尺寸 img-size +/0- 50%，布尔值 --single-cls：训练单个类别的数据集，布尔值 1.11. 3.5.2 训练常见错误 1 1、执行训练命令报错：RuntimeError: Model replicas must have an equal number of parameters.，错误显示，模型的副本必须有相同的参数 2、解决方式：这个可能是由于 Pytorch 的版本问题导致的错误，我的 torch 版本为15.0，把版本降为1.4.0即可（参考）： pip install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html 1.12. 3.5.3 训练常见错误 2 1、执行训练命令报错：ModuleNotFoundError: No module named 'yaml' 2、解决方式：这是由于没有安装yaml库包错误，虽然导入是：import yaml，但是安装的名字却不是yaml，下面是正确安装 yaml： pip install PyYAML 1.13. 3.5.4 训练常见错误 3 1、执行训练命令报错：AttributeError: 'DistributedDataParallel' object has no attribute 'model' 2、错误解决方式：这个是由于--device的默认值为''，此时默认会使用多 GPU 进行训练，但是多 GPU 训练时就会出现上面这个问题，这可能时一个 bug（参考），解决方式就是使用单 GPU，把训练命令改成如下： python train.py --img 640 --batch 16 --epochs 5 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights '' --device 0 1.14. 3.6 使用 tensorboard 可视化结果 在 yolov5 目录下，使用： tensorboard --logdir=runs 然后把返回的 url 地址粘贴到浏览器中即可！我测试显示结果如下： 注意：如果返回拒绝了我们的请求，可以在 tensorboard 的后面加上参数--port ip： tensorboard --logdir=runs --host=192.168.0.134 1.15. 3.7 测试 测试的更多可选参数： --weights ：预训练模型路径，默认值weights/yolov5s.pt --data：数据集的配置文件，默认为data/coco.yaml --batch-size：默认值32 --img-size：推理大小（pixels），默认640 --conf-thres：目标置信度阈值，默认0.001 --iou-thres：NMS 的 IOU 阈值，默认0.65 --save-json：把结果保存为 cocoapi-compatible 的 json 文件 --task：默认val，可选其他值：val, test, study --device：cuda 设备，例如：0或0,1,2,3或cpu，默认'' --half：半精度的 FP16 推理 --single-cls：将其视为单类别，布尔值 --augment：增强推理，布尔值 --verbose：显示类别的mAP，布尔值 测试命令示例： python test.py --weights yolov5s.pt --data ./data/coco.yaml --img 640 数据准备有两种方式： 一种是直接指定训练集和测试集图片的路径（本文使用的这种方法） 另外一种是给出训练和测试集图片的 txt 文件 1.16. 4.1 准备数据集 yolov5中的数据集的标签都是保存为YOLO格式的txt文件的，关于： 怎么标注数据集 VOC 数据和 YOLO 数据格式时是什么样的 怎么把 VOC 格式数据转化为 YOLO 格式数据 以及 VOC 格式和 YOLO 格式相互转化计算过程 请参考：这篇博客，这里不在赘述！！！ 数据集标注好之后，存放如下目录格式： (yolov5) shl@zfcv:~/shl/yolov5$ tree hat_hair_beard hat_hair_beard ├── images │ ├── train2017 # 训练集图片，这里我只列举几张示例 │ │ ├── 000050.jpg │ │ ├── 000051.jpg │ │ └── 000052.jpg │ └── val2017 # 验证集图片 │ ├── 001800.jpg │ ├── 001801.jpg │ └── 001802.jpg └── labels ├── train2017 # 训练集的标签文件 │ ├── 000050.txt │ ├── 000051.txt │ └── 000052.txt └── val2017 # 验证集的标签文件 ├── 001800.txt ├── 001801.txt └── 001802.txt 6 directories, 13 files (yolov5) shl@zfcv:~/shl/yolov5$ had_hair_beard：存放数据的目录，该目录位于yolov5目录下 images：目录下存放的是图片，包含训练集和验证集图片 labels：目录下存放的是标签文件，包含训练集和验证集图片对应的标签文件 按照上面的结构组织好数据的目录结构，然后就可以修改一些训练相关的文件了！ 1.17. 4.2 修改数据和模型配置文件 1.17.1. 4.2.1 修改数据配置文件 原先的配置文件为：./yolov5/data/coco128.yaml，该文件中内容为： (yolov5) shl@zfcv:~/shl/yolov5/data$ ls Annotations coco.yaml hat_hair_beard.yaml JPEGImages coco128.yaml get_coco2017.sh ImageSets VOC2007 (yolov5) shl@zfcv:~/shl/yolov5/data$ cat coco128.yaml # COCO 2017 dataset http://cocodataset.org - first 128 training images # Download command: python -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f','coco128.zip')\" # Train command: python train.py --data ./data/coco128.yaml # Dataset should be placed next to yolov5 folder: # /parent_folder # /coco128 # /yolov5 # train and val datasets (image directory or *.txt file with image paths) train: ../coco128/images/train2017/ val: ../coco128/images/train2017/ # number of classes nc: 80 # class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'](yolov5) shl@zfcv:~/shl/yolov5/data$ 我们把该文件拷贝一份（这是我个人的习惯，你也可以不改，直接在 coco128.yaml 文件中进修改） cp coco128.yaml hat_hair_beard.yaml 然后在hat_hair_beard.yaml中需要修改3处内容： 1、训练集和验证集图片的路径 train: /home/shl/shl/yolov5/hat_hair_beard/images/train2017 val: /home/shl/shl/yolov5/hat_hair_beard/images/val2017 注意： 最好用绝对路径，我在使用相对路径的时候报错，说路径存在 2、修改类别数nc nc=7 #我数据集一共分 7 个类别 3、修改类别列表，把类别修改为自己的类别 names: ['hard_hat', 'other', 'regular', 'long_hair', 'braid', 'bald', 'beard'] 修改后的hat_hair_beard.yaml完整配置内容如下： # COCO 2017 dataset http://cocodataset.org - first 128 training images # Download command: python -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f','coco128.zip')\" # Train command: python train.py --data ./data/coco128.yaml # Dataset should be placed next to yolov5 folder: # /parent_folder # /coco128 # /yolov5 # train and val datasets (image directory or *.txt file with image paths) #train: ../my_dataset/hat_hair_beard/images/train2017/ #val: ../my_dataset/hat_hair_beard/images/train2017/ #train: ../hat_hair_beard/images/train2017 train: /home/shl/shl/yolov5/hat_hair_beard/images/train2017 #val: ../hat_hair_beard/images/val2017 val: /home/shl/shl/yolov5/hat_hair_beard/images/val2017 # number of classes nc: 7 # class names names: ['hard_hat', 'other', 'regular', 'long_hair', 'braid', 'bald', 'beard'] 1.17.2. 4.2.2 修改模型配置文件 修改模型配置文件，这里我使用的是yolov5/models/yolov5s.yaml模型的配置文件，个人习惯我还是把改配置文件拷贝一份，为：hat_hair_beard_yolov5s.yaml（你也可以不改，直接在 yolov5.yaml 中修改）yolov5s.yaml配置文件中原内容为： (yolov5) shl@zfcv:~/shl/yolov5/models$ cat yolov5s.yaml # parameters nc: 80 # number of classes depth_multiple: 0.33 # model depth multiple width_multiple: 0.50 # layer channel multiple # anchors anchors: - [10,13, 16,30, 33,23] # P3/8 - [30,61, 62,45, 59,119] # P4/16 - [116,90, 156,198, 373,326] # P5/32 # yolov5 backbone backbone: # [from, number, module, args] [[-1, 1, Focus, [64, 3]], # 1-P1/2 [-1, 1, Conv, [128, 3, 2]], # 2-P2/4 [-1, 3, Bottleneck, [128]], [-1, 1, Conv, [256, 3, 2]], # 4-P3/8 [-1, 9, BottleneckCSP, [256]], [-1, 1, Conv, [512, 3, 2]], # 6-P4/16 [-1, 9, BottleneckCSP, [512]], [-1, 1, Conv, [1024, 3, 2]], # 8-P5/32 [-1, 1, SPP, [1024, [5, 9, 13]]], [-1, 6, BottleneckCSP, [1024]], # 10 ] # yolov5 head head: [[-1, 3, BottleneckCSP, [1024, False]], # 11 [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]], # 12 (P5/32-large) [-2, 1, nn.Upsample, [None, 2, 'nearest']], [[-1, 6], 1, Concat, [1]], # cat backbone P4 [-1, 1, Conv, [512, 1, 1]], [-1, 3, BottleneckCSP, [512, False]], [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]], # 17 (P4/16-medium) [-2, 1, nn.Upsample, [None, 2, 'nearest']], [[-1, 4], 1, Concat, [1]], # cat backbone P3 [-1, 1, Conv, [256, 1, 1]], [-1, 3, BottleneckCSP, [256, False]], [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1]], # 22 (P3/8-small) [[], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5) ] (yolov5) shl@zfcv:~/shl/yolov5/models$ 因此，在hat_hair_beard.yaml中只需要修改一处，把nc修改为自己的类别数即可 nc : 7 上面数据的准备和配置文件的修改全部准备好之后，就可以开始训练了！！！ 1.18. 4.3 训练自己的数据集 1.19. 4.3.1 使用 yolovs.pt 预训练模型进行训练 训练命令： python train.py --img 640 --batch 16 --epochs 300 --data ./data/hat_hair_beard.yaml --cfg ./models/hat_hair_beard_yolov5s.yaml --weights ./weights/yolov5s.pt --device 1 关于参数的介绍，上面已经介绍了，这里不再赘述 正常训练，如下图： 训练结束后，会生成两个预训练的模型： best.pt：保存的是中间一共比较好模型 last.pt：训练结束后保存的最后模型 我把最终训练的模型保存拷贝一份，防止下载再训练给覆盖，白白训练！ 注意： 当使用--device参数设置多GPU进行训练时，可能会报错：RuntimeError: Model replicas must have an equal number of parameters. 具体错误如下图，会报错的命令： --device 1：指定单个 GPU 不会报错 --device 1,2,3：当使用两个以上的 GPU 会报错 官方的 iusses 解决方式：把 torch 的把那本更新为torch1.4，而我的torch为1.5版本 1.19.1. 4.3.2 使用 yolov5l.pt 预训练模型进行训练 这是用 yolov5l.pt 预训练模型训练的效果 可以看到，使用yolov5l.pt预训练模型，保存的最终模型和最优模型都比较大： best.pt：382M last.pt：192M 1.20. 4.4 使用训练好的预训练模型进行测试 1、单张图片测试命令： python detect.py --source inference/1_input/1_img/hat3.jpg --we ights ./weights/last_hat_hair_beard_20200804.pt --output inference/2_output/1_img/ --device 1 原图： 测试结果图：从结果可以看到，置信度还是很高的，而且目标都检测到了！（保存的结果图片，会把上一此保存的测试结果图删了，最终只保存最后一侧测试的结果图） 2、图片目录测试 python detect.py --source inference/1_input/2_imgs_hat --weights ./weights/last_hat_hair_beard_20200804.pt --output inference/2_output/2_imgs_hat --device 1 下面是测试图片集的效果： 1.21. 4.5 在 Tensorbaord 上查看数据的训练过程中的一些指标 使用如下命令，然后在浏览器中查看一些数据指标的可视化图 tensorbaord --logdir=runs 1、scalars 2、images 使用预训练的模型进行推理测试，YOLOv5支持多种数据源推理测试，如下： 图像 目录 视频 网络摄像头 http 流 rtsp 流 使用python detect.py进行推理，关于推理的更多参数使用如下命令查看： python detect.py -h --weights ：预训练模型.pt的路径，默认值为：weights/yolov5s.pt --source：输入的数据源，可以是：图片、目录、视频、网络摄像头、http和rtsp流，默认值为：interence/images --output： 输出检测结果的路径，默认值为：inference/output --img-size ：用于推理图片的大小（pixels），默认值为：640 --conf-thres：对象的置信度阈值（object confidence threshold），默认值为：0.4 --iou-thres ：NMS 的 IOU 阈值（ IOU threshold for NMS），默认值为：0.5 --fourcc：输出视频的编码格式（必须是 ffmepeg 支持的），例如：H264格式，默认格式为：mp4v --half： 使用版精度F16推理（half precision FP16 inference），布尔值，默认为true --device：cuda 设备，例如：0或0,1,2,3或cpu，默认'' --view-img ：显示结果，‘布尔值，默认为 true’ --save-txt ：把结果保存到*.txt文件中 --classes：过滤类别 CLASSES [CLASSES …]，filter by class --agnostic-nms：类不可知 NMS --augment：增强推理（augmented inference） 我把自己的推理的输出和输入组织成如下文件结构： (yolov5) shl@zfcv:~/shl/yolov5/inference$ tree -L 2 . ├── 1_input # 输入 │ ├── 1_img │ ├── 2_imgs │ ├── 3_video │ ├── 4_Webcam │ ├── 5_http │ └── 6_rtsp └── 2_output # 输出 ├── 1_img ├── 2_imgs ├── 3_video ├── 4_Webcam ├── 5_http └── 6_rtsp 1.22. 5.1 图像推理测试 1、推理图片命令 python detect.py --source inference/1_input/1_img/bus.jpg --weights ./weights/yolov5s.pt --output inference/2_output/1_img/ 直接指定输出结果保存的目录即可，保存名字和输入图片命令相同！2、测试结果如下 测试图片：测试结果： 1.23. 5.2 目录推理测试 1、推理目录下图片集命令 python detect.py --source inference/1_input/2_imgs --weights ./weights/yolov5s.pt --output inference/2_output/2_imgs 2、测试结果如下 如果检测中有些图片置信度比较低的可以通过--conf-thres参数过滤掉，例如： python detect.py --source inference/1_input/2_imgs --weights ./weights/yolov5s.pt --output inference/2_output/2_imgs --conf-thres 0.8 默认--conf-thres的值为0.4，如果我们把置信度的阈值设置为0.8，此时在下面的这图中就看不到检测目标的框！ 1.24. 5.3 视频推理测试 1、推理视频命令 python detect.py --source test.mp4 --weights ./weights/yolov5s.pt --output test_result/3_video 如果想指定输出视频的fourcc格式，用如下命令： python detect.py --source test.mp4 --weights ./weights/yolov5s.pt --output test_result/3_video --fourcc H264 关于ffmeg支持的fourcc格式（参考），注意：视频的格式必须时四个字符长度 2、推理视频结果 本来我是想上传视频的，但是CSDN目前只支持腾讯、优酷、哔哩哔哩，没有自己的视频服务器，如果上传还需要等待审核，比较慢，我我就直接放个 gif 动图，大家简单看下效果吧，又由于CSDN限制上传图片小于5M，因此只能截取不到 1 秒（泪奔，我不想再去压缩一下，费劲）： 1.25. 5.4 网络摄像头推理测试 1.26. 5.5 http 流推理测试 1.27. 5.6 rtsp 流推理测试 推理测试命令： # 示例语法（不要运行此单元格） python detect.py --source ./file.jpg # 图片 ./file.mp4 # 视频 ./dir # 目录 0 # 网络摄像头 'rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa' # rtsp流 'http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8' # http流 1.28. 6.1 训练的模型的测试表现可视化 1、训练开始后，浏览train*.jpg图像查看训练的图片（training images）、标签（labels）和数据增强的结果。注意：mosaic数据增强的数据用于训练（增强图片如下图所示），这是由UItralytics在YOLOv4中创建的一种图像数据增强方法。如果你发现你标注的标签有不正确的，你应该回去重新标注！ Image(filename='./train_batch1.jpg', width=900) # view augmented training mosaics 2、第一个 epoch 完成之后，查看test_batch0_gt.jpg，可以看到测试batch 0 ground truth的标签，如下图所示： Image(filename='./test_batch0_gt.jpg', width=900) # view test image labels 3、通过查看test_batch0_pred.jpg来查看test batch 0 predictions，结果下图所示： Image(filename='./test_batch0_pred.jpg', width=900) # view test image predictions 1.29. 6.2 训练损失和性能指标视化 训练损失（training losses）和性能指标（performance metrrics）被保存到Tensorboard和results.txt日志文件中。result.txt绘制训练完成之后的结果，保存为result.png。可以使用如下代码，绘制部分完成的results.txt from utils.utils import plot_results; plot_results() # plot results.txt as results.png Image(filename='./results.png', width=1000) # view results.png 这里我们显示YOLOv5s在coco128上训练100 epochs： 橘黄色曲线：是从头开始训练 蓝色曲线：从预训练的yolov5s.pt权重开始训练， 参考 1参考 2参考 3 ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ ⊕ ♠ "},"5 - 实验笔记/Yolo+Deepsort/Yolov5_DeepSort_Pytorch 训练自己的数据.html":{"url":"5 - 实验笔记/Yolo+Deepsort/Yolov5_DeepSort_Pytorch 训练自己的数据.html","title":"Yolov5_DeepSort_Pytorch 训练自己的数据","keywords":"","body":"1. 1. 环境2. 2. 目标检测的数据准备2.1. 1）数据标注2.2. 2）分训练集与验证集2.3. 3）修改 JPEGImages 为 images2.4. 4）xml 转为 txt 与生成最后训练使用的 train.txt 与 val.txt2.5. 5）修改训练配置（两处）3. 3. 训练目标检测模型4. 4. 准备分类 / 重识别数据（四处）5. 5. 训练分类 / 重识别模型6. 6. 测试跟踪（视频）7. 参考 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 关于基本的配置，请看【目标跟踪】Yolov5_DeepSort_Pytorch 复现 目录 1. 环境 2. 目标检测的数据准备 1）数据标注 2）分训练集与验证集 3）修改 JPEGImages 为 images 4）xml 转为 txt 与生成最后训练使用的 train.txt 与 val.txt 5）修改训练配置（两处） 3. 训练目标检测模型 4. 准备分类 / 重识别数据（四处） 5. 训练分类 / 重识别模型 6. 测试跟踪（视频） 参考 好吧。似乎，写到很详细了，大家对于数据集还是有一些疑问。我大致说一说，目标检测的数据集，可以只做检测，划分为一类就可以。 然后将对应的数据抠取出来，然后，将其分别划分到哪些类。分类的数据也可以来自其他的对应于想要跟踪的几类。 对于流程进行说明一下。请大家多思考，仔细跟着博客走。 1. 1. 环境 ubuntu16.04 cuda10.1 cudnn7 python3.6 Cython matplotlib>=3.2.2 numpy>=1.18.5 opencv-python>=4.1.2 Pillow PyYAML>=5.3 scipy>=1.4.1 tensorboard>=2.2 torch>=1.7.0 (my 1.7.1) torchvision>=0.8.1 (my 0.8.2) tqdm>=4.41.0 seaborn>=0.11.0 easydict thop pycocotools>=2.0 2. 2. 目标检测的数据准备 2.1. 1）数据标注 这里可以使用 cvat 标注，然后下载数据为 VOC： 目录 voc： 将数据放到 Yolov5_DeepSort_Pytorch/yolov5/data 目录下。 2.2. 2）分训练集与验证集 实际上我只有 train.txt 与 val.txt。 # -*- coding: UTF-8 -*- ''' @author: gu @contact: 1065504814@qq.com @time: 2021/3/4 上午11:52 @file: generate_txt.py @desc: reference https://blog.csdn.net/qqyouhappy/article/details/110451619 ''' import os import random trainval_percent = 1 train_percent = 0.9 xmlfilepath = 'datasets/voc/Annotations' txtsavepath = 'datasets/voc/ImageSets' total_xml = os.listdir(xmlfilepath) num = len(total_xml) list = range(num) tv = int(num * trainval_percent) tr = int(tv * train_percent) trainval = random.sample(list, tv) train = random.sample(trainval, tr) ftrainval = open('data/voc/ImageSets/Main/trainval.txt', 'w') ftest = open('data/voc/ImageSets/Main/test.txt', 'w') ftrain = open('data/voc/ImageSets/Main/train.txt', 'w') fval = open('data/voc/ImageSets/Main/val.txt', 'w') for i in list: name = total_xml[i][:-4] + '\\n' print(name) if i in trainval: ftrainval.write(name) if i in train: ftrain.write(name) else: fval.write(name) else: ftest.write(name) ftrainval.close() ftrain.close() fval.close() ftest.close() train.txt 里的文件名称，大概是这样： 003_000855 004_000146 002_000830 002_000720 002_002105 001_000888 2.3. 3）修改 JPEGImages 为 images 修改 JPEGImages 为 images。这个地方是因为 yolov5 默认读取 images 与 labels。 2.4. 4）xml 转为 txt 与生成最后训练使用的 train.txt 与 val.txt # voc_label.py import xml.etree.ElementTree as ET import pickle import os from os import listdir, getcwd from os.path import join sets = ['train', 'val'] classes = [\"***\"] # your class def convert(size, box): dw = 1. / size[0] dh = 1. / size[1] x = (box[0] + box[1]) / 2.0 y = (box[2] + box[3]) / 2.0 w = box[1] - box[0] h = box[3] - box[2] x = x * dw w = w * dw y = y * dh h = h * dh return (x, y, w, h) def convert_annotation(image_id): in_file = open('data/voc/Annotations/%s.xml' % (image_id)) out_file = open('data/voc/labels/%s.txt' % (image_id), 'w') tree = ET.parse(in_file) root = tree.getroot() size = root.find('size') w = int(size.find('width').text) h = int(size.find('height').text) for obj in root.iter('object'): # difficult = obj.find('difficult').text cls = obj.find('name').text if cls not in classes: continue cls_id = classes.index(cls) xmlbox = obj.find('bndbox') b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text)) bb = convert((w, h), b) out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n') wd = getcwd() print(wd) for image_set in sets: if not os.path.exists('data/voc/labels/'): os.makedirs('datasets/voc/labels/') image_ids = open('data/voc/ImageSets/Main/%s.txt' % (image_set)).read().strip().split() list_file = open('../data/voc/%s.txt' % (image_set), 'w') for image_id in image_ids: list_file.write('data/voc/images/%s.jpg\\n' % (image_id)) convert_annotation(image_id) print(image_id) list_file.close() 训练的 train.txt 文件，大概是这样（在 / data/voc / 目录上）： data/voc/images/003_000855.jpg data/voc/images/004_000146.jpg data/voc/images/002_000830.jpg data/voc/images/002_000720.jpg 此时的目录： data voc Annotations images ImageSets labels train.txt val.txt 2.5. 5）修改训练配置（两处） ---- 数据，在 data 目录下，复制 coco.yaml，修改为如下： # PASCAL VOC dataset http://host.robots.ox.ac.uk/pascal/VOC/ # Train command: python train.py --data voc.yaml # Default dataset location is next to /yolov5: # /parent_folder # /VOC # /yolov5 # download command/URL (optional) #download: bash data/scripts/get_voc.sh # train and = data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/] train: ./data/voc/train.txt # 16551 images val: ./data/voc/val.txt # 4952 images # number of clsses nc: 1 # class names names: [ '***'] # your class ---- 模型，在 models 目录下，修改对应想要训练的 yolo 模型 yaml 文件，这里以 yolov5s 为例（nc 为类别总数）： # parameters nc: 1 # number of classes 只需要修改类别总数。 3. 3. 训练目标检测模型 1）在 / Yolov5_DeepSort_Pytorch/yolov5 目录下运行： cd yolov5 python ./test.py --weights ./weights/yolov5s.pt --data ./data/your_data_yaml_file.yaml --save-txt 将预训练的模型，放到下面的目录下。例如：yolov5s.pt。 2）模型测试可以使用下面命令： # -*- coding: UTF-8 -*- ''' @author: gu @contact: 1065504814@qq.com @time: 2021/3/4 下午8:01 @file: crop_image.py @desc: https://blog.csdn.net/qq_36249824/article/details/108428698 ''' import cv2 import xml.etree.ElementTree as ET import numpy as np import xml.dom.minidom import os import argparse def main(): # JPG文件的地址 img_path = 'data/voc/images/' # XML文件的地址 anno_path = 'data/voc/Annotations/' # 存结果的文件夹 cut_path = 'data/voc/crops/' if not os.path.exists(cut_path): os.makedirs(cut_path) # 获取文件夹中的文件 imagelist = os.listdir(img_path) # print(imagelist for image in imagelist: image_pre, ext = os.path.splitext(image) img_file = img_path + image img = cv2.imread(img_file) xml_file = anno_path + image_pre + '.xml' # DOMTree = xml.dom.minidom.parse(xml_file) # collection = DOMTree.documentElement # objects = collection.getElementsByTagName(\"object\") tree = ET.parse(xml_file) root = tree.getroot() # if root.find('object') == None: # return obj_i = 0 for obj in root.iter('object'): obj_i += 1 cls = obj.find('name').text xmlbox = obj.find('bndbox') b = [int(float(xmlbox.find('xmin').text)), int(float(xmlbox.find('ymin').text)), int(float(xmlbox.find('xmax').text)), int(float(xmlbox.find('ymax').text))] img_cut = img[b[1]:b[3], b[0]:b[2], :] path = os.path.join(cut_path, cls) # 目录是否存在,不存在则创建 mkdirlambda = lambda x: os.makedirs(x) if not os.path.exists(x) else True mkdirlambda(path) cv2.imwrite(os.path.join(cut_path, cls, '{}_{:0>2d}.jpg'.format(image_pre, obj_i)), img_cut) print(\"&&&&\") if __name__ == '__main__': main() 3）模型精度验证： deep_sort_pytorch deep_sort deep data train 1_0001.jpg ... 1_nnnn.jpg ... n test 1_0001.jpg ... 1_nnnn.jpg ... n 4）断开后接着训练，使用 --resume 5）想使用 tensorboard，注释 model/yolo.py line 282-286 6）数据增强 ---data/hyp.scratch.yaml 和 data/hyp.finetune.yaml。 其中 5，6 参考：https://blog.csdn.net/weixin_41868104/article/details/114685071 4. 4. 准备分类 / 重识别数据（四处） 实际上要根据你自己的数据，写点处理数据的脚本，我这里就大致给出三个步骤吧。 --- 抠数据 可以将标注 gt 中的数据，抠出来，然后拿来训练模型。 transform_train = torchvision.transforms.Compose([ torchvision.transforms.Resize((128, 64)), torchvision.transforms.RandomCrop((128, 64), padding=4), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) --- 使用预训练模型与标注，这个要自己去拿部分数据训练，然后不断重复（此处是用于帮助下面一步半自动标注用于分类的数据）。 --- 将数据分为 train 与 test，目录中分别以类 1,2,3,4 或者其他的表示，这里是因为默认的读取方法，懒得改了。 目录大概是这样： deep_sort_pytorch deep_sort deep data train 1_0001.jpg ... 1_nnnn.jpg ... n test 1_0001.jpg ... 1_nnnn.jpg ... n --- 修改 train.py 中 train dataset 的预处理如下： transform_train = torchvision.transforms.Compose([ torchvision.transforms.Resize((128, 64)), torchvision.transforms.RandomCrop((128, 64), padding=4), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) 5. 5. 训练分类 / 重识别模型 在 Yolov5_DeepSort_Pytorch/deep_sort_pytorch/deep_sort/deep 目录下运行： python train.py --data-dir data/ 并修改 model.py 文件中的此处为自己的类别数（这一点很重要，很多人出现模型与推理类别不一样的问题，就是这里的设置）： 6. 6. 测试跟踪（视频） 在 / Yolov5_DeepSort_Pytorch 目录下运行： python track.py --weights ./yolov5/weights/yolov5s_our.pt --source your_video.mp4 --save-txt 这里由于数据隐私，就不给图了。 如果有多个类别要跟踪的话，--classes 中设置一下，如果类别数为 2 的话，就加个参数： --classes 0 1 7. 参考 1.yolov5 训练自己的 VOC 数据集 2.【pytorch 学习】 图片数据集的导入和预处理 "},"5 - 实验笔记/YOLO实验/Installing Darknet.html":{"url":"5 - 实验笔记/YOLO实验/Installing Darknet.html","title":"Installing Darknet","keywords":"","body":"1.1. 1. Installing The Base System 本文由 简悦 SimpRead 转码， 原文地址 pjreddie.com Darknet is easy to install with only two optional dependancies: OpenCV if you want a wider variety of supported image types. CUDA if you want GPU computation. Both are optional so lets start by just installing the base system. I've only tested this on Linux and Mac computers. If it doesn't work for you, email me or something? 1.1. 1. Installing The Base System First clone the Darknet git repository here. This can be accomplished by: git clone https://github.com/pjreddie/darknet.git cd darknet make If this works you should see a whole bunch of compiling information fly by: mkdir -p obj gcc -I/usr/local/cuda/include/ -Wall -Wfatal-errors -Ofast.... gcc -I/usr/local/cuda/include/ -Wall -Wfatal-errors -Ofast.... gcc -I/usr/local/cuda/include/ -Wall -Wfatal-errors -Ofast.... ..... gcc -I/usr/local/cuda/include/ -Wall -Wfatal-errors -Ofast -lm.... If you have any errors, try to fix them? If everything seems to have compiled correctly, try running it! ./darknet You should get the output: usage: ./darknet Great! Now check out the cool things you can do with darknet here. Compiling With CUDA Darknet on the CPU is fast but it's like 500 times faster on GPU! You'll have to have an Nvidia GPU and you'll have to install CUDA. I won't go into CUDA installation in detail because it is terrifying. Once you have CUDA installed, change the first line of the Makefile in the base directory to read: GPU=1 Now you can make the project and CUDA will be enabled. 默认它将在您系统中的第0个图形卡上运行网络 By default it will run the network on the 0th graphics card in your system (if you installed CUDA correctly you can list your graphics cards using nvidia-smi). 选择合适的GPU If you want to change what card Darknet uses you can give it the optional command line flag -i , like: ./darknet -i 1 imagenet test cfg/alexnet.cfg alexnet.weights 改用CPU If you compiled using CUDA but want to do CPU computation for whatever reason you can use -nogpu to use the CPU instead: ./darknet -nogpu imagenet test cfg/alexnet.cfg alexnet.weights Compiling With OpenCV support for weird formats view images and detections without having to save them to disk By default, Darknet uses stb_image.h for image loading. If you want more support for weird formats (like CMYK jpegs, thanks Obama) you can use OpenCV instead! OpenCV also allows you to view images and detections without having to save them to disk. First install OpenCV. If you do this from source it will be long and complex so try to get a package manager to do it for you. Next, change the 2nd line of the Makefile to read: OPENCV=1 You're done! To try it out, first re-make the project. Then use the imtest routine to test image loading and displaying: ./darknet imtest data/eagle.jpg If you get a bunch of windows with eagles in them you've succeeded! They may look like: "},"5 - 实验笔记/YOLO实验/YOLO: Real-Time Object Detection.html":{"url":"5 - 实验笔记/YOLO实验/YOLO: Real-Time Object Detection.html","title":"YOLO Real Time Object Detection","keywords":"","body":"1. Yolo - Real Time Object Detection1.1. 1. Comparison to Other Detectors1.2. 2. Performance on the COCO Dataset1.3. 3. How It Works1.3.1. What's New in Version 3?1.4. 4. Detection Using A Pre-Trained Model1.4.1. Multiple Images1.4.2. Changing The Detection Threshold1.4.3. Tiny YOLOv31.5. 5. Real-Time Detection on a Webcam 网络摄像头1.6. 6. Training YOLO on VOC1.6.1. Get The Pascal VOC Data1.6.2. Generate Labels for VOC1.6.3. Modify Cfg for Pascal Data1.6.4. Download Pretrained Convolutional Weights1.6.5. Train The Model1.7. 7. Training YOLO on COCO1.7.1. Get The COCO Data1.7.2. Modify cfg for COCO1.7.3. Train The Model1.8. 8. YOLOv3 on the Open Images dataset1.9. 9. What Happened to the Old YOLO Site?1.10. 10. Cite1. Yolo - Real Time Object Detection 原文地址 pjreddie.com You only look once (YOLO) is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. 1.1. 1. Comparison to Other Detectors YOLOv3 is extremely fast and accurate. In mAP measured at .5 IOU YOLOv3 is on par with Focal Loss but about 4x faster. Moreover, you can easily tradeoff between speed and accuracy simply by changing the size of the model, no retraining required! 1.2. 2. Performance on the COCO Dataset ModelTrainTestmAPFLOPSFPSCfgWeightsSSD300COCO trainvaltest-dev41.2-46linkSSD500COCO trainvaltest-dev46.5-19linkYOLOv2 608x608COCO trainvaltest-dev48.162.94 Bn40cfgweightsTiny YOLOCOCO trainvaltest-dev23.75.41 Bn244cfgweightsSSD321COCO trainvaltest-dev45.4-16linkDSSD321COCO trainvaltest-dev46.1-12linkR-FCNCOCO trainvaltest-dev51.9-12linkSSD513COCO trainvaltest-dev50.4-8linkDSSD513COCO trainvaltest-dev53.3-6linkFPN FRCNCOCO trainvaltest-dev59.1-6linkRetinanet-50-500COCO trainvaltest-dev50.9-14linkRetinanet-101-500COCO trainvaltest-dev53.1-11linkRetinanet-101-800COCO trainvaltest-dev57.5-5linkYOLOv3-320COCO trainvaltest-dev51.538.97 Bn45cfgweightsYOLOv3-416COCO trainvaltest-dev55.365.86 Bn35cfgweightsYOLOv3-608COCO trainvaltest-dev57.9140.69 Bn20cfgweightsYOLOv3-tinyCOCO trainvaltest-dev33.15.56 Bn220cfgweightsYOLOv3-sppCOCO trainvaltest-dev60.6141.45 Bn20cfgweights 1.3. 3. How It Works Prior detection systems repurpose classifiers or localizers to perform detection. They apply the model to an image at multiple locations and scales. High scoring regions of the image are considered detections. We use a totally different approach. We apply a single neural network to the full image. This network divides the image into regions and predicts bounding boxes and probabilities for each region. These bounding boxes are weighted by the predicted probabilities. Our model has several advantages over classifier-based systems. It looks at the whole image at test time so its predictions are informed by global context in the image. It also makes predictions with a single network evaluation unlike systems like R-CNN which require thousands for a single image. This makes it extremely fast, more than 1000x faster than R-CNN and 100x faster than Fast R-CNN. See our paper for more details on the full system. 1.3.1. What's New in Version 3? YOLOv3 uses a few tricks to improve training and increase performance, including: multi-scale predictions, a better backbone classifier, and more. The full details are in our paper! 1.4. 4. Detection Using A Pre-Trained Model This post will guide you through detecting objects with the YOLO system using a pre-trained model. If you don't already have Darknet installed, you should do that first. Or instead of reading all that just run: git clone https://github.com/pjreddie/darknet cd darknet make Easy! You already have the config file for YOLO in the cfg/ subdirectory. You will have to download the pre-trained weight file here (237 MB). Or just run this: wget https://pjreddie.com/media/files/yolov3.weights Then run the detector! ./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg You will see some output like this: layer filters size input output 0 conv 32 3 x 3 / 1 416 x 416 x 3 -> 416 x 416 x 32 0.299 BFLOPs 1 conv 64 3 x 3 / 2 416 x 416 x 32 -> 208 x 208 x 64 1.595 BFLOPs ....... 105 conv 255 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 255 0.353 BFLOPs 106 detection truth_thresh: Using default '1.000000' Loading weights from yolov3.weights...Done! data/dog.jpg: Predicted in 0.029329 seconds. dog: 99% truck: 93% bicycle: 99% Darknet prints out the objects it detected, its confidence, and how long it took to find them. We didn't compile Darknet with OpenCV so it can't display the detections directly. Instead, it saves them in predictions.png. You can open it to see the detected objects. Since we are using Darknet on the CPU it takes around 6-12 seconds per image. If we use the GPU version it would be much faster. I've included some example images to try in case you need inspiration. Try data/eagle.jpg, data/dog.jpg, data/person.jpg, or data/horses.jpg! The detect command is shorthand for a more general version of the command. It is equivalent to the command: ./darknet detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights data/dog.jpg You don't need to know this if all you want to do is run detection on one image but it's useful to know if you want to do other things like run on a webcam (which you will see later on). 1.4.1. Multiple Images Instead of supplying an image on the command line, you can leave it blank to try multiple images in a row. Instead you will see a prompt when the config and weights are done loading: ./darknet detect cfg/yolov3.cfg yolov3.weights layer filters size input output 0 conv 32 3 x 3 / 1 416 x 416 x 3 -> 416 x 416 x 32 0.299 BFLOPs 1 conv 64 3 x 3 / 2 416 x 416 x 32 -> 208 x 208 x 64 1.595 BFLOPs ....... 104 conv 256 3 x 3 / 1 52 x 52 x 128 -> 52 x 52 x 256 1.595 BFLOPs 105 conv 255 1 x 1 / 1 52 x 52 x 256 -> 52 x 52 x 255 0.353 BFLOPs 106 detection Loading weights from yolov3.weights...Done! Enter Image Path: Enter an image path like data/horses.jpg to have it predict boxes for that image. Once it is done it will prompt you for more paths to try different images. Use Ctrl-C to exit the program once you are done. 1.4.2. Changing The Detection Threshold By default, YOLO only displays objects detected with a confidence of .25 or higher. You can change this by passing the -thresh flag to the yolo command. For example, to display all detection you can set the threshold to 0: ./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg -thresh 0 Which produces: ![][all] So that's obviously not super useful but you can set it to different values to control what gets thresholded by the model. 1.4.3. Tiny YOLOv3 We have a very small model as well for constrained environments, yolov3-tiny. To use this model, first download the weights: wget https://pjreddie.com/media/files/yolov3-tiny.weights Then run the detector with the tiny config file and weights: ./darknet detect cfg/yolov3-tiny.cfg yolov3-tiny.weights data/dog.jpg 1.5. 5. Real-Time Detection on a Webcam 网络摄像头 Running YOLO on test data isn't very interesting if you can't see the result. Instead of running it on a bunch of images let's run it on the input from a webcam! To run this demo you will need to compile Darknet with CUDA and OpenCV. Then run the command: ./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights YOLO will display the current FPS and predicted classes as well as the image with bounding boxes drawn on top of it. You will need a webcam connected to the computer that OpenCV can connect to or it won't work. If you have multiple webcams connected and want to select which one to use you can pass the flag -c to pick (OpenCV uses webcam 0 by default). You can also run it on a video file if OpenCV can read the video: ./darknet detector demo cfg/coco.data cfg/yolov3.cfg yolov3.weights That's how we made the YouTube video above. 1.6. 6. Training YOLO on VOC You can train YOLO from scratch if you want to play with different training regimes, hyper-parameters, or datasets. Here's how to get it working on the Pascal VOC dataset. 1.6.1. Get The Pascal VOC Data To train YOLO you will need all of the VOC data from 2007 to 2012. You can find links to the data here. To get all the data, make a directory to store it all and from that directory run: wget https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar wget https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar wget https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar tar xf VOCtrainval_11-May-2012.tar tar xf VOCtrainval_06-Nov-2007.tar tar xf VOCtest_06-Nov-2007.tar There will now be a VOCdevkit/ subdirectory with all the VOC training data in it. 1.6.2. Generate Labels for VOC Now we need to generate the label files that Darknet uses. Darknet wants a .txt file for each image with a line for each ground truth object in the image that looks like: Where x, y, width, and height are relative to the image's width and height. To generate these file we will run the voc_label.py script in Darknet's scripts/ directory. Let's just download it again because we are lazy. wget https://pjreddie.com/media/files/voc_label.py python voc_label.py After a few minutes, this script will generate all of the requisite files. Mostly it generates a lot of label files in VOCdevkit/VOC2007/labels/ and VOCdevkit/VOC2012/labels/. In your directory you should see: ls 2007_test.txt VOCdevkit 2007_train.txt voc_label.py 2007_val.txt VOCtest_06-Nov-2007.tar 2012_train.txt VOCtrainval_06-Nov-2007.tar 2012_val.txt VOCtrainval_11-May-2012.tar The text files like 2007_train.txt list the image files for that year and image set. Darknet needs one text file with all of the images you want to train on. In this example, let's train with everything except the 2007 test set so that we can test our model. Run: cat 2007_train.txt 2007_val.txt 2012_*.txt > train.txt Now we have all the 2007 trainval and the 2012 trainval set in one big list. That's all we have to do for data setup! 1.6.3. Modify Cfg for Pascal Data Now go to your Darknet directory. We have to change the cfg/voc.data config file to point to your data: 1 classes= 20 2 train = /train.txt 3 valid = 2007_test.txt 4 names = data/voc.names 5 backup = backup You should replace with the directory where you put the VOC data. 1.6.4. Download Pretrained Convolutional Weights For training we use convolutional weights that are pre-trained on Imagenet. We use weights from the darknet53 model. You can just download the weights for the convolutional layers here (76 MB). wget https://pjreddie.com/media/files/darknet53.conv.74 1.6.5. Train The Model Now we can train! Run the command: ./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74 1.7. 7. Training YOLO on COCO You can train YOLO from scratch if you want to play with different training regimes, hyper-parameters, or datasets. Here's how to get it working on the COCO dataset. 1.7.1. Get The COCO Data To train YOLO you will need all of the COCO data and labels. The script scripts/get_coco_dataset.sh will do this for you. Figure out where you want to put the COCO data and download it, for example: cp scripts/get_coco_dataset.sh data cd data bash get_coco_dataset.sh Now you should have all the data and the labels generated for Darknet. 1.7.2. Modify cfg for COCO Now go to your Darknet directory. We have to change the cfg/coco.data config file to point to your data: 1 classes= 80 2 train = /trainvalno5k.txt 3 valid = /5k.txt 4 names = data/coco.names 5 backup = backup You should replace with the directory where you put the COCO data. You should also modify your model cfg for training instead of testing. cfg/yolo.cfg should look like this: [net] # Testing # batch=1 # subdivisions=1 # Training batch=64 subdivisions=8 .... 1.7.3. Train The Model Now we can train! Run the command: ./darknet detector train cfg/coco.data cfg/yolov3.cfg darknet53.conv.74 If you want to use multiple gpus run: ./darknet detector train cfg/coco.data cfg/yolov3.cfg darknet53.conv.74 -gpus 0,1,2,3 If you want to stop and restart training from a checkpoint: ./darknet detector train cfg/coco.data cfg/yolov3.cfg backup/yolov3.backup -gpus 0,1,2,3 1.8. 8. YOLOv3 on the Open Images dataset wget https://pjreddie.com/media/files/yolov3-openimages.weights ./darknet detector test cfg/openimages.data cfg/yolov3-openimages.cfg yolov3-openimages.weights 1.9. 9. What Happened to the Old YOLO Site? If you are using YOLO version 2 you can still find the site here: https://pjreddie.com/darknet/yolov2/ 1.10. 10. Cite If you use YOLOv3 in your work please cite our paper! @article{yolov3, title={YOLOv3: An Incremental Improvement}, author={Redmon, Joseph and Farhadi, Ali}, journal = {arXiv}, year={2018} } "},"5 - 实验笔记/YOLO实验/在 Linux 服务器环境下使用 yolov3 训练 voc 数据集.html":{"url":"5 - 实验笔记/YOLO实验/在 Linux 服务器环境下使用 yolov3 训练 voc 数据集.html","title":"在 Linux 服务器环境下使用 yolov3 训练 voc 数据集","keywords":"","body":"1.1.1. 一、通过 Xshell 等工具连接到 Linux 服务器1.1.2. 二、检查服务器中的 cuda 版本1.1.3. 三、下载安装 darknet1.1.4. 四、更改配置文件 Makefile1.1.5. 五、测试一下 yolov3 的效果1.1.6. 六、下载上传 VOC 数据集1.1.7. 七、解包生成相应的文件夹1.1.8. 八、执行 voc_label.py 文件1.1.9. 九、修改配置文件 voc.data1.1.10. 十、修改配置文件 yolov3-voc.cfg1.1.11. 十一、下载预训练权重并进行训练1.1.12. 十二、测试1.1.13. 十三、测试过程指定置信度阈值1.1.14. 十四、最后 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1.1. 一、通过 Xshell 等工具连接到 Linux 服务器 由于我使用的是 window 电脑，所以需要借助第三方工具来连接服务器，使用 mac 系统或者 Linux 系统的话，可以直接在终端使用 ssh 连接到远程服务器。 可以在服务器中新建一个文件夹来放置项目所需的各种文件。 1.1.2. 二、检查服务器中的 cuda 版本 在服务器终端中输入以下命令，查看 cuda 的版本，注意 V 需要大写 tar xvf VOCtest_06-Nov-2007.tar tar xvf VOCtrainval_06-Nov-2007.tar tar xvf VOCtrainval_11-May-2012.tar 可以看到服务器中的 cuda 版本是 10.0 之所以要检查 cuda 的版本，是因为之前在 cuda9.0 版本下，训练数据集后，最后的检测失败了（但也不能排除我操作上的原因），看了许多博客，如果最后的检测不出结果，有可能是 cuda 版本的原因，这一点需要格外注意一下。 下载安装新的 cuda，可以查询相关的博客。 1.1.3. 三、下载安装 darknet 执行命令： git clone https://github.com/pjreddie/darknet.git 执行成功后会在当前目录下自动生成一个 darknet 文件夹 使用命令进入 darknet： cd darknet 文件夹中的文件如下，有些文件是后续生成或者上传进去的，所以你执行后的命令肯定跟下面截图的不一样，后续我会对这些多出来的文件一一讲解 1.1.4. 四、更改配置文件 Makefile 使用 vim 编译器打开 Makefile 文档： vim Makefile 将 GPU 和 CUDNN 后面的数值 0 改成 1，否则默认用 CPU 训练数据的话，速度会无比的缓慢，如果有安装 OpenCV，那么可以在第三行将 OPENCV 的值也置为一，这里我只是检测了图片，因此没有安装 OpenCV。 修改完成之后，在命令行输入 make，否则修改的代码不会生效 make 1.1.5. 五、测试一下 yolov3 的效果 去 yolo 官网下载一个 yolov3.weights 的权重，下载命令如下： wget https://pjreddie.com/media/files/yolov3.weights 下载过程可能会特别缓慢，或者是下载失败，这时候可以去找一些网盘的资源，下载到本地后，输入命令 rz 上传文件到 darknet 目录下，如图所示： 上传完成权重文件之后，就可以测试一下 yolov3 了 确保当前目录在 darknet 下，执行命令： ./darknet detector test cfg/coco.data cfg/yolov3.cfg yolov3.weights data/dog.jpg ./darknet：当前目录 test：表示是检测，不是训练 cfg/coco.data：配置文件，包括物体总的类别数量，训练和测试文件的路径，生成权重保存在什么目录下等配置信息，在使用 yolov3 训练自己的数据集的时候是需要创建一个这样的文件的 cfg/yolov3.cfg：训练和测试过程的配置文件，训练 voc 数据集时，主要是要根据是训练还是测试，打开 / 关闭相应的注释，后面会再提到这个文件。 yolov3.weights：下载的权重文件，相当于就是已经训练的很好的权重信息，测试图片时可以直接拿来用 data/dog.jpg：要测试的图片，在 darknet/data 目录下，有一些其他的图片也可以拿来测试。 执行命令以后，会出现如下的结果： 可以看到图片中的三个物体都被检测出来了 由于 Linux 服务器下不能查看图片，所以可以用 sz 命令，上传到本地来查看图片 上述的 predictions.jpg，就是已经标记好的图片，输入命令： sz predictions.jpg 可以在本地查看到图片如下： 1.1.6. 六、下载上传 VOC 数据集 下载 VOC 数据集，官网如下，如果下载缓慢，也可以去找网盘资源 https://pjreddie.com/projects/pascal-voc-dataset-mirror/ 下载如下三个压缩包到本地，使用 rz 命令上传到服务器中，放到 darknet 目录下： 1.1.7. 七、解包生成相应的文件夹 使用命令进行解包： tar xvf VOCtest_06-Nov-2007.tar tar xvf VOCtrainval_06-Nov-2007.tar tar xvf VOCtrainval_11-May-2012.tar 操作后，会在当前目录生成一个 VOCdevkit 文件夹，里面有 VOC2007，VOC2012 两个子文件夹，拿 VOC2012 文件夹举例，相应的目录结构如下： 这些文件夹解压时会自动生成，但是训练自己的 VOC 格式的数据集时需要手动创建 1.1.8. 八、执行 voc_label.py 文件 将 scripts 文件下的 voc_label.py 拷贝到 darknet 目录下 cp scripts/voc_label.py ./ 在 darknet 目录下，运行该 py 文件 python voc_label.py 运行完成后在 VOCdevkit/VOC2007（VOC2012）就可以看到多了一个 labels 文件 在 darknet 目录下也会生成一些 txt 文件： 1.1.9. 九、修改配置文件 voc.data 修改配置文件 voc.data。该文件位于 darknet/cfg 目录下，修改 train 和 valid 后面的路径，改为自己的路径 说明： classes=20：VOC 数据集一共 20 个类别 train=...：训练文件的路径，train.txt 是运行上面的 py 文件生成的，每行都是训练图片的绝对路径 vaild=...：测试文件的路径，也是运行上面的 py 文件生成的 names=...：20 个类别的名称，voc.name 文件是内置在 darknet 里面的，训练 VOC 数据集时不用手动修改，但是训练自己的数据集时就要手动修改了 backup=...：训练过程中存放权重文件，有过程中的权重文件和最终训练完成的权重的文件 1.1.10. 十、修改配置文件 yolov3-voc.cfg 该文件在 darknet/cfg 目录下，打开 yolov3-voc.cfg 文件，将第 6、7 行 training 下的注释去掉，如果按照默认的 batch=1，subdivisions=1，训练时会出现大量的 nan 把里面的 max_batches 调小，可以显著降低训练时间 1.1.11. 十一、下载预训练权重并进行训练 终端下执行如下命令，下载预训练权重，注意是在 darknet 目录下执行该命令。如果速度慢，就找网盘资源。 wget https://pjreddie.com/media/files/darknet53.conv.74 完成上面的所有工作后，就可以进行训练了，训练的代码： ./darknet detector train cfg/voc.data cfg/yolov3-voc.cfg darknet53.conv.74 train：代表训练 其他的参数在测试 yolov3 效果时说明过，总之就是指定一些配置文件和权重等 训练的时长在 GPU 性能较好的情况下大概需要好几个小时，但是训练一段时间，当损失降的比较低时，可以提前终止训练。 训练结束后，在 backup 文件夹中，可以看到许多过程的权重文件，里面的任何一个都可以用于测试，当然训练的越久，效果也就越好，训练完成后，会有一个后缀为 final 的文件，那个就是最终的权重文件了（我这里偷懒没有训练到最后），注意 yolov3-voc.backup 也是可以用的，这个文件保存的是最新的权重信息。 1.1.12. 十二、测试 我测试的是单张图片，我是将图片上传到 data 文件夹下，下面的 XXX 是图片的名称，当然 data 文件夹里面有些本来就有的图片，那个也可以用来测试 ./darknet detector test cfg/voc.data cfg/yolov3-voc.cfg backup/yolov3-voc_600.weights data/XXX.jpg 测试成功的话，会出现图片中某个物体的概率，（默认的阈值好像是 50%），结果图片（也就是物体框起来的图片）就是 predictions.jpg，上传到自己电脑就可以查看了。 测试时应该要修改配置文件 yolov3-voc.cfg（打开，关闭注释即可），但是经过测试，发现用训练的 batch 和 subdivisions 也是可以测试成功的 1.1.13. 十三、测试过程指定置信度阈值 测试过程中可以手动指定置信度阈值，在测试代码后加上，数值可以随意指定 -thresh 0.25 最后这个 0.25 就是置信度，代表相似度为 0.25 及以上的目标都会被标出 1.1.14. 十四、最后 如果需要测试多张图片，可以参考： YOLOv3 批量测试图片并保存在自定义文件夹下 如果想要了解一些参数的效果，可以参考： Yolov3 参数理解 参考博客： 【学习笔记—Yolov3】Yolov3 训练 VOC 数据集 & 训练自己的数据集 Yolov3 参数理解 YOLOV3 训练自己的数据集（VOC 数据集格式） YOLO V3 置信度阈值调整 YOLOv3 批量测试图片并保存在自定义文件夹下 YoLov3 训练自己的数据集（小白手册） "},"5 - 实验笔记/YOLO实验/快速开始ScaledYOLOv4-知乎.html":{"url":"5 - 实验笔记/YOLO实验/快速开始ScaledYOLOv4-知乎.html","title":"快速开始ScaledYOLOv4-知乎","keywords":"","body":"1.1. Scaled-YOLOv41.2. 环境准备1.2.1. 基础环境1.2.2. 开发环境1.2.3. 脚本依赖1.3. 模型准备1.4. 现有模型测试1.4.1. 准备 COCO 数据集1.4.2. 测试 YOLOv4-P51.4.3. 测试 YOLOv4-P71.5. 自定义数据集训练1.5.1. 准备数据集1.5.2. 准备参数文件1.5.3. 训练模型1.5.4. 错误 RuntimeError: main thread is not in main loop1.5.5. 训练指标1.5.6. 测试模型 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 代码： https://github.com/ikuokuo/start-scaled-yolov4 1.1. Scaled-YOLOv4 代码: https://github.com/WongKinYiu/ScaledYOLOv4 论文: https://arxiv.org/abs/2011.08036 文章: https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982 1.2. 环境准备 1.2.1. 基础环境 Nvidia 显卡的主机 Ubuntu 18.04 系统安装，可见 制作 USB 启动盘，及系统安装 Nvidia Driver 驱动安装，可见 Ubuntu 初始配置 - Nvidia 驱动 1.2.2. 开发环境 下载并安装 Anaconda 之后于 Terminal ,执行虚拟环境： # 创建 Python 虚拟环境 conda create -n scaled-yolov4 python=3.8 -y conda activate scaled-yolov4 # 安装 PyTorch with CUDA conda install pytorch==1.7.1 torchvision==0.8.2 cudatoolkit=10.2 -c pytorch -y pytorch 等版本请对照表 torch, torchvision, python cudatoolkit 版本请对照表 CUDA Toolkit and Compatible Driver Versions conda 下载过慢，可尝试配置国内镜像源 下载 CUDA Toolkit ，其版本也注意对应 Nvidia 驱动版本。下一步需要。命令参考： wget https://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run sudo sh cuda_10.2.89_440.33.01_linux.run 注意：安装时，请手动取消驱动安装选项。 下载 mish-cuda 并安装： # install mish-cuda, if you use different pytorch version, you could try https://github.com/thomasbrandon/mish-cuda git clone https://github.com/JunnYu/mish-cuda cd mish-cuda python setup.py build install 下载 ScaledYOLOv4-large: git clone -b yolov4-large https://github.com/WongKinYiu/ScaledYOLOv4 1.2.3. 脚本依赖 conda activate scaled-yolov4 cd start-scaled-yolov4/ pip install -r scripts/requirements.txt 1.3. 模型准备 下载官方的 yolov4-p5.pt, yolov4-p6.pt, yolov4-p7.pt 权重文件到 ScaledYOLOv4/weights/ 目录。 1.4. 现有模型测试 1.4.1. 准备 COCO 数据集 下载 COCO 数据集， coco2017 ├── annotations │ ├── instances_train2017.json │ └── instances_val2017.json ├── test2017 ├── train2017 └── val2017 转成 YOLOv5 数据集结构， export COCO_DIR=~/datasets/coco2017 export OUTPUT_DIR=~/datasets/coco2017_yolov5 # train2017 训练集 # - 图片：目录软链到 images/ # - 标注：转换存储进 labels/*.txt # - 物体类型：全部记录进 *.names # - 图片列表：有物体标注的记录进 *.txt, 无的进 *.txt.ignored python scripts/coco2yolov5.py \\ --coco_img_dir $COCO_DIR/train2017/ \\ --coco_ann_file $COCO_DIR/annotations/instances_train2017.json \\ --output_dir $OUTPUT_DIR # val2017 验证集 # - 物体类型：依照训练集的记录，保证顺序 python scripts/coco2yolov5.py \\ --coco_img_dir $COCO_DIR/val2017/ \\ --coco_ann_file $COCO_DIR/annotations/instances_val2017.json \\ --output_dir $OUTPUT_DIR \\ --obj_names_file $OUTPUT_DIR/train2017.names 如下： coco2017_yolov5/ ├── images │ ├── train2017 -> /home/john/datasets/coco2017/train2017 │ └── val2017 -> /home/john/datasets/coco2017/val2017 ├── labels │ ├── train2017 │ └── val2017 ├── train2017.names ├── train2017.txt ├── train2017.txt.ignored ├── val2017.txt └── val2017.txt.ignored coco2017_yolov5 软链到 ScaledYOLOv4/ 目录，并添加 ScaledYOLOv4/data/coco2017_yolov5.yaml 文件，描述数据集： # train and val datasets (image directory or *.txt file with image paths) train: ./coco2017_yolov5/images/train2017 val: ./coco2017_yolov5/images/val2017 test: ./coco2017_yolov5/images/val2017 # number of classes nc: 80 # class names names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] 1.4.2. 测试 YOLOv4-P5 cd ScaledYOLOv4 conda activate scaled-yolov4 pip install opencv-python pyyaml scipy tqdm python test.py \\ --img 896 \\ --conf 0.001 \\ --batch 8 \\ --device 0 \\ --data data/coco2017_yolov5.yaml \\ --weights weights/yolov4-p5.pt 结果如下： Fusing layers... Model Summary: 331 layers, 7.07943e+07 parameters, 6.81919e+07 gradients Scanning labels coco2017_yolov5/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 100%|█| 5000/5000 [00:00进行推断， python detect.py \\ --img 896 \\ --conf 0.5 \\ --device 0 \\ --weights weights/yolov4-p5.pt \\ --source demo.jpg 结果如下， Fusing layers... Model Summary: 331 layers, 7.07943e+07 parameters, 6.81919e+07 gradients image 1/1 /home/john/Codes/ScaledYOLOv4/demo.jpg: 768x896 1 cats, 1 dogs, Done. (0.029s) Results saved to inference/output Done. (0.133s) 1.4.3. 测试 YOLOv4-P7 python test.py \\ --img 1536 \\ --conf 0.001 \\ --batch 6 \\ --device 0 \\ --data data/coco2017_yolov5.yaml \\ --weights weights/yolov4-p7.pt 结果如下： Fusing layers... Model Summary: 503 layers, 2.87475e+08 parameters, 2.7862e+08 gradients Scanning labels coco2017_yolov5/labels/val2017.cache (4952 found, 0 missing, 48 empty, 0 duplicate, for 5000 images): 100%|█| 5000/5000 [00:00进行推断， python detect.py \\ --img 1536 \\ --conf 0.5 \\ --device 0 \\ --weights weights/yolov4-p7.pt \\ --source demo.jpg 结果如下， Fusing layers... Model Summary: 503 layers, 2.87475e+08 parameters, 2.7862e+08 gradients image 1/1 /home/john/Codes/ScaledYOLOv4/demo.jpg: 1152x1536 1 cats, 1 dogs, 1 chairs, 1 couchs, 1 potted plants, Done. (0.079s) Results saved to inference/output Done. (0.282s) 1.5. 自定义数据集训练 1.5.1. 准备数据集 这里从 COCO 数据集拿出一个子集，作为自定义数据集的演示： cat subset.names cat dog EOF export COCO_DIR=~/datasets/coco2017 export OUTPUT_DIR=~/datasets/coco2017_yolov5_subset python scripts/coco2yolov5.py \\ --coco_img_dir $COCO_DIR/train2017/ \\ --coco_ann_file $COCO_DIR/annotations/instances_train2017.json \\ --output_dir $OUTPUT_DIR \\ --obj_names_file subset.names python scripts/coco2yolov5.py \\ --coco_img_dir $COCO_DIR/val2017/ \\ --coco_ann_file $COCO_DIR/annotations/instances_val2017.json \\ --output_dir $OUTPUT_DIR \\ --obj_names_file subset.names coco2017_yolov5_subset 软链到 ScaledYOLOv4/ 目录，并添加 ScaledYOLOv4/data/coco2017_yolov5_subset.yaml 文件，描述数据集： # train and val datasets (image directory or *.txt file with image paths) train: ./coco2017_yolov5_subset/train2017.txt val: ./coco2017_yolov5_subset/val2017.txt test: ./coco2017_yolov5_subset/val2017.txt # number of classes nc: 2 # class names names: ['cat', 'dog'] 1.5.2. 准备参数文件 这里以 YOLOv4-P6 为例，P5, P7 一样。 复制 ScaledYOLOv4/models/yolov4-p6.yaml 到 ScaledYOLOv4/models/coco2017_yolov5_subset/yolov4-p6.yaml 文件，修改 nc 参数： nc: 2 # number of classes 1.5.3. 训练模型 conda activate scaled-yolov4 pip install tensorboard python train.py -h 参数， optional arguments: -h, --help show this help message and exit --weights WEIGHTS initial weights path --cfg CFG model.yaml path --data DATA data.yaml path --hyp HYP hyperparameters path, i.e. data/hyp.scratch.yaml --epochs EPOCHS --batch-size BATCH_SIZE total batch size for all GPUs --img-size IMG_SIZE [IMG_SIZE ...] train,test sizes --rect rectangular training --resume [RESUME] resume from given path/last.pt, or most recent run if blank --nosave only save final checkpoint --notest only test final epoch --noautoanchor disable autoanchor check --evolve evolve hyperparameters --bucket BUCKET gsutil bucket --cache-images cache images for faster training --name NAME renames results.txt to results_name.txt if supplied --device DEVICE cuda device, i.e. 0 or 0,1,2,3 or cpu --multi-scale vary img-size +/- 50% --single-cls train as single-class dataset --adam use torch.optim.Adam() optimizer --sync-bn use SyncBatchNorm, only available in DDP mode --local_rank LOCAL_RANK DDP parameter, do not modify --logdir LOGDIR logging directory 训练， python train.py \\ --batch-size 2 \\ --img 1280 1280 \\ --data data/coco2017_yolov5_subset.yaml \\ --cfg models/coco2017_yolov5_subset/yolov4-p6.yaml \\ --weights '' \\ --sync-bn \\ --device 0,1 \\ --name yolov4-p6 \\ --epochs 100 信息如下： 如要恢复训练： python train.py \\ --batch-size 2 \\ --img 1280 1280 \\ --data data/coco2017_yolov5_subset.yaml \\ --cfg models/coco2017_yolov5_subset/yolov4-p6.yaml \\ --weights 'runs/exp0_yolov4-p6/weights/last.pt' \\ --sync-bn \\ --device 0,1 \\ --name yolov4-p6 \\ --resume 1.5.4. 错误 RuntimeError: main thread is not in main loop Exception ignored in: Traceback (most recent call last): File \"/home/john/anaconda3/envs/scaled-yolov4/lib/python3.8/tkinter/__init__.py\", line 4014, in __del__ self.tk.call('image', 'delete', self.name) RuntimeError: main thread is not in main loop Tcl_AsyncDelete: async handler deleted by the wrong thread Aborted (core dumped) 如果发生此错误，可于 train.py __main__ 修改 GUI 的 backend： if __name__ == '__main__': import matplotlib.pyplot as plt plt.switch_backend(\"agg\") 1.5.5. 训练指标 训练完成后，内容如下： runs/exp0_yolov4-p6/ ├── events.out.tfevents.1610070159.john-ubuntu18.17638.0 ├── hyp.yaml ├── labels.png ├── opt.yaml ├── results.png ├── results.txt ├── test_batch0_gt.jpg ├── test_batch0_pred.jpg ├── train_batch0.jpg ├── train_batch1.jpg ├── train_batch2.jpg └── weights ├── best_yolov4-p6.pt ├── best_yolov4-p6_strip.pt ├── last_000.pt ├── last_yolov4-p6.pt └── last_yolov4-p6_strip.pt labels.png: 标注分布图 results.png: 训练过程图 results.txt: 训练过程日志 results.png 要训练完成后才有，如果训练过程中要查看，可用 tensorboard： $ tensorboard --logdir runs TensorFlow installation not found - running with reduced feature set. Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit) 打开 http://localhost:6006/ 可见： 1.5.6. 测试模型 python test.py \\ --img 1280 \\ --conf 0.001 \\ --batch 8 \\ --device 0 \\ --data data/coco2017_yolov5_subset.yaml \\ --weights runs/exp0_yolov4-p6/weights/best_yolov4-p6_strip.pt 进行推断， python detect.py \\ --img 1280 \\ --conf 0.5 \\ --device 0 \\ --weights runs/exp0_yolov4-p6/weights/best_yolov4-p6_strip.pt --source demo.jpg GoCoding 个人实践的经验分享，可关注公众号！ "},"5 - 实验笔记/YOLO实验/笔记 - 在服务器上用 YOLOv4 训练和测试数据集.html":{"url":"5 - 实验笔记/YOLO实验/笔记 - 在服务器上用 YOLOv4 训练和测试数据集.html","title":"笔记 - 在服务器上用 YOLOv4 训练和测试数据集","keywords":"","body":"1.1.1. 目录2. 编译 darknet3. 训练 PASCAL VOC2007 数据集3.1. 准备预训练模型和数据集3.2. 生成 darknet 需要的 label 文件3.3. 修改几个配置文件3.4. 训练3.5. 段错误3.6. 测试4. 训练自己的数据集4.1. 训练完 map 为 0 的问题5. 批量测试图片并保存6. 将 weights 权值文件转换为 tflite 权值文件7. 附加内容7.1. 显示检测框的置信度7.2. 改变检测框的粗细7.3. 保存检测框的内容到本地 (批量图片检测)7.4. 保存检测框的内容到本地 (视频检测) 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1.1. 目录 编译 darknet 训练 PASCAL VOC2007 数据集 准备预训练模型和数据集 生成 darknet 需要的 label 文件 修改几个配置文件 训练 段错误 测试 训练自己的数据集 训练完 map 为 0 的问题 批量测试图片并保存 将 weights 权值文件转换为 tflite 权值文件 附加内容 显示检测框的置信度 改变检测框的粗细 保存检测框的内容到本地 (批量图片检测) 保存检测框的内容到本地 (视频检测) 如果希望先训练 PASCAL VOC 数据集，可以按顺序阅读，如果想直接训练自己的数据集，可以先看编译darknet部分， 然后直接跳到训练自己的数据集部分。 yolov4 出来有一段时间了，我也用 yolov4 训练了自己的数据集，效果还是非常不错的。本文记录了用 yolov4 训练并测试 PASCAL VOC 和自己制作的数据集的方法，并且记录了如何将 weights 文件转换为移植到安卓端需要用到的 tflite 文件。yolov4 的 paper: YOLOv4yolov4 的代码地址: darknet 参考博文：基于 Darknet 深度学习框架训练 YoloV4 模型，并用自己的模型批量处理图片并保存在文件夹内系统环境：GeForce GTX 1080 TiCUDA 9.1Cudnn 7.1.3Python 3.6.7因为服务器和本机的图形界面交互没做好，所以没有用 OpenCV。（2020-7-9 更正：服务器和本机的图形交互用 xmanger 就好啦，然后编译时就可以令 OPENCV=1） 2. 编译 darknet 打开 darknet-master/Makefile，如果没有下载 darknet，命令行输入： git clone https://github.com/AlexeyAB/darknet.git 在 darknet 下找到 Makefile 文件，并修改 Makefile： cd ./darknet ls vi Makefile Q:Linux中如何修改文件内容并保存? 如果想修改这个文件的内容，按下间键盘上的'i'键，最下方就会变成INSERT，就可以修改了 写个echo的代码进行保存 Esc+:+wq 这个是保存，回车保存，保存完成以后，再次打开这个文件会发现刚才写入的代码已经保存到里面了。 不保存的命令Esc+:+q!这个是不保存 GPU=1 CUDNN=1 CUDNN_HALF=1 OPENCV=0 AVX=0 OPENMP=0 LIBSO=0 然后命令行输入： cd darknet make 编译完成后输入./darknet， 如果出现usage:./darknet说明编译成功。可以用作者训练好的权值文件 yolov4.weights 测试一下 (下载速度太慢可私信邮箱)，下载后放在 darknet 目录下，在命令行输入： wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights ./darknet detect cfg/yolov4.cfg yolov4.weights data/horses.jpg 因为没有用 OpenCV，所以检测结果没有直接显示，而是保存为 darknet/predictons.jpg，看一下检测效果： 3. 训练 PASCAL VOC2007 数据集 作者在 github 上给的是训练 PASCAL VOC2007+2012 案例，因为我只下载了 VOC2007 的数据集，所以介绍只训练 VOC2007 的方法，其实方法都差不多。 3.1. 准备预训练模型和数据集 首先要下载预训练模型 yolov4.conv.137(下载速度太慢可私信邮箱)，并放在 darknet 目录下，将 VOC2007 数据集放在 darknet/data/voc/VOCdevkit 目录下。 3.2. 生成 darknet 需要的 label 文件 将 darknet/scripts 目录下的 voc_label.py 文件复制到 darknet/data/voc 目录下：cp scripts/voc_label.py data/voc/，并修改voc_label.py文件。将开头改为： # sets=[('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val'), ('2007', 'test')] sets = [('2007', 'train'),('2007', 'val'),('2007', 'test')] 最后两行改为： # os.system(\"cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt > train.txt\") # os.system(\"cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt > train.all.txt\") os.system(\"cat 2007_train.txt 2007_val.txt > train.txt\") os.system(\"cat 2007_train.txt 2007_val.txt 2007_test.txt > train.all.txt\") 修改完成后运行： cd data/voc python voc_label.py 运行结果产生五个文件 2007_train.txt, 2007_test.txt, 2007_val.txt, train.all.txt, train.txt。 3.3. 修改几个配置文件 1.cfg/voc.data classes= 20 train = /home/zmh/darknet/data/voc/train.txt # 修改为训练集路径 valid = /home/zmh/darknet/data/voc/2007_test.txt # 修改为测试集路径 names = data/voc.names # 类别名称文件 backup = backup # 模型保存文件夹 2.cfg/yolo-obj.cfg将 cfg/yolov4-custom.cfg 复制，并将复制后的文件命名为 yolo-obj.cfg： cd cfg cp yolov4-custom.cfg yolo-obj.cfg 修改 yolo-obj.cfg:第 1 处 [net] #Testing #batch=1 #subdivisions=1 #Training batch=64 subdivisions=16 # 如果训练时出现out of memory，可以修改为32或者64 width=608 # 修改为416 height=608 # 修改为416 channels=3 第 2 处 learning_rate=0.001 burn_in=1000 max_batches = 500500 # 修改为(类别数*2000) policy=steps steps=400000,450000 # 修改为0.8*max_batches，0.9*max_batches scales=.1,.1 第 3、4、5 处ctrl+f 查找yolo，一共有三处，修改三处 yolo 上面的 filters，以及 yolo 下面的 classes： [convolutional] size=1 stride=1 pad=1 filters=255 # 修改为(类别数+5)*3 activation=linear [yolo] mask = 0,1,2 anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401 classes=80 # 修改为类别数，20 num=9 3.4. 训练 ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov4.conv.137 训练得到的权值文件保存在 darknet/backup 中。训练是默认用第 0 块显卡，如果要指定第 x 块显卡，可以： ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov4.conv.137 -i x 如果要使用多 GPU 训练，可以先使用一个 GPU 训练得到 1000 次的权值文件 yolo-obj_1000.weights，然后停止训练，接着使用多 GPU 训练（最多四块）： ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov-obj_1000.weights -gpus 0,1,2,3 如果要保存 log 文件： # tee后面是log文件名字，自己设置 ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov4.conv.137 | tee yolo_train.log 在使用 OPENCV 的情况下，如果要画 mAp 曲线： ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov4.conv.137 -map 3.5. 段错误 如果训练时出现段错误：mosaic=1 -compile Darknet with OpenCV for using mosaic=1free(): corrupted unsorted chunks作者有给解决方法：因为我没有用 OpenCV，所以要将 cfg/yolo-obj.cfg 中的mosaic设置为 0。 3.6. 测试 测试图片： ./darknet detector test cfg/voc.data cfg/yolo-obj.cfg backup/yolo-obj_8000.weights data/person.jpg 测试视频（需要 OpenCV）： ./darknet detector demo cfg/voc.data cfg/yolo-obj.cfg backup/yolo-obj_8000.weights test.mp4 -out_filename output.mp4 FPS 很小的话，加一个 - dont_show 或者最小化 xmanger 就行了。训练了 8000 次的检测效果： 4. 训练自己的数据集 完成了 VOC2007 的训练和测试过程，离训练自己的数据集只有一步之遥了，为了方便处理，我把自己的数据集整理成 PASCAL VOC 的格式，具体过程见轻松自制 PASCAL VOC 数据集。假设自己的数据集已经被制作成了 PASCAL VOC 格式了，并且命名为 VOC 2007。如果还没有下载预训练模型 yolov4.conv.137(下载速度太慢可私信邮箱)，需要先下载，下载完成放在 darknet 目录下，并将自己的数据集 (VOC 2007) 放在 darknet/data/voc/VOCdevkit 目录下。接下来进行以下几个跟训练 PASCAL VOC 数据集类似的操作（我的数据集只有两个类别：green light 和 red light）：1. 生成 darknet 需要的 label 文件如果之前已经修改过 voc_label.py 文件了，重新python voc_label.py就行了，然后会生成五个新的文件：2007_train.txt, 2007_test.txt, 2007_val.txt, train.all.txt, train.txt。如果之前没有训练过 PASCAL VOC 数据集，将 darknet/scripts 目录下的 voc_label.py 文件复制到 darknet/data/voc 目录下：cp scripts/voc_label.py data/voc/，并修改voc_label.py文件。将开头改为： # sets=[('2012', 'train'), ('2012', 'val'), ('2007', 'train'), ('2007', 'val'), ('2007', 'test')] sets = [('2007', 'train'),('2007', 'val'),('2007', 'test')] classes = [\"green light\", \"red light\"] # 修改为自己的类别 最后两行改为： # os.system(\"cat 2007_train.txt 2007_val.txt 2012_train.txt 2012_val.txt > train.txt\") # os.system(\"cat 2007_train.txt 2007_val.txt 2007_test.txt 2012_train.txt 2012_val.txt > train.all.txt\") os.system(\"cat 2007_train.txt 2007_val.txt > train.txt\") os.system(\"cat 2007_train.txt 2007_val.txt 2007_test.txt > train.all.txt\") 修改完成后运行： cd data/voc python voc_label.py 同样的，会生成五个新的文件：2007_train.txt, 2007_test.txt, 2007_val.txt, train.all.txt, train.txt。2. 修改 voc.data 文件如果训练过 PASCAL VOC，只要修改其中的 classes。否则根据注释修改： classes= 2 # 改为自己数据集的类别数 train = /home/zmh/darknet/data/voc/train.txt # 修改为训练集路径 valid = /home/zmh/darknet/data/voc/2007_test.txt # 修改为测试集路径 names = data/voc.names # 类别名称文件，下面会讲怎么修改里面的内容 backup = backup # 模型保存文件夹 3. 修改 cfg/yolo-obj.cfg 文件如果没有训练过 PASCAL VOC，需要将 cfg/yolov4-custom.cfg 复制，并将复制后的文件命名为 yolo-obj.cfg： cd cfg cp yolov4-custom.cfg yolo-obj.cfg 然后修改 yolo-obj.cfg:第 1 处，如果训练过 PASCAL VOC，可不用修改： [net] #Testing #batch=1 #subdivisions=1 #Training batch=64 subdivisions=16 # 如果训练时出现out of memory，可以修改为32或者64 width=608 # 修改为416 height=608 # 修改为416 channels=3 下面几处即使训练过 PASCAL VOC，同样要对应修改，第 2 处： learning_rate=0.001 burn_in=1000 max_batches = 4000 # 修改为(类别数*2000) policy=steps steps=3200,3600 # 修改为0.8*max_batches，0.9*max_batches scales=.1,.1 以及第 3-5 处：ctrl+f 查找yolo，修改三处 yolo 上面的 filters，以及 yolo 下面的 classes： [convolutional] size=1 stride=1 pad=1 filters=21 # 修改为(类别数+5)*3 activation=linear [yolo] mask = 0,1,2 anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401 classes=2 # 修改为类别数，20 num=9 4. 修改 data/voc.names 文件（注意，之前训练PASCAL数据集是没有这个步骤的）voc.names 文件中保存的是 VOC 数据集的类别（20 个），我们要修改为自己数据集的类别。 aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor 改为： green light red light 5. 训练训练之前，转移或删除 darknet/backup 中之前得到的权值文件。 ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov4.conv.137 如果要打印 loss 曲线，同时打印 map 值，在后面加 - map： ./darknet detector train cfg/voc.data cfg/yolo-obj.cfg yolov4.conv.137 -map 训练完得到四个权值文件（已重命名）：指定 GPU 或者多 GPU 训练、打印训练 log、段错误的解决方法等，可以看训练 PASCAL VOC 数据集的训练部分。 4.1. 训练完 map 为 0 的问题 如果训练完发现 map 为 0，有可能是：1. 没有改 voc_label.py 中的类名。2. 没有使用正确的预训练模型 (yolov4.conv.137)。3. 数据集有问题，或者没有成功转为 yolo 格式。如果要计算每个权值文件的 mAP 值，可以： ./darknet detector map cfg/voc.data cfg/yolo-obj.cfg backup/yolo-tflight_1000.weights 然后可以选一个 mAP 值最高的权值文件测试。6. 测试测试单张图片： ./darknet detector test cfg/voc.data cfg/yolo-obj.cfg backup/yolo-tflight_4000.weights 测试视频 (需要 OpenCV)： ./darknet detector demo cfg/voc.data cfg/yolo-obj.cfg backup/yolo-tflight_4000.weights -ext_output test.mp4 看一下效果： 5. 批量测试图片并保存 如果要测试大量图片，一张一张测试是非常麻烦的，为了能够批量测试图片，并保存在指定文件夹中，需要以下操作：1. 修改 darknet/src/detector.c 文件在 detector.c 文件的开头位置，添加GetFilename函数，为了解决上个版本文件名长度的问题，这里做了修改，直接将上个版本的常数用变量 length 替代，length 是测试图片的文件名，因为要去掉 \".jpg\"，所以 strncpy 那里要用 \"length-4\"。代码亲测有效。 static int coco_ids[] = {1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,27,28,31,32,33,34,35,36,37,38,39,40,41,42,43,44,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,67,70,72,73,74,75,76,77,78,79,80,81,82,84,85,86,87,88,89,90}; // 在下面添加GetFilename函数 char *GetFilename(char *p) { static char name[64]; // 作为返回值不能是局部变量 memset(name, 0, sizeof(name)); // 清空之前的静态变量 int length = 0; char *q = strrchr(p,'/') + 1; // 获取第一个'/'后面的位置 length = strlen(q); strncpy(name,q,length-4); // 赋值并且不要后缀 return name; } ctrl+f 查找save_image，查找下一个，然后找到 \"save_image(im, \"predictions\")\"，替换源代码： // save_image(im, \"predictions\"); char b[64]; snprintf(b, 64, \"output/%s\", GetFilename(input)); save_image(im, b); 然后在 darknet 目录下重新make。2. 准备包含所有需要检测图片路径的 txt 文件将需要检测的图片放在 darknet/data/test 文件夹下，然后在 darknet/data 下新建一个 py 文件： touch make_test_txt.py 文件中的内容为： # coding: utf-8 import os paths = \"./test\" # 测试图片的路径 f = open('test.txt', 'w') # 最后得到的图片路径txt文件 filenames = os.listdir(paths) filenames.sort() for filename in filenames: out_path = \"data/test/\" + filename # 引号内为测试图片文件夹的路径 print(out_path) f.write(out_path + '\\n') f.close() 然后运行make_test_txt.py，在 data 目录下会生成一个test.txt文件。3. 批量测试在 darknet 目录下新建一个文件夹output，运行： ./darknet detector test cfg/voc.data cfg/yolo-obj.cfg backup/yolo-tflight_4000.weights -ext_output -dont_show result.txt 注意，保存测试图片文件名的test.txt文件两边有，result.txt 是把检测的文本输出打印出来，可有可无，检测的图片输出保存在output文件夹中。 6. 将 weights 权值文件转换为 tflite 权值文件 将模型移植到安卓端需要将 weights 文件转换为 tflite 文件，系统环境：Tensorflow 2.1.0tensorflow_addons 0.9.1运行时可能会提升缺少其他安装包，根据提示使用 pip 安装即可。首先克隆代码到本地： git clone https://github.com/hunglc007/tensorflow-yolov4-tflite.git 然后只需要修改tensorflow-yolov4-tflite-master/data/classes/coco.names，将内容修改为自己的类别名： green light red light 或者在 data/classes 中新建一个. names 文件，比如 “tfclight.names”，内容为自己的类名。然后打开 core/config.py，将 __C.YOLO.CLASSES = \"./data/classes/coco.names\" 改为 __C.YOLO.CLASSES = \"./data/classes/tfclight.names\" 接着将要转换的 weight 文件放在 data 目录下。最后在 tensorflow-yolov4-tflite-master 目录下，选择以下其中一条命令运行： # yolov4非量化转换 python convert_tflite.py --weights ./data/yolo-tflight_4000.weights --output ./data/yolo-tflight_4000.tflite # yolov4 quantize float16（量化转换） python convert_tflite.py --weights ./data/yolo-tflight_4000.weights --output ./data/yolo-tflight_4000-fp16.tflite --quantize_mode float16 得到的结果：参考：tensorflow-yolov4-tflite 7. 附加内容 附加的内容由于篇幅原因，请移步到另一篇博客：https://blog.csdn.net/Creama_/article/details/107960513 7.1. 显示检测框的置信度 7.2. 改变检测框的粗细 7.3. 保存检测框的内容到本地 (批量图片检测) 7.4. 保存检测框的内容到本地 (视频检测) "},"5 - 实验笔记/🌟Pytorch 搭建 YoloV4 目标检测平台.html":{"url":"5 - 实验笔记/🌟Pytorch 搭建 YoloV4 目标检测平台.html","title":"🌟Pytorch 搭建 YoloV4 目标检测平台","keywords":"","body":"1.1.1. 睿智的目标检测 30——Pytorch 搭建 YoloV4 目标检测平台2. 学习前言3. 什么是 YOLOV44. 代码下载5. YOLOV4 改进的部分（不完全）6. YOLOV4 结构解析6.1. 1、主干特征提取网络 Backbone6.2. 2、特征金字塔6.3. 3、YoloHead 利用获得到的特征进行预测6.4. 4、预测结果的解码6.5. 5、在原图上进行绘制7. YOLOV4 的训练7.1. 1、YOLOV4 的改进训练技巧7.1.1. a)、Mosaic 数据增强7.1.2. b)、Label Smoothing 平滑7.1.3. c)、CIOU7.1.4. d)、学习率余弦退火衰减7.2. 2、loss 组成7.2.1. a)、计算 loss 所需参数7.2.2. b)、y_pre 是什么7.2.3. c)、y_true 是什么。7.2.4. d)、loss 的计算过程8. 训练自己的 YOLOV4 模型 本文由 简悦 SimpRead 转码， 原文地址 blog.csdn.net 1.1.1. 睿智的目标检测 30——Pytorch 搭建 YoloV4 目标检测平台 学习前言 什么是 YOLOV4 代码下载 YOLOV4 改进的部分（不完全） YOLOV4 结构解析 1、主干特征提取网络 Backbone 2、特征金字塔 3、YoloHead 利用获得到的特征进行预测 4、预测结果的解码 5、在原图上进行绘制 YOLOV4 的训练 1、YOLOV4 的改进训练技巧 a)、Mosaic 数据增强 b)、Label Smoothing 平滑 c)、CIOU d)、学习率余弦退火衰减 2、loss 组成 a)、计算 loss 所需参数 b)、y_pre 是什么 c)、y_true 是什么。 d)、loss 的计算过程 训练自己的 YOLOV4 模型 2. 学习前言 也做了一下 pytorch 版本的。 3. 什么是 YOLOV4 YOLOV4 是 YOLOV3 的改进版，在 YOLOV3 的基础上结合了非常多的小 Tricks。尽管没有目标检测上革命性的改变，但是 YOLOV4 依然很好的结合了速度与精度。根据上图也可以看出来，YOLOV4 在 YOLOV3 的基础上，在 FPS 不下降的情况下，mAP 达到了 44，提高非常明显。 YOLOV4 整体上的检测思路和 YOLOV3 相比相差并不大，都是使用三个特征层进行分类与回归预测。 请注意！ 强烈建议在学习 YOLOV4 之前学习 YOLOV3，因为 YOLOV4 确实可以看作是 YOLOV3 结合一系列改进的版本！ 强烈建议在学习 YOLOV4 之前学习 YOLOV3，因为 YOLOV4 确实可以看作是 YOLOV3 结合一系列改进的版本！ 强烈建议在学习 YOLOV4 之前学习 YOLOV3，因为 YOLOV4 确实可以看作是 YOLOV3 结合一系列改进的版本！ （重要的事情说三遍！） YOLOV3 可参考该博客：https://blog.csdn.net/weixin_44791964/article/details/105310627 4. 代码下载 https://github.com/bubbliiiing/yolov4-pytorch喜欢的可以给个 star 噢！ 5. YOLOV4 改进的部分（不完全） 1、主干特征提取网络：DarkNet53 => CSPDarkNet53 2、特征金字塔：SPP，PAN 3、分类回归层：YOLOv3（未改变） 4、训练用到的小技巧：Mosaic 数据增强、Label Smoothing 平滑、CIOU、学习率余弦退火衰减 5、激活函数：使用 Mish 激活函数 以上并非全部的改进部分，还存在一些其它的改进，由于 YOLOV4 使用的改进实在太多了，很难完全实现与列出来，这里只列出来了一些我比较感兴趣，而且非常有效的改进。 还有一个重要的事情：论文中提到的 SAM，作者自己的源码也没有使用。 还有其它很多的 tricks，不是所有的 tricks 都有提升，我也没法实现全部的 tricks。 整篇 BLOG 会结合 YOLOV3 与 YOLOV4 的差别进行解析 6. YOLOV4 结构解析 为方便理解，本文将所有通道数都放到了最后一维度。为方便理解，本文将所有通道数都放到了最后一维度。为方便理解，本文将所有通道数都放到了最后一维度。 6.1. 1、主干特征提取网络 Backbone 当输入是 416x416 时，特征结构如下：当输入是 608x608 时，特征结构如下：主干特征提取网络 Backbone 的改进点有两个：a). 主干特征提取网络：DarkNet53 => CSPDarkNet53b). 激活函数：使用 Mish 激活函数 如果大家对 YOLOV3 比较熟悉的话，应该知道 Darknet53 的结构，其由一系列残差网络结构构成。在 Darknet53 中，其存在 resblock_body 模块，其由一次下采样和多次残差结构的堆叠构成，Darknet53 便是由 resblock_body 模块组合而成。 而在 YOLOV4 中，其对该部分进行了一定的修改。1、其一是将 DarknetConv2D 的激活函数由 LeakyReLU 修改成了 Mish，卷积块由 DarknetConv2D_BN_Leaky 变成了 DarknetConv2D_BN_Mish。Mish 函数的公式与图像如下：M i s h = x × t a n h ( l n ( 1 + e x ) ) Mish=x \\times tanh(ln(1+e^x)) Mish=x×tanh(ln(1+ex))2、其二是将 resblock_body 的结构进行修改，使用了 CSPnet 结构。此时 YOLOV4 当中的 Darknet53 被修改成了 CSPDarknet53。CSPnet 结构并不算复杂，就是将原来的残差块的堆叠进行了一个拆分，拆成左右两部分：主干部分继续进行原来的残差块的堆叠；另一部分则像一个残差边一样，经过少量处理直接连接到最后。因此可以认为 CSP 中存在一个大的残差边。 #---------------------------------------------------# # CSPdarknet的结构块 # 存在一个大残差边 # 这个大残差边绕过了很多的残差结构 #---------------------------------------------------# class Resblock_body(nn.Module): def __init__(self, in_channels, out_channels, num_blocks, first): super(Resblock_body, self).__init__() self.downsample_conv = BasicConv(in_channels, out_channels, 3, stride=2) if first: self.split_conv0 = BasicConv(out_channels, out_channels, 1) self.split_conv1 = BasicConv(out_channels, out_channels, 1) self.blocks_conv = nn.Sequential( Resblock(channels=out_channels, hidden_channels=out_channels//2), BasicConv(out_channels, out_channels, 1) ) self.concat_conv = BasicConv(out_channels*2, out_channels, 1) else: self.split_conv0 = BasicConv(out_channels, out_channels//2, 1) self.split_conv1 = BasicConv(out_channels, out_channels//2, 1) self.blocks_conv = nn.Sequential( *[Resblock(out_channels//2) for _ in range(num_blocks)], BasicConv(out_channels//2, out_channels//2, 1) ) self.concat_conv = BasicConv(out_channels, out_channels, 1) def forward(self, x): x = self.downsample_conv(x) x0 = self.split_conv0(x) x1 = self.split_conv1(x) x1 = self.blocks_conv(x1) x = torch.cat([x1, x0], dim=1) x = self.concat_conv(x) return x 全部实现代码为： import torch import torch.nn.functional as F import torch.nn as nn import math from collections import OrderedDict #-------------------------------------------------# # MISH激活函数 #-------------------------------------------------# class Mish(nn.Module): def __init__(self): super(Mish, self).__init__() def forward(self, x): return x * torch.tanh(F.softplus(x)) #-------------------------------------------------# # 卷积块 # CONV+BATCHNORM+MISH #-------------------------------------------------# class BasicConv(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride=1): super(BasicConv, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False) self.bn = nn.BatchNorm2d(out_channels) self.activation = Mish() def forward(self, x): x = self.conv(x) x = self.bn(x) x = self.activation(x) return x #---------------------------------------------------# # CSPdarknet的结构块的组成部分 # 内部堆叠的残差块 #---------------------------------------------------# class Resblock(nn.Module): def __init__(self, channels, hidden_channels=None, residual_activation=nn.Identity()): super(Resblock, self).__init__() if hidden_channels is None: hidden_channels = channels self.block = nn.Sequential( BasicConv(channels, hidden_channels, 1), BasicConv(hidden_channels, channels, 3) ) def forward(self, x): return x+self.block(x) #---------------------------------------------------# # CSPdarknet的结构块 # 存在一个大残差边 # 这个大残差边绕过了很多的残差结构 #---------------------------------------------------# class Resblock_body(nn.Module): def __init__(self, in_channels, out_channels, num_blocks, first): super(Resblock_body, self).__init__() self.downsample_conv = BasicConv(in_channels, out_channels, 3, stride=2) if first: self.split_conv0 = BasicConv(out_channels, out_channels, 1) self.split_conv1 = BasicConv(out_channels, out_channels, 1) self.blocks_conv = nn.Sequential( Resblock(channels=out_channels, hidden_channels=out_channels//2), BasicConv(out_channels, out_channels, 1) ) self.concat_conv = BasicConv(out_channels*2, out_channels, 1) else: self.split_conv0 = BasicConv(out_channels, out_channels//2, 1) self.split_conv1 = BasicConv(out_channels, out_channels//2, 1) self.blocks_conv = nn.Sequential( *[Resblock(out_channels//2) for _ in range(num_blocks)], BasicConv(out_channels//2, out_channels//2, 1) ) self.concat_conv = BasicConv(out_channels, out_channels, 1) def forward(self, x): x = self.downsample_conv(x) x0 = self.split_conv0(x) x1 = self.split_conv1(x) x1 = self.blocks_conv(x1) x = torch.cat([x1, x0], dim=1) x = self.concat_conv(x) return x class CSPDarkNet(nn.Module): def __init__(self, layers): super(CSPDarkNet, self).__init__() self.inplanes = 32 self.conv1 = BasicConv(3, self.inplanes, kernel_size=3, stride=1) self.feature_channels = [64, 128, 256, 512, 1024] self.stages = nn.ModuleList([ Resblock_body(self.inplanes, self.feature_channels[0], layers[0], first=True), Resblock_body(self.feature_channels[0], self.feature_channels[1], layers[1], first=False), Resblock_body(self.feature_channels[1], self.feature_channels[2], layers[2], first=False), Resblock_body(self.feature_channels[2], self.feature_channels[3], layers[3], first=False), Resblock_body(self.feature_channels[3], self.feature_channels[4], layers[4], first=False) ]) self.num_features = 1 # 进行权值初始化 for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() def forward(self, x): x = self.conv1(x) x = self.stages[0](x) x = self.stages[1](x) out3 = self.stages[2](x) out4 = self.stages[3](out3) out5 = self.stages[4](out4) return out3, out4, out5 def darknet53(pretrained, **kwargs): model = CSPDarkNet([1, 2, 8, 8, 4]) if pretrained: if isinstance(pretrained, str): model.load_state_dict(torch.load(pretrained)) else: raise Exception(\"darknet request a pretrained path. got [{}]\".format(pretrained)) return model 6.2. 2、特征金字塔 当输入是 416x416 时，特征结构如下：当输入是 608x608 时，特征结构如下：在特征金字塔部分，YOLOV4 结合了两种改进:a). 使用了 SPP 结构。b). 使用了 PANet 结构。如上图所示，除去 CSPDarknet53 和 Yolo Head 的结构外，都是特征金字塔的结构。1、SPP 结构参杂在对 CSPdarknet53 的最后一个特征层的卷积里，在对 CSPdarknet53 的最后一个特征层进行三次 DarknetConv2D_BN_Leaky 卷积后，分别利用四个不同尺度的最大池化进行处理，最大池化的池化核大小分别为 13x13、9x9、5x5、1x1（1x1 即无处理） #---------------------------------------------------# # SPP结构，利用不同大小的池化核进行池化 # 池化后堆叠 #---------------------------------------------------# class SpatialPyramidPooling(nn.Module): def __init__(self, pool_sizes=[5, 9, 13]): super(SpatialPyramidPooling, self).__init__() self.maxpools = nn.ModuleList([nn.MaxPool2d(pool_size, 1, pool_size//2) for pool_size in pool_sizes]) def forward(self, x): features = [maxpool(x) for maxpool in self.maxpools[::-1]] features = torch.cat(features + [x], dim=1) return features 其可以它能够极大地增加感受野，分离出最显著的上下文特征。2、PANet 是 2018 的一种实例分割算法，其具体结构由反复提升特征的意思。上图为原始的 PANet 的结构，可以看出来其具有一个非常重要的特点就是特征的反复提取。在（a）里面是传统的特征金字塔结构，在完成特征金字塔从下到上的特征提取后，还需要实现（b）中从上到下的特征提取。 而在 YOLOV4 当中，其主要是在三个有效特征层上使用了 PANet 结构。实现代码如下： #---------------------------------------------------# # yolo_body #---------------------------------------------------# class YoloBody(nn.Module): def __init__(self, config): super(YoloBody, self).__init__() self.config = config # backbone self.backbone = darknet53(None) self.conv1 = make_three_conv([512,1024],1024) self.SPP = SpatialPyramidPooling() self.conv2 = make_three_conv([512,1024],2048) self.upsample1 = Upsample(512,256) self.conv_for_P4 = conv2d(512,256,1) self.make_five_conv1 = make_five_conv([256, 512],512) self.upsample2 = Upsample(256,128) self.conv_for_P3 = conv2d(256,128,1) self.make_five_conv2 = make_five_conv([128, 256],256) # 3*(5+num_classes)=3*(5+20)=3*(4+1+20)=75 final_out_filter2 = len(config[\"yolo\"][\"anchors\"][2]) * (5 + config[\"yolo\"][\"classes\"]) self.yolo_head3 = yolo_head([256, final_out_filter2],128) self.down_sample1 = conv2d(128,256,3,stride=2) self.make_five_conv3 = make_five_conv([256, 512],512) # 3*(5+num_classes)=3*(5+20)=3*(4+1+20)=75 final_out_filter1 = len(config[\"yolo\"][\"anchors\"][1]) * (5 + config[\"yolo\"][\"classes\"]) self.yolo_head2 = yolo_head([512, final_out_filter1],256) self.down_sample2 = conv2d(256,512,3,stride=2) self.make_five_conv4 = make_five_conv([512, 1024],1024) # 3*(5+num_classes)=3*(5+20)=3*(4+1+20)=75 final_out_filter0 = len(config[\"yolo\"][\"anchors\"][0]) * (5 + config[\"yolo\"][\"classes\"]) self.yolo_head1 = yolo_head([1024, final_out_filter0],512) def forward(self, x): # backbone x2, x1, x0 = self.backbone(x) P5 = self.conv1(x0) P5 = self.SPP(P5) P5 = self.conv2(P5) P5_upsample = self.upsample1(P5) P4 = self.conv_for_P4(x1) P4 = torch.cat([P4,P5_upsample],axis=1) P4 = self.make_five_conv1(P4) P4_upsample = self.upsample2(P4) P3 = self.conv_for_P3(x2) P3 = torch.cat([P3,P4_upsample],axis=1) P3 = self.make_five_conv2(P3) P3_downsample = self.down_sample1(P3) P4 = torch.cat([P3_downsample,P4],axis=1) P4 = self.make_five_conv3(P4) P4_downsample = self.down_sample2(P4) P5 = torch.cat([P4_downsample,P5],axis=1) P5 = self.make_five_conv4(P5) out2 = self.yolo_head3(P3) out1 = self.yolo_head2(P4) out0 = self.yolo_head1(P5) return out0, out1, out2 6.3. 3、YoloHead 利用获得到的特征进行预测 当输入是 416x416 时，特征结构如下：当输入是 608x608 时，特征结构如下：1、在特征利用部分，YoloV4 提取多特征层进行目标检测，一共提取三个特征层，分别位于中间层，中下层，底层，三个特征层的 shape 分别为 (76,76,256)、(38,38,512)、(19,19,1024)。 2、输出层的 shape 分别为 (19,19,75)，(38,38,75)，(76,76,75)，最后一个维度为 75 是因为该图是基于 voc 数据集的，它的类为 20 种，YoloV4 只有针对每一个特征层存在 3 个先验框，所以最后维度为 3x25；如果使用的是 coco 训练集，类则为 80 种，最后的维度应该为 255 = 3x85，三个特征层的 shape 为 (19,19,255)，(38,38,255)，(76,76,255) 实现代码如下： #---------------------------------------------------# # 最后获得yolov4的输出 #---------------------------------------------------# def yolo_head(filters_list, in_filters): m = nn.Sequential( conv2d(in_filters, filters_list[0], 3), nn.Conv2d(filters_list[0], filters_list[1], 1), ) return m #---------------------------------------------------# # yolo_body #---------------------------------------------------# class YoloBody(nn.Module): def __init__(self, config): super(YoloBody, self).__init__() self.config = config # backbone self.backbone = darknet53(None) self.conv1 = make_three_conv([512,1024],1024) self.SPP = SpatialPyramidPooling() self.conv2 = make_three_conv([512,1024],2048) self.upsample1 = Upsample(512,256) self.conv_for_P4 = conv2d(512,256,1) self.make_five_conv1 = make_five_conv([256, 512],512) self.upsample2 = Upsample(256,128) self.conv_for_P3 = conv2d(256,128,1) self.make_five_conv2 = make_five_conv([128, 256],256) # 3*(5+num_classes)=3*(5+20)=3*(4+1+20)=75 final_out_filter2 = len(config[\"yolo\"][\"anchors\"][2]) * (5 + config[\"yolo\"][\"classes\"]) self.yolo_head3 = yolo_head([256, final_out_filter2],128) self.down_sample1 = conv2d(128,256,3,stride=2) self.make_five_conv3 = make_five_conv([256, 512],512) # 3*(5+num_classes)=3*(5+20)=3*(4+1+20)=75 final_out_filter1 = len(config[\"yolo\"][\"anchors\"][1]) * (5 + config[\"yolo\"][\"classes\"]) self.yolo_head2 = yolo_head([512, final_out_filter1],256) self.down_sample2 = conv2d(256,512,3,stride=2) self.make_five_conv4 = make_five_conv([512, 1024],1024) # 3*(5+num_classes)=3*(5+20)=3*(4+1+20)=75 final_out_filter0 = len(config[\"yolo\"][\"anchors\"][0]) * (5 + config[\"yolo\"][\"classes\"]) self.yolo_head1 = yolo_head([1024, final_out_filter0],512) def forward(self, x): # backbone x2, x1, x0 = self.backbone(x) P5 = self.conv1(x0) P5 = self.SPP(P5) P5 = self.conv2(P5) P5_upsample = self.upsample1(P5) P4 = self.conv_for_P4(x1) P4 = torch.cat([P4,P5_upsample],axis=1) P4 = self.make_five_conv1(P4) P4_upsample = self.upsample2(P4) P3 = self.conv_for_P3(x2) P3 = torch.cat([P3,P4_upsample],axis=1) P3 = self.make_five_conv2(P3) P3_downsample = self.down_sample1(P3) P4 = torch.cat([P3_downsample,P4],axis=1) P4 = self.make_five_conv3(P4) P4_downsample = self.down_sample2(P4) P5 = torch.cat([P4_downsample,P5],axis=1) P5 = self.make_five_conv4(P5) out2 = self.yolo_head3(P3) out1 = self.yolo_head2(P4) out0 = self.yolo_head1(P5) return out0, out1, out2 6.4. 4、预测结果的解码 由第二步我们可以获得三个特征层的预测结果，shape 分别为 (N,19,19,255)，(N,38,38,255)，(N,76,76,255) 的数据，对应每个图分为 19x19、38x38、76x76 的网格上 3 个预测框的位置。 但是这个预测结果并不对应着最终的预测框在图片上的位置，还需要解码才可以完成。 此处要讲一下 yolo3 的预测原理，yolo3 的 3 个特征层分别将整幅图分为 19x19、38x38、76x76 的网格，每个网络点负责一个区域的检测。 我们知道特征层的预测结果对应着三个预测框的位置，我们先将其 reshape 一下，其结果为 (N,19,19,3,85)，(N,38,38,3,85)，(N,76,76,3,85)。 最后一个维度中的 85 包含了 4+1+80，分别代表 x_offset、y_offset、h 和 w、置信度、分类结果。 yolo3 的解码过程就是将每个网格点加上它对应的 x_offset 和 y_offset，加完后的结果就是预测框的中心，然后再利用 先验框和 h、w 结合 计算出预测框的长和宽。这样就能得到整个预测框的位置了。 当然得到最终的预测结构后还要进行得分排序与非极大抑制筛选这一部分基本上是所有目标检测通用的部分。不过该项目的处理方式与其它项目不同。其对于每一个类进行判别。1、取出每一类得分大于 self.obj_threshold 的框和得分。2、利用框的位置和得分进行非极大抑制。 实现代码如下，当调用 yolo_eval 时，就会对每个特征层进行解码： class DecodeBox(nn.Module): def __init__(self, anchors, num_classes, img_size): super(DecodeBox, self).__init__() self.anchors = anchors self.num_anchors = len(anchors) self.num_classes = num_classes self.bbox_attrs = 5 + num_classes self.img_size = img_size def forward(self, input): # input为bs,3*(1+4+num_classes),13,13 # 一共多少张图片 batch_size = input.size(0) # 13，13 input_height = input.size(2) input_width = input.size(3) # 计算步长 # 每一个特征点对应原来的图片上多少个像素点 # 如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点 # 416/13 = 32 stride_h = self.img_size[1] / input_height stride_w = self.img_size[0] / input_width # 把先验框的尺寸调整成特征层大小的形式 # 计算出先验框在特征层上对应的宽高 scaled_anchors = [(anchor_width / stride_w, anchor_height / stride_h) for anchor_width, anchor_height in self.anchors] # bs,3*(5+num_classes),13,13 -> bs,3,13,13,(5+num_classes) prediction = input.view(batch_size, self.num_anchors, self.bbox_attrs, input_height, input_width).permute(0, 1, 3, 4, 2).contiguous() # 先验框的中心位置的调整参数 x = torch.sigmoid(prediction[..., 0]) y = torch.sigmoid(prediction[..., 1]) # 先验框的宽高调整参数 w = prediction[..., 2] # Width h = prediction[..., 3] # Height # 获得置信度，是否有物体 conf = torch.sigmoid(prediction[..., 4]) # 种类置信度 pred_cls = torch.sigmoid(prediction[..., 5:]) # Cls pred. FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor # 生成网格，先验框中心，网格左上角 batch_size,3,13,13 grid_x = torch.linspace(0, input_width - 1, input_width).repeat(input_width, 1).repeat( batch_size * self.num_anchors, 1, 1).view(x.shape).type(FloatTensor) grid_y = torch.linspace(0, input_height - 1, input_height).repeat(input_height, 1).t().repeat( batch_size * self.num_anchors, 1, 1).view(y.shape).type(FloatTensor) # 生成先验框的宽高 anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0])) anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1])) anchor_w = anchor_w.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(w.shape) anchor_h = anchor_h.repeat(batch_size, 1).repeat(1, 1, input_height * input_width).view(h.shape) # 计算调整后的先验框中心与宽高 pred_boxes = FloatTensor(prediction[..., :4].shape) pred_boxes[..., 0] = x.data + grid_x pred_boxes[..., 1] = y.data + grid_y pred_boxes[..., 2] = torch.exp(w.data) * anchor_w pred_boxes[..., 3] = torch.exp(h.data) * anchor_h # 用于将输出调整为相对于416x416的大小 _scale = torch.Tensor([stride_w, stride_h] * 2).type(FloatTensor) output = torch.cat((pred_boxes.view(batch_size, -1, 4) * _scale, conf.view(batch_size, -1, 1), pred_cls.view(batch_size, -1, self.num_classes)), -1) return output.data 6.5. 5、在原图上进行绘制 通过第四步，我们可以获得预测框在原图上的位置，而且这些预测框都是经过筛选的。这些筛选后的框可以直接绘制在图片上，就可以获得结果了。 7. YOLOV4 的训练 7.1. 1、YOLOV4 的改进训练技巧 7.1.1. a)、Mosaic 数据增强 Yolov4 的 mosaic 数据增强参考了 CutMix 数据增强方式，理论上具有一定的相似性！CutMix 数据增强方式利用两张图片进行拼接。但是 mosaic 利用了四张图片，根据论文所说其拥有一个巨大的优点是丰富检测物体的背景！且在 BN 计算的时候一下子会计算四张图片的数据！就像下图这样：实现思路如下：1、每次读取四张图片。 2、分别对四张图片进行翻转、缩放、色域变化等，并且按照四个方向位置摆好。3、进行图片的组合和框的组合 def rand(a=0, b=1): return np.random.rand()*(b-a) + a def merge_bboxes(bboxes, cutx, cuty): merge_bbox = [] for i in range(len(bboxes)): for box in bboxes[i]: tmp_box = [] x1,y1,x2,y2 = box[0], box[1], box[2], box[3] if i == 0: if y1 > cuty or x1 > cutx: continue if y2 >= cuty and y1 = cutx and x1 cutx: continue if y2 >= cuty and y1 = cutx and x1 = cuty and y1 = cutx and x1 cuty or x2 = cuty and y1 = cutx and x1 0: image = image.transpose(Image.FLIP_LEFT_RIGHT) box[:, [0,2]] = iw - box[:, [2,0]] # 对输入进来的图片进行缩放 new_ar = w/h scale = rand(scale_low, scale_high) if new_ar 1] -= 1 x[..., 0][x[..., 0]1] = 1 x[x0: np.random.shuffle(box) box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy box[:, 0:2][box[:, 0:2]w] = w box[:, 3][box[:, 3]>h] = h box_w = box[:, 2] - box[:, 0] box_h = box[:, 3] - box[:, 1] box = box[np.logical_and(box_w>1, box_h>1)] box_data = np.zeros((len(box),5)) box_data[:len(box)] = box image_datas.append(image_data) box_datas.append(box_data) img = Image.fromarray((image_data*255).astype(np.uint8)) for j in range(len(box_data)): thickness = 3 left, top, right, bottom = box_data[j][0:4] draw = ImageDraw.Draw(img) for i in range(thickness): draw.rectangle([left + i, top + i, right - i, bottom - i],outline=(255,255,255)) img.show() # 将图片分割，放在一起 cutx = np.random.randint(int(w*min_offset_x), int(w*(1 - min_offset_x))) cuty = np.random.randint(int(h*min_offset_y), int(h*(1 - min_offset_y))) new_image = np.zeros([h,w,3]) new_image[:cuty, :cutx, :] = image_datas[0][:cuty, :cutx, :] new_image[cuty:, :cutx, :] = image_datas[1][cuty:, :cutx, :] new_image[cuty:, cutx:, :] = image_datas[2][cuty:, cutx:, :] new_image[:cuty, cutx:, :] = image_datas[3][:cuty, cutx:, :] # 对框进行进一步的处理 new_boxes = merge_bboxes(box_datas, cutx, cuty) return new_image, new_boxes 7.1.2. b)、Label Smoothing 平滑 标签平滑的思想很简单，具体公式如下： new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes 当 label_smoothing 的值为 0.01 得时候，公式变成如下所示： new_onehot_labels = y * (1 - 0.01) + 0.01 / num_classes 其实 Label Smoothing 平滑就是将标签进行一个平滑，原始的标签是 0、1，在平滑后变成 0.005(如果是二分类)、0.995，也就是说对分类准确做了一点惩罚，让模型不可以分类的太准确，太准确容易过拟合。 实现代码如下： #---------------------------------------------------# # 平滑标签 #---------------------------------------------------# def smooth_labels(y_true, label_smoothing,num_classes): return y_true * (1.0 - label_smoothing) + label_smoothing / num_classes 7.1.3. c)、CIOU IoU 是比值的概念，对目标物体的 scale 是不敏感的。然而常用的 BBox 的回归损失优化和 IoU 优化不是完全等价的，寻常的 IoU 无法直接优化没有重叠的部分。 于是有人提出直接使用 IOU 作为回归优化 loss，CIOU 是其中非常优秀的一种想法。 CIOU 将目标与 anchor 之间的距离，重叠率、尺度以及惩罚项都考虑进去，使得目标框回归变得更加稳定，不会像 IoU 和 GIoU 一样出现训练过程中发散等问题。而惩罚因子把预测框长宽比拟合目标框的长宽比考虑进去。 CIOU 公式如下C I O U = I O U − ρ 2 ( b , b g t ) c 2 − α v CIOU = IOU - \\frac{\\rho^2(b,b^{gt})}{c^2} - \\alpha v CIOU=IOU−c2ρ2(b,bgt)​−αv其中， ρ 2 ( b , b g t ) \\rho^2(b,b^{gt}) ρ2(b,bgt) 分别代表了预测框和真实框的中心点的欧式距离。 c 代表的是能够同时包含预测框和真实框的最小闭包区域的对角线距离。 而 α \\alpha α和 v v v 的公式如下α = v 1 − I O U + v \\alpha = \\frac{v}{1-IOU+v} α=1−IOU+vv​v = 4 π 2 ( a r c t a n w g t h g t − a r c t a n w h ) 2 v = \\frac{4}{\\pi ^2}(arctan\\frac{w^{gt}}{h^{gt}}-arctan\\frac{w}{h})^2 v=π24​(arctanhgtwgt​−arctanhw​)2把 1-CIOU 就可以得到相应的 LOSS 了。L O S S C I O U = 1 − I O U + ρ 2 ( b , b g t ) c 2 + α v LOSS_{CIOU} = 1 - IOU + \\frac{\\rho^2(b,b^{gt})}{c^2} + \\alpha v LOSSCIOU​=1−IOU+c2ρ2(b,bgt)​+αv def box_ciou(b1, b2): \"\"\" 输入为： ---------- b1: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh b2: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh 返回为： ------- ciou: tensor, shape=(batch, feat_w, feat_h, anchor_num, 1) \"\"\" # 求出预测框左上角右下角 b1_xy = b1[..., :2] b1_wh = b1[..., 2:4] b1_wh_half = b1_wh/2. b1_mins = b1_xy - b1_wh_half b1_maxes = b1_xy + b1_wh_half # 求出真实框左上角右下角 b2_xy = b2[..., :2] b2_wh = b2[..., 2:4] b2_wh_half = b2_wh/2. b2_mins = b2_xy - b2_wh_half b2_maxes = b2_xy + b2_wh_half # 求真实框和预测框所有的iou intersect_mins = torch.max(b1_mins, b2_mins) intersect_maxes = torch.min(b1_maxes, b2_maxes) intersect_wh = torch.max(intersect_maxes - intersect_mins, torch.zeros_like(intersect_maxes)) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] b1_area = b1_wh[..., 0] * b1_wh[..., 1] b2_area = b2_wh[..., 0] * b2_wh[..., 1] union_area = b1_area + b2_area - intersect_area iou = intersect_area / (union_area + 1e-6) # 计算中心的差距 center_distance = torch.sum(torch.pow((b1_xy - b2_xy), 2), axis=-1) # 找到包裹两个框的最小框的左上角和右下角 enclose_mins = torch.min(b1_mins, b2_mins) enclose_maxes = torch.max(b1_maxes, b2_maxes) enclose_wh = torch.max(enclose_maxes - enclose_mins, torch.zeros_like(intersect_maxes)) # 计算对角线距离 enclose_diagonal = torch.sum(torch.pow(enclose_wh,2), axis=-1) ciou = iou - 1.0 * (center_distance) / (enclose_diagonal + 1e-7) v = (4 / (math.pi ** 2)) * torch.pow((torch.atan(b1_wh[..., 0]/b1_wh[..., 1]) - torch.atan(b2_wh[..., 0]/b2_wh[..., 1])), 2) alpha = v / (1.0 - iou + v) ciou = ciou - alpha * v return ciou 7.1.4. d)、学习率余弦退火衰减 余弦退火衰减法，学习率会先上升再下降，这是退火优化法的思想。（关于什么是退火算法可以百度。） 上升的时候使用线性上升，下降的时候模拟 cos 函数下降。执行多次。 效果如图所示：pytorch 有直接实现的函数，可直接调用。 lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5) 7.2. 2、loss 组成 7.2.1. a)、计算 loss 所需参数 在计算 loss 的时候，实际上是 y_pre 和 y_true 之间的对比：y_pre 就是一幅图像经过网络之后的输出，内部含有三个特征层的内容；其需要解码才能够在图上作画y_true 就是一个真实图像中，它的每个真实框对应的 (19,19)、(38,38)、(76,76) 网格上的偏移位置、长宽与种类。其仍需要编码才能与 y_pred 的结构一致实际上 y_pre 和 y_true 内容的 shape 都是(batch_size,19,19,3,85)(batch_size,38,38,3,85)(batch_size,76,76,3,85) 7.2.2. b)、y_pre 是什么 网络最后输出的内容就是三个特征层每个网格点对应的预测框及其种类，即三个特征层分别对应着图片被分为不同 size 的网格后，每个网格点上三个先验框对应的位置、置信度及其种类。对于输出的 y1、y2、y3 而言，[…, : 2] 指的是相对于每个网格点的偏移量，[…, 2: 4] 指的是宽和高，[…, 4: 5] 指的是该框的置信度，[…, 5: ] 指的是每个种类的预测概率。现在的 y_pre 还是没有解码的，解码了之后才是真实图像上的情况。 7.2.3. c)、y_true 是什么。 y_true 就是一个真实图像中，它的每个真实框对应的 (19,19)、(38,38)、(76,76) 网格上的偏移位置、长宽与种类。其仍需要编码才能与 y_pred 的结构一致 7.2.4. d)、loss 的计算过程 在得到了 y_pre 和 y_true 后怎么对比呢？不是简单的减一下! loss 值需要对三个特征层进行处理，这里以最小的特征层为例。1、利用 y_true 取出该特征层中真实存在目标的点的位置 (m,19,19,3,1) 及其对应的种类(m,19,19,3,80)。2、将 prediction 的预测值输出进行处理，得到 reshape 后的预测值 y_pre，shape 为 (m,19,19,3,85)。还有解码后的 xy，wh。3、对于每一幅图，计算其中所有真实框与预测框的 IOU，如果某些预测框和真实框的重合程度大于 0.5，则忽略。4、计算 ciou 作为回归的 loss，这里只计算正样本的回归 loss。5、计算置信度的 loss，其有两部分构成，第一部分是实际上存在目标的，预测结果中置信度的值与 1 对比；第二部分是实际上不存在目标的，在第四步中得到其最大 IOU 的值与 0 对比。6、计算预测种类的 loss，其计算的是实际上存在目标的，预测类与真实类的差距。 其实际上计算的总的 loss 是三个 loss 的和，这三个 loss 分别是： 实际存在的框，CIOU LOSS。 实际存在的框，预测结果中置信度的值与 1 对比；实际不存在的框，预测结果中置信度的值与 0 对比，该部分要去除被忽略的不包含目标的框。 实际存在的框，种类预测结果与实际结果的对比。 其实际代码如下： #---------------------------------------------------# # 平滑标签 #---------------------------------------------------# def smooth_labels(y_true, label_smoothing,num_classes): return y_true * (1.0 - label_smoothing) + label_smoothing / num_classes def box_ciou(b1, b2): \"\"\" 输入为： ---------- b1: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh b2: tensor, shape=(batch, feat_w, feat_h, anchor_num, 4), xywh 返回为： ------- ciou: tensor, shape=(batch, feat_w, feat_h, anchor_num, 1) \"\"\" # 求出预测框左上角右下角 b1_xy = b1[..., :2] b1_wh = b1[..., 2:4] b1_wh_half = b1_wh/2. b1_mins = b1_xy - b1_wh_half b1_maxes = b1_xy + b1_wh_half # 求出真实框左上角右下角 b2_xy = b2[..., :2] b2_wh = b2[..., 2:4] b2_wh_half = b2_wh/2. b2_mins = b2_xy - b2_wh_half b2_maxes = b2_xy + b2_wh_half # 求真实框和预测框所有的iou intersect_mins = torch.max(b1_mins, b2_mins) intersect_maxes = torch.min(b1_maxes, b2_maxes) intersect_wh = torch.max(intersect_maxes - intersect_mins, torch.zeros_like(intersect_maxes)) intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] b1_area = b1_wh[..., 0] * b1_wh[..., 1] b2_area = b2_wh[..., 0] * b2_wh[..., 1] union_area = b1_area + b2_area - intersect_area iou = intersect_area / (union_area + 1e-6) # 计算中心的差距 center_distance = torch.sum(torch.pow((b1_xy - b2_xy), 2), axis=-1) # 找到包裹两个框的最小框的左上角和右下角 enclose_mins = torch.min(b1_mins, b2_mins) enclose_maxes = torch.max(b1_maxes, b2_maxes) enclose_wh = torch.max(enclose_maxes - enclose_mins, torch.zeros_like(intersect_maxes)) # 计算对角线距离 enclose_diagonal = torch.sum(torch.pow(enclose_wh,2), axis=-1) ciou = iou - 1.0 * (center_distance) / (enclose_diagonal + 1e-7) v = (4 / (math.pi ** 2)) * torch.pow((torch.atan(b1_wh[..., 0]/b1_wh[..., 1]) - torch.atan(b2_wh[..., 0]/b2_wh[..., 1])), 2) alpha = v / (1.0 - iou + v) ciou = ciou - alpha * v return ciou def clip_by_tensor(t,t_min,t_max): t=t.float() result = (t >= t_min).float() * t + (t t_max).float() * t_max return result def MSELoss(pred,target): return (pred-target)**2 def BCELoss(pred,target): epsilon = 1e-7 pred = clip_by_tensor(pred, epsilon, 1.0 - epsilon) output = -target * torch.log(pred) - (1.0 - target) * torch.log(1.0 - pred) return output class YOLOLoss(nn.Module): def __init__(self, anchors, num_classes, img_size, label_smooth=0, cuda=True): super(YOLOLoss, self).__init__() self.anchors = anchors self.num_anchors = len(anchors) self.num_classes = num_classes self.bbox_attrs = 5 + num_classes self.img_size = img_size self.label_smooth = label_smooth self.ignore_threshold = 0.5 self.lambda_conf = 1.0 self.lambda_cls = 1.0 self.lambda_loc = 1.0 self.cuda = cuda def forward(self, input, targets=None): # input为bs,3*(5+num_classes),13,13 # 一共多少张图片 bs = input.size(0) # 特征层的高 in_h = input.size(2) # 特征层的宽 in_w = input.size(3) # 计算步长 # 每一个特征点对应原来的图片上多少个像素点 # 如果特征层为13x13的话，一个特征点就对应原来的图片上的32个像素点 stride_h = self.img_size[1] / in_h stride_w = self.img_size[0] / in_w # 把先验框的尺寸调整成特征层大小的形式 # 计算出先验框在特征层上对应的宽高 scaled_anchors = [(a_w / stride_w, a_h / stride_h) for a_w, a_h in self.anchors] # bs,3*(5+num_classes),13,13 -> bs,3,13,13,(5+num_classes) prediction = input.view(bs, int(self.num_anchors/3), self.bbox_attrs, in_h, in_w).permute(0, 1, 3, 4, 2).contiguous() # 对prediction预测进行调整 conf = torch.sigmoid(prediction[..., 4]) # Conf pred_cls = torch.sigmoid(prediction[..., 5:]) # Cls pred. # 找到哪些先验框内部包含物体 mask, noobj_mask, t_box, tconf, tcls, box_loss_scale_x, box_loss_scale_y = self.get_target(targets, scaled_anchors,in_w, in_h,self.ignore_threshold) noobj_mask, pred_boxes_for_ciou = self.get_ignore(prediction, targets, scaled_anchors, in_w, in_h, noobj_mask) if self.cuda: mask, noobj_mask = mask.cuda(), noobj_mask.cuda() box_loss_scale_x, box_loss_scale_y= box_loss_scale_x.cuda(), box_loss_scale_y.cuda() tconf, tcls = tconf.cuda(), tcls.cuda() pred_boxes_for_ciou = pred_boxes_for_ciou.cuda() t_box = t_box.cuda() box_loss_scale = 2-box_loss_scale_x*box_loss_scale_y # losses. ciou = (1 - box_ciou( pred_boxes_for_ciou[mask.bool()], t_box[mask.bool()]))* box_loss_scale[mask.bool()] loss_loc = torch.sum(ciou / bs) loss_conf = torch.sum(BCELoss(conf, mask) * mask / bs) + \\ torch.sum(BCELoss(conf, mask) * noobj_mask / bs) # print(smooth_labels(tcls[mask == 1],self.label_smooth,self.num_classes)) loss_cls = torch.sum(BCELoss(pred_cls[mask == 1], smooth_labels(tcls[mask == 1],self.label_smooth,self.num_classes))/bs) # print(loss_loc,loss_conf,loss_cls) loss = loss_conf * self.lambda_conf + loss_cls * self.lambda_cls + loss_loc * self.lambda_loc return loss, loss_conf.item(), loss_cls.item(), loss_loc.item() def get_target(self, target, anchors, in_w, in_h, ignore_threshold): # 计算一共有多少张图片 bs = len(target) # 获得先验框 anchor_index = [[0,1,2],[3,4,5],[6,7,8]][[13,26,52].index(in_w)] subtract_index = [0,3,6][[13,26,52].index(in_w)] # 创建全是0或者全是1的阵列 mask = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) noobj_mask = torch.ones(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) tx = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) ty = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) tw = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) th = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) t_box = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, 4, requires_grad=False) tconf = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) tcls = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, self.num_classes, requires_grad=False) box_loss_scale_x = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) box_loss_scale_y = torch.zeros(bs, int(self.num_anchors/3), in_h, in_w, requires_grad=False) for b in range(bs): for t in range(target[b].shape[0]): # 计算出在特征层上的点位 gx = target[b][t, 0] * in_w gy = target[b][t, 1] * in_h gw = target[b][t, 2] * in_w gh = target[b][t, 3] * in_h # 计算出属于哪个网格 gi = int(gx) gj = int(gy) # 计算真实框的位置 gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0) # 计算出所有先验框的位置 anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((self.num_anchors, 2)), np.array(anchors)), 1)) # 计算重合程度 anch_ious = bbox_iou(gt_box, anchor_shapes) # Find the best matching anchor box best_n = np.argmax(anch_ious) if best_n not in anchor_index: continue # Masks if (gj self.ignore_threshold] = 0 return noobj_mask, pred_boxes 8. 训练自己的 YOLOV4 模型 yolo4 整体的文件夹构架如下：本文使用 VOC 格式进行训练。训练前将标签文件放在 VOCdevkit 文件夹下的 VOC2007 文件夹下的 Annotation 中。训练前将图片文件放在 VOCdevkit 文件夹下的 VOC2007 文件夹下的 JPEGImages 中。在训练前利用 voc2yolo3.py 文件生成对应的 txt。再运行根目录下的 voc_annotation.py，运行前需要将 classes 改成你自己的 classes。 classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"] 就会生成对应的 2007_train.txt，每一行对应其图片位置及其真实框的位置。在训练前需要修改 model_data 里面的 voc_classes.txt 文件，需要将 classes 改成你自己的 classes。运行 train.py 即可开始训练。 "},"5 - 实验笔记/yolo--- 参数解释之 cfg 文件参数.html":{"url":"5 - 实验笔记/yolo--- 参数解释之 cfg 文件参数.html","title":"yolo--- 参数解释之 cfg 文件参数","keywords":"","body":"1. yolo--- 参数解释之 cfg 文件参数 原文地址 www.cnblogs.com 1. yolo--- 参数解释之 cfg 文件参数 batch: 每一次迭代送到网络的图片数量，也叫批数量。增大这个可以让网络在较少的迭代次数内完成一个epoch。在固定最大迭代次数的前提下，增加batch会延长训练时间，但会更好的寻找到梯度下降的方向。如果你显存够大，可以适当增大这个值来提高内存利用率。这个值是需要大家不断尝试选取的，过小的话会让训练不够收敛，过大会陷入局部最优。 subdivision：这个参数很有意思的，它会让你的每一个batch不是一下子都丢到网络里。而是分成subdivision对应数字的份数，一份一份的跑完后，在一起打包算作完成一次iteration。这样会降低对显存的占用情况。如果设置这个参数为1的话就是一次性把所有batch的图片都丢到网络里，如果为2的话就是一次丢一半。 angle：图片旋转角度，这个用来增强训练效果的。从本质上来说，就是通过旋转图片来变相的增加训练样本集。 saturation，exposure，hue：饱和度，曝光度，色调，这些都是为了增强训练效果用的。 learning_rate：学习率，训练发散的话可以降低学习率。学习遇到瓶颈，loss不变的话也减低学习率。 max_batches： 最大迭代次数。 policy：学习策略，一般都是step这种步进式。 step，scales：这两个是组合一起的，举个例子：learn_rate: 0.001, step:100,25000,35000 scales: 10, .1, .1 这组数据的意思就是在0-100次iteration期间learning rate为原始0.001，在100-25000次iteration期间learning rate为原始的10倍0.01，在25000-35000次iteration期间learning rate为当前值的0.1倍，就是0.001， 在35000到最大iteration期间使用learning rate为当前值的0.1倍，就是0.0001。随着iteration增加，降低学习率可以是模型更有效的学习，也就是更好的降低train loss。 最后一层卷积层中filters数值是 5×（类别数 + 5）。 region里需要把classes改成你的类别数。 最后一行的random，是一个开关。如果设置为1的话，就是在训练的时候每一batch图片会随便改成320-640（32整倍数）大小的图片。目的和上面的色度，曝光度等一样。如果设置为0的话，所有图片就只修改成默认的大小 416*416。 "},"6 - 数字生活/1. 深度学习图像标注工具 - Labelme.html":{"url":"6 - 数字生活/1. 深度学习图像标注工具 - Labelme.html","title":"1. 深度学习图像标注工具 - Labelme","keywords":"","body":"1.1. Labelme 简介1.2. 安装使用1.3. 安装 labelme1.4. 使用 labelme1.5. 参考 本文由 简悦 SimpRead 转码， 原文地址 zhuanlan.zhihu.com 1.1. Labelme 简介 LabelMe 是一个用于在线图像标注的 Javascript 标注工具。与传统图像标注工具相比，其优势在于我们可以在任意地方使用该工具。此外，它也可以帮助我们标注图像，不需要在电脑中安装或复制大型数据集。 1.2. 安装使用 1.3. 安装 labelme 所有操作在已经安装 Anaconda 环境下运行：Anaconda3 Win10 安装教程 打开Anaconda3自带的Anaconda Prompt； 创建一个虚拟的 py 环境： conda create –name=labelme python=3.6 安装pyqt： conda install pyqt 安装labelme： pip install labelme -i https://pypi.tuna.tsinghua.edu.cn/simple 这里我进行了换源 ，原因是因为安装速度过慢！ 左侧选项依次是： 打开文件、打开目录、下一张、上一张、保存、创建多边形、编辑多边形、复制、删除、撤销操作、图片放大… 中间是： 图片区域 右边显示的有： flags、标签名称列表、多边形标注、图片文件列表 顶部菜单栏： 文件、编辑、视图、帮助 1.4. 使用 labelme 此处打开一个图片文件夹做示范： 点击左侧Open Dir选择需要标注的数据文件夹。 在顶部 edit 菜单栏中可选不同的标记方案，依次为：多边形（默认），矩形，圆、直线，点。 制作图像分割的数据，选择多边形，点击左侧的 create polygons ，回到图片，按下鼠标左键会生成一个点，完成标注后会形成一个标注区域，同时弹出 labelme 的框，键入标签名字，点击 OK或者回车完成标注。 如果需要更改标注的数据，可以选择左侧的编辑框，或者把鼠标移动到标签上，点击鼠标右键，可以选择编辑标签或者标注的名字。在编辑模式下，把鼠标移动到边界上，右键，可以增加点。 标注完成后点击Save保存。会在图片路径下生成同名的json文件。在目录下打开终端键入： labelme_json_to_dataset .json 会把生成的 json 转化成对应的数据文件： *.png info.yaml label.png label_names.txt label_viz.png 如果这里你嫌弃一个一个转化 json 文件太麻烦的话可以进行批量转化： 1. 利用 labelme 批量转换. json 文件 2. 也可以利用我写的一个 python 脚本： python import os path = './' # path为json文件存放的路径 json_file = os.listdir(path) os.system(\"activate labelme\") for file in json_file: os.system(\"labelme_json_to_dataset.exe %s\"%(path + '/' + file))注意：要把所有的 json 文件放在同一个文件夹哦！ 1.5. 参考 win10 下 Anaconda 使用 conda 连接网络出现错误 (CondaHTTPError: HTTP 000 CONNECTION FAILED for url） labeme 批量转化 json 未生成 info.yaml 解决办法 官方项目地址 数据标注软件 labelme 详解 "},"6 - 数字生活/Kindle使用手册.html":{"url":"6 - 数字生活/Kindle使用手册.html","title":"Kindle使用手册","keywords":"","body":"1. Kindle使用手册1.1. Vol-0 #Resource1.2. Vol-1 # 用Kindle看漫画1.3. Vol-2 # Kindle与词典1.4. Vol-3 # 如何在美亚购买Kindle原版电子书？1.5. Vol-4 #批量管理Kindle中的电子书1.6. Vol-5 # 苹果手机一键推送至Kindle1.7. Vol-6 # Kindle oasis 截屏1.8. Vol-7# 高效使用 Google 搜索引擎1.8.1. 1.给所有下载的电子书加格式设定。1.8.2. 2.使用 site 命令实现站内搜索1.8.3. 3.使用 filetype 命令实现确定文件格式搜索1.8.4. 4.限制特定网站、特定格式的电子书搜索1.8.5. 5.在搜索栏中建立起使用双引号（“”）的思维1.8.6. 6.在搜索栏中建立起使用双引号（+-号）的思维1. Kindle使用手册 日期：2020/01/02 1.1. Vol-0 #Resource 漫画资源：https://volmoe.com/ 15210166695@163.com 12345678 常规电子书资源：https://www.jiumodiary.com/ 1.2. Vol-1 # 用Kindle看漫画 经验贴：https://zhuanlan.zhihu.com/p/51235973 1.3. Vol-2 # Kindle与词典 Kindle字典：https://zhuanlan.zhihu.com/p/51848020 Kindle字典资源：https://zhuanlan.zhihu.com/p/51848020 1.4. Vol-3 # 如何在美亚购买Kindle原版电子书？ https://zhuanlan.zhihu.com/p/65754930 1.5. Vol-4 #批量管理Kindle中的电子书 管理电子书 https://u.nu/kcwx Kindle本地管理、亚马逊云端管理 1.6. Vol-5 # 苹果手机一键推送至Kindle https://u.nu/r7i2 1.7. Vol-6 # Kindle oasis 截屏 按住对角，USB线连接电脑。可在Kindle的存储器中找到。 1.8. Vol-7# 高效使用 Google 搜索引擎 1.8.1. 1.给所有下载的电子书加格式设定。 比如搜索「史蒂夫·乔布斯传mobi」，注意为了提高搜索精度，最好不要出现空格。 其实用这一招，基本上已经可以搞定绝大多数的电子书了，如果找不到，那你就要考虑一下所找的书有没有电子版的问题了。 若你还想让自己的搜书技能再上一层楼，不妨再试试用下方的几个技巧。 1.8.2. 2.使用 site 命令实现站内搜索 具体命令的格式为：site:网站链接 书名 (其中冒号为英文状态下输入的符号、网站链接与书名之间有空格) 比如静读君一般比较喜欢去「威锋论坛」找电子书，但是在该论坛里面一般搜索不到我想要的书，于是就可以使用该site命令了: 1.8.3. 3.使用 filetype 命令实现确定文件格式搜索 若你对电子书格式有一定的要求，比如你更喜欢 PDF格式的电子书，于是就可以在Google 上搜索的时候，直接使用 filetype 命令。 搜索格式：filetype:文件格式 书名 1.8.4. 4.限制特定网站、特定格式的电子书搜索 命令格式：site: 网址 filetype:文件格式 书名 1.8.5. 5.在搜索栏中建立起使用双引号（“”）的思维 如果你想了解关于《魔鬼经济学》这本书相关的文章内容，如果你输入“魔鬼经济学”，加不加双引号的差别是巨大的。不加的话，搜索系统会自动把魔鬼跟经济、学等字词拆分开来，找到的内容很散。 而如果加双引号的话，只有包含“魔鬼经济学”连在一起完整的几个字的内容才能显现。 1.8.6. 6.在搜索栏中建立起使用双引号（+-号）的思维 相反，“+号”就是务必要有这些内容才能找到。比如输入“科幻小说+三体”，那么找出来的内容则必然包括与三体相关的内容。 "},"6 - 数字生活/卡西欧gg1000指针调节.html":{"url":"6 - 数字生活/卡西欧gg1000指针调节.html","title":"卡西欧gg1000指针调节","keywords":"","body":"指针基准位置调节：https://www.casio.com.cn/index/movieplayer.html?id=1910 时区自动调节 https://haokan.baidu.com/v?pd=wisenatural&vid=9055976466324736719 "}}